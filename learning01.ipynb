{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is the start of the \"trial and error approach\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "- https://github.com/xurxodiz/cardwalker/tree/master/oracle\n",
    "- https://laterna--magica.blogspot.com/2011/10/oracle-parser.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false
   },
   "outputs": [],
   "source": [
    "sets = json.load(open('./AllSets.json', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hideCode": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "for k, v in sorted(sets.items()):\n",
    "    print(k, v['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cards_usaga = sets['USG']['cards']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cards_all=[]\n",
    "for k, sett in sets.items():\n",
    "    if (k in ['UGL', 'UST', 'UNH']) or (len(k)>3): # Ignore Unglued, Unstable and promotional things\n",
    "        continue\n",
    "    for card in sett['cards']:\n",
    "        card['set'] = k\n",
    "    cards_all.extend(sett['cards'])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASPAS_TEXT = \"ASPAS_TEXT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mains_col_names = ['name', 'manaCost', 'text_preworked', 'type', 'power', 'toughness',\n",
    "                   'types', 'supertypes', 'subtypes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('postgresql+psycopg2://mtg:mtg@localhost:5432/mtg')\n",
    "engine.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataframe of cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false
   },
   "outputs": [],
   "source": [
    "# Load deck list\n",
    "filename = './decks/Benalia-knights-rotation-proof.txt'\n",
    "deck_regex = r'^(?P<amount>\\d+) (?P<card_name>.*?)\\n'\n",
    "with open(filename, 'r') as f:\n",
    "    txt = f.readlines()\n",
    "    #print(txt)\n",
    "    deck_list = []\n",
    "    for x in txt:\n",
    "        deck_list.extend(re.findall(deck_regex, x))\n",
    "#deck_list # -> [(amount, card_name), (amount, card_name), ...]\n",
    "cards_in_deck_names_list = []\n",
    "for amount, card in deck_list:\n",
    "    for i in range(int(amount)):\n",
    "        cards_in_deck_names_list.append(card)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cards = cards_usaga\n",
    "cards = cards_all\n",
    "cards_df = pd.DataFrame.from_dict(cards)\n",
    "\n",
    "# Keep only the card in the deck, and as many copy of it as necessary\n",
    "cards_df = cards_df.drop_duplicates(subset=['name'])\n",
    "cards_df = cards_df.merge(\n",
    "    pd.DataFrame(cards_in_deck_names_list), how='right', left_on=['name'], right_on=[0])\n",
    "cards_df['card_id_in_deck'] = cards_df.index\n",
    "cards_df = cards_df.set_index('card_id_in_deck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false
   },
   "source": [
    "##  Are there different cards with the same name?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false
   },
   "source": [
    "No, there are not. Some cards with the same name have texts which are slightly differently written, but are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# Lets learn about duplicated card names\n",
    "test = pd.DataFrame.from_dict(cards_all).set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hideCode": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "print(test.shape)\n",
    "a=test[test['name'].duplicated()]#['name'].unique().shape\n",
    "print(a.shape)\n",
    "b = test[\n",
    "    (test['name'].duplicated())&(test['text'].duplicated())]\n",
    "print(b.shape)\n",
    "c = test[\n",
    "    (test['name'].duplicated(keep=False))&(~test['text'].duplicated(keep=False))]\n",
    "for idx, row in c.sort_values(by='name')[['name', 'text']].iterrows():\n",
    "    print(row['name'], '---', row['text'], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DF version: Let's start by trying to extract static habilities from cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "#cards_df = pd.DataFrame.from_dict(cards_usaga)\n",
    "cards_df = pd.DataFrame.from_dict(cards_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# Filter out new types\n",
    "#set(cards_df.types.apply(lambda x: tuple(set(x))))\n",
    "ignore_types = ('Conspiracy', 'Eaturecray', 'Phenomenon', 'Plane', 'Planeswalker', 'Scheme', 'Vanguard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "cards_df = cards_df[cards_df.types.apply(lambda x: not set(x).intersection(ignore_types))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "#cards_df.head(4).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does parenthesis contain usefull info or only explanations of abilities/effects?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like its always an explanation (so, no usefull info to discern possible targets, zones affected, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "pattern_parenthesis = r'\\((.*?)\\)'\n",
    "test = \"\"\"('Flying',), ('Trample',), ('Paper',), ('First strike',),\n",
    "       ('Phasing',), ('Haste',), ('Flash',), ('Island',), ('Defender',),\n",
    "       ('Blue',), ('Reach',), ('Devour X',), ('Vigilance',),\n",
    "       ('Double strike',), ('Indestructible',), ('Artifacts',),\n",
    "       ('Deathtouch',), ('Lifelink',), ('Menace',), ('Werewolf',),\n",
    "       ('Leviathans',), ('While voting',), ('Flying', 'Demon'),\n",
    "       ('Islandwalk',), ('Hexproof',), ('Plains',), ('Instant',),\n",
    "       ('Swamp',), ('Mountain',), ('Forest',), ('Dinosaur',),\n",
    "       ('Dinosaur Knight',), ('Leviathan',), ('Simultaneously',),\n",
    "       ('Rat',), ('During combat',), ('Investigate',),\n",
    "       ('Minotaur Pirate',), ('Each noncreature',), ('Vampire',),\n",
    "       ('Pyrogenius',), ('Swampwalk',), ('Bolster X',), ('Timebender',),\n",
    "       ('Bold Pyromancer',), ('Scry X',), ('Desertwalk',), ('Prowess',),\n",
    "       ('Martial Paragon',), ('Death Wielder',), ('Equipment',),\n",
    "       ('Valiant Protector',)\"\"\"\n",
    "a = re.findall(pattern_parenthesis, test)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "pattern_parenthesis = r'\\((.*?)\\)'\n",
    "cards_df['in_parentheses'] = cards_df['text'].apply(lambda x: tuple(re.findall(pattern_parenthesis, str(x))))\n",
    "set(cards_df['in_parentheses'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "#This example is not explaning an ability, but it is explaning something (an effect)\n",
    "st = 'If two or more creatures are tied for greatest power, target any one of them.'\n",
    "cards_df[cards_df['text'].str.contains(st).fillna(False)]['text'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove anything between parenthesis and replace name by SELF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace name by SELF and remove anything between parethesis\n",
    "pattern_parenthesis = r' ?\\(.*?\\)'\n",
    "def prework_text(card):\n",
    "    t = str(card['text']).replace(card['name'], 'SELF')\n",
    "    t = re.sub(pattern_parenthesis, '', t)\n",
    "    return t\n",
    "    \n",
    "cards_df['text_preworked'] = cards_df.apply(prework_text, axis=1)\n",
    "#cards_df['text_preworked']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "lands = [('Plains', '{W}'), ('Swamp', '{B}'), ('Island', '{U}'), ('Mountain', '{R}'), ('Forest', '{G}')]\n",
    "for land_name, sym in lands:\n",
    "    cards_df.loc[(cards_df[cards_df['name']==land_name]).index, 'text_preworked'] = '{T}: Add ' + sym +'.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sep = \"ª\"\n",
    "if cards_df['text_preworked'].str.contains(sep).any():\n",
    "    raise Exception(\"Bad separator symbol. It is contained in some text.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# replace card names by their ids - DOES NOT WORK: It replaces stuff that are not really card names\n",
    "# for example, just run name_id_dict.get('When')\n",
    "# Repalcing When in the cards will obviously replace a lot of stuff we don't want to\n",
    "name_id_dict = {c['name']: c['id'] for c in cards_all}\n",
    "#name_id_dict\n",
    "temp = sep.join(cards_df['text_preworked'])\n",
    "for i, (name, id_) in enumerate(name_id_dict.items()):\n",
    "    if not i%1000: print(i)\n",
    "    temp = temp.replace(name, id_)\n",
    "cards_df['text_preworked_name_id_replaced'] = temp.split(sep)\n",
    "#cards_df['text_preworked']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert cards_df[cards_df['text_preworked'].str.contains('\\(').fillna(False)]['text_preworked'].empty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain specific vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build some domain specific vocabulary for MTG. For example, let's list supertypes, types, subtypes, know all card names, this kind f thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create set of cards names\n",
    "cards_names = set(cards_df.name.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create set of supertypes\n",
    "array_of_supertypes_tuples = cards_df['supertypes'].dropna().apply(tuple).unique()\n",
    "cards_supertypes = tuple()\n",
    "for tup in array_of_supertypes_tuples:\n",
    "    cards_supertypes += tup\n",
    "    \n",
    "cards_supertypes = set(cards_supertypes)\n",
    "cards_supertypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create set of types\n",
    "array_of_types_tuples = cards_df['types'].dropna().apply(tuple).unique()\n",
    "cards_types = tuple()\n",
    "for tup in array_of_types_tuples:\n",
    "    cards_types += tup\n",
    "    \n",
    "cards_types = set(cards_types)\n",
    "cards_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create set of types\n",
    "array_of_subtypes_tuples = cards_df['subtypes'].dropna().apply(tuple).unique()\n",
    "cards_subtypes = tuple()\n",
    "for tup in array_of_subtypes_tuples:\n",
    "    cards_subtypes += tup\n",
    "    \n",
    "cards_subtypes = set(cards_subtypes)\n",
    "#cards_subtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cards_df.head(10).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pickle\n",
    "r = requests.get('http://media.wizards.com/2018/downloads/MagicCompRules%2020180713.txt')\n",
    "if not r.status_code == 200:\n",
    "    r.raise_for_status()\n",
    "comprules = r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rules.txt', 'r', encoding='latin-1') as f:\n",
    "    comprules = '\\n'.join(f.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_abilities_pat = r'702\\.\\d+\\. ([A-Za-z ]+)'\n",
    "abilities = re.findall(kw_abilities_pat, comprules)\n",
    "abilities.pop(0) # Its just the rulings \n",
    "abilities.sort()\n",
    "#abilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can we detect an abilities sentence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should:\n",
    "- Split sentences in a card by '\\n' (=card_sentences_list)\n",
    "- Split each element in card_sentences_list by ', ' (=split_candidate_sentences)\n",
    "- Search for the pattern r'^ability' in each item of split_candidate_sentences\n",
    "- If the pattern is found for evey item, then, split_candidate_sentences is an abilities sentence\n",
    "\n",
    "We can, at the same time, detect activated abilites sentences and \"rest\" sentences (which are not abilites and not triggered abilites ones).\n",
    "- Split sentences in a card by '\\n' (=card_sentences_list)\n",
    "- Those sentences which contain : are activated abilites\n",
    "\n",
    "Sentences which are not in any case above are \"rest\" sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ability_start_pattern = r'|'.join(['^'+ab+r'\\b' for ab in abilities])\n",
    "#print(ability_start_pattern)\n",
    "def is_ability_sentence(sentence):\n",
    "    elem_starting_with_ability = []\n",
    "    exceptions = ['Cycling abilities you activate cost up to {2} less to activate.']\n",
    "    if sentence in exceptions:\n",
    "        return False\n",
    "    elems = sentence.split(', ')\n",
    "    for elem in elems:\n",
    "        if re.search(ability_start_pattern, elem):\n",
    "            elem_starting_with_ability.append(re.search(ability_start_pattern, elem))\n",
    "        else:\n",
    "            return False\n",
    "    if len(elems)==len(elem_starting_with_ability):\n",
    "        return True\n",
    "    raise Exception('We should never get here')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "df = cards_df\n",
    "df['split_sentences'] = df['text_preworked'].apply(lambda x: x.split('\\n')) # list of sentences\n",
    "df['split_sentences_is_ability'] = df['split_sentences'].apply(lambda x: [is_ability_sentence(y) for y in x])\n",
    "\n",
    "df[df['split_sentences_is_ability'].apply(lambda x: True in x)][\n",
    "    ['split_sentences', 'split_sentences_is_ability']].iloc[1]['split_sentences']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect all possible differente abilities text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "ability_sentences = list(itertools.chain.from_iterable(df['split_sentences']))\n",
    "ability_sentences_is_ability = list(itertools.chain.from_iterable(df['split_sentences_is_ability']))\n",
    "abilities_full_set = []\n",
    "for a, b in zip(ability_sentences, ability_sentences_is_ability):\n",
    "    if b: abilities_full_set.append(a)\n",
    "abilities_full_set = set(abilities_full_set)\n",
    "len(abilities_full_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets detetect all paragraphs types (and keep each ability as a separate paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "def splitDataFrameList(df,target_column,separator=None):\n",
    "    '''\n",
    "    https://gist.github.com/jlln/338b4b0b55bd6984f883\n",
    "    df = dataframe to split,\n",
    "    target_column = the column containing the values to split\n",
    "    separator = the symbol used to perform the split\n",
    "    returns: a dataframe with each entry for the target column separated, with each element moved into a new row. \n",
    "    The values in the other columns are duplicated across the newly divided rows.\n",
    "    '''\n",
    "    def splitListToRows(row,row_accumulator,target_column,separator):\n",
    "        split_row = row[target_column]#.split(separator)\n",
    "        if isinstance(split_row, collections.Iterable):\n",
    "            for s in split_row:\n",
    "                new_row = row.to_dict()\n",
    "                new_row[target_column] = s\n",
    "                row_accumulator.append(new_row)\n",
    "        else:\n",
    "            new_row = row.to_dict()\n",
    "            new_row[target_column] = pd.np.nan\n",
    "            row_accumulator.append(new_row)\n",
    "    new_rows = []\n",
    "    df.apply(splitListToRows, axis=1, args=(new_rows,target_column,separator))\n",
    "    new_df = pd.DataFrame(new_rows)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'name':['a','b','c'], 'c':['a','b','c'], 'b':['a','b','c'], \"items\":[['a1','a2','a3'],['b1','b2','b3'],['c1','c2','c3']]})\n",
    "display(df)\n",
    "test = splitDataFrameList(df, target_column=\"items\")\n",
    "display(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paragraph_type(paragraph):\n",
    "    if is_ability_sentence(paragraph):\n",
    "        return 'ability'\n",
    "    elif ':' in paragraph:\n",
    "        return 'activated'\n",
    "    else:\n",
    "        return 'rest'\n",
    "\n",
    "def split_abilities_and_keep_the_rest(df_row):\n",
    "    '''Returns a list of abilities or a list of one element, which is not ability'''\n",
    "    if df_row['paragraph_type'] == 'ability':\n",
    "        return df_row['paragraph'].split(',')\n",
    "    \n",
    "    return [df_row['paragraph']]\n",
    "\n",
    "def get_aspas(text):\n",
    "    if pd.isnull(text):\n",
    "        return pd.np.nan\n",
    "    \n",
    "    reg = re.findall(r'\\\"(.+?)\\\"', text)\n",
    "    \n",
    "    if not reg:\n",
    "        return pd.np.nan\n",
    "    \n",
    "    res = reg[0]\n",
    "    \n",
    "    return res\n",
    "        \n",
    "    \n",
    "def get_paragraphs_and_types_df(card_row):\n",
    "    res = pd.DataFrame()\n",
    "    temp = pd.DataFrame()\n",
    "    \n",
    "    # Get initial paragraphs\n",
    "    temp['paragraph'] = card_row['text_preworked'].split('\\n')\n",
    "    temp[ASPAS_TEXT] = temp['paragraph'].apply(get_aspas)\n",
    "    # TODO CONTINUE FROM HERE CORRECT THIS\n",
    "    temp['paragraph'] = temp.apply(lambda x: x['paragraph'].replace(x[ASPAS_TEXT], ASPAS_TEXT)\n",
    "                                             if not pd.isnull(x[ASPAS_TEXT]) else x['paragraph'],\n",
    "                                  axis=1)\n",
    "    \n",
    "    temp['paragraph_type'] = temp['paragraph'].apply(get_paragraph_type)\n",
    "    \n",
    "    # Split the abilities paragraphs into multiple rows\n",
    "    temp['paragraph'] = temp.apply(split_abilities_and_keep_the_rest, axis=1)\n",
    "    temp = splitDataFrameList(temp, 'paragraph')\n",
    "    res = temp\n",
    "    \n",
    "    res['card_id'] = card_row.name\n",
    "    res['paragraph_order'] = range(res.shape[0])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cards_df['df_paragraphs'] = cards_df.apply(get_paragraphs_and_types_df, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cards_df[['text_preworked','df_paragraphs']].iloc[21]['df_paragraphs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cards_df_paragraphs = pd.concat(cards_df['df_paragraphs'].values)\n",
    "cards_df_paragraphs.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "temp = cards_df_paragraphs[~pd.isnull(cards_df_paragraphs[ASPAS_TEXT])]\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# Check if we get a different paragraph order for each ability\n",
    "cards_df_paragraphs[cards_df_paragraphs['type']=='ability'].sort_values(by=['card_id', 'paragraph_order'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show cards with triggered abilities\n",
    "#cards_df[cards_df['df_sentences'].apply(lambda x: 'activated' in x['type'].values)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets use the same approach and separate paragraphs in abilities-complements, costs-effects and keep the rest as is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ability_and_complement_regex = r'(' + ability_start_pattern +')' + r'(.*)'\n",
    "#ability_and_complement_regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pop_and_complements_df(paragraph_row):\n",
    "    res = pd.DataFrame()\n",
    "    pat_ability = re.compile(ability_and_complement_regex)\n",
    "    \n",
    "    if paragraph_row['paragraph_type'] == 'ability':\n",
    "        \n",
    "        #print(res['pop'].iloc[0])\n",
    "        #print(re.findall(pat, res['pop'].iloc[0]))\n",
    "        x = paragraph_row['paragraph']\n",
    "        if (not pd.isnull(x)) and re.findall(pat_ability, x):\n",
    "            ability = re.findall(pat_ability, x)[0][0].strip()\n",
    "            ability_complement = re.findall(pat_ability, x)[0][1].strip()\n",
    "        else:\n",
    "            import pdb\n",
    "            pdb.set_trace()\n",
    "        \n",
    "        res['pop'] = [ability, ability_complement] \n",
    "        res['pop_type'] =  ['ability', 'ability_complement'] \n",
    "        res['pop_order'] = range(res['pop'].shape[0])\n",
    "    \n",
    "    elif paragraph_row['paragraph_type'] == 'activated':\n",
    "        '''Break the costs in individual ones'''\n",
    "        costs, effect = paragraph_row['paragraph'].split(':')\n",
    "        \n",
    "        exceptions = ['Pay half your life, rounded up']\n",
    "        if costs in exceptions:\n",
    "            costs = costs.replace(',','')\n",
    "            \n",
    "        res['pop'] =  costs.split(',') + [effect]\n",
    "        types = ['activation_cost' for x in costs.split(',')] + ['activated_effect']\n",
    "        \n",
    "        res['pop_type'] =  types\n",
    "        res['pop_order'] = range(res['pop'].shape[0])\n",
    "        \n",
    "    else:\n",
    "        '''Keep the rest as rest or effect'''\n",
    "        effect = paragraph_row['paragraph']\n",
    "        \n",
    "        res['pop'] =  [effect]\n",
    "        res['pop_type'] =  ['effect']\n",
    "        res['pop_order'] = range(res['pop'].shape[0])\n",
    "        \n",
    "        \n",
    "    res['card_id'] = paragraph_row['card_id']\n",
    "    res['paragraph_order'] = paragraph_row['paragraph_order']\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cards_df_paragraphs['pop'] = cards_df_paragraphs.apply(get_pop_and_complements_df, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cards_df_paragraphs.iloc[3]['pop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cards_df_pops = pd.concat(cards_df_paragraphs['pop'].values, sort=True)\n",
    "#cards_df_pops['pop_hash'] = cards_df_pops['pop'].apply(lambda x: uuid.uuid4().hex)\n",
    "cards_df_pops.sort_values(by=['card_id','paragraph_order','pop_order']).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "activated_ability_paragraph_hash = cards_df_paragraphs[cards_df_paragraphs['type']=='activated'].sample(1)['paragraph_hash'].iloc[0]\n",
    "cards_df_pops[cards_df_pops['paragraph_hash']==activated_ability_paragraph_hash]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "investigate = '08038de1ded341a1b63f792d29b8dad8'\n",
    "cards_df_pops[cards_df_pops['paragraph_hash']==investigate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "#investigate = cards_df_pops[cards_df_pops['pop']=='Creatures you control have \"{T}'].iloc[0]['card_id']\n",
    "investigate = '7011018896f7a9a24b7f9dff722a7e990c43922b'\n",
    "cards_df_pops[cards_df_pops['card_id']==investigate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "#investigate = cards_df_pops[cards_df_pops['pop']=='Creatures you control have \"{T}'].iloc[0]['card_id']\n",
    "investigate = 'ade9880f3121cdf8db57c3f4ba0375c843ec14c0'\n",
    "cards_df_pops[cards_df_pops['card_id']==investigate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "cards_df_pops[cards_df_pops['pop']=='Pay half your life']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cards_df_pops[cards_df_pops['pop_type']=='activation_cost']['pop'].dropna().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many abilities, activated abilities and effects there are\n",
    "cards_df_pops['cont'] = 1\n",
    "\n",
    "index = ['pop_type']\n",
    "values = ['cont']\n",
    "\n",
    "pivot_pop = cards_df_pops.pivot_table(index=index, values=values, aggfunc=pd.np.sum)\n",
    "pivot_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show cards with triggered abilities\n",
    "#cards_df[cards_df['df_sentences'].apply(lambda x: 'activated' in x['type'].values)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets use the same approach and separate conditions-\"result effect\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_regex = r'((?:if |whenever |when |only |unless ).*?[,.])'\n",
    "#condition_regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_condition_regex = r'(at the (?:beginning |end )of.*?[,.])'\n",
    "#step_condition_regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "def get_condition(text):\n",
    "    if pd.isnull(text):\n",
    "        return None\n",
    "    \n",
    "    reg = re.findall(condition_regex, text, flags=re.IGNORECASE)\n",
    "    if not reg:\n",
    "        return None\n",
    "    \n",
    "    return reg\n",
    "\n",
    "def clean_effect_from_condition(row):\n",
    "    clean_effect = row['pop']\n",
    "    \n",
    "    if (not row['condition']):\n",
    "        return clean_effect\n",
    "    \n",
    "    condition = ''.join(row['condition'])\n",
    "    clean_effect = clean_effect.replace(condition, '')\n",
    "    return clean_effect\n",
    "    \n",
    "cards_df_pops['condition'] = cards_df_pops['pop'].apply(get_condition)\n",
    "cards_df_pops['effect_wo_condition'] = cards_df_pops.apply(clean_effect_from_condition, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conditions_and_effects_df(pop_row, original_cols=[]):\n",
    "    res = pd.DataFrame()\n",
    "    text = pop_row['pop']\n",
    "    \n",
    "    # Get list of conditions in text\n",
    "    reg_cond = re.findall(condition_regex, text, flags=re.IGNORECASE)\n",
    "    if not reg_cond:\n",
    "        reg_cond = []\n",
    "    \n",
    "    # Get list of step (time) conditions in text\n",
    "    reg_step_cond = re.findall(step_condition_regex, text, flags=re.IGNORECASE)\n",
    "    if not reg_step_cond:\n",
    "        reg_step_cond = []\n",
    "    \n",
    "    # Get the rest of the text in a list\n",
    "    text_wo_conditions = text\n",
    "    for cond in reg_cond + reg_step_cond:\n",
    "        text_wo_conditions = text_wo_conditions.replace(cond, '')\n",
    "    text_wo_conditions = text_wo_conditions.strip(',. ')\n",
    "    text_wo_conditions = [text_wo_conditions]\n",
    "    \n",
    "    temp = []\n",
    "    for part in reg_cond:\n",
    "        temp.append({'part_order':text.find(part), 'part': part.strip(',. '), 'part_type': 'condition'})\n",
    "    for part in reg_step_cond:\n",
    "        temp.append({'part_order':text.find(part), 'part': part.strip(',. '), 'part_type': 'step_condition'})\n",
    "    for part in text_wo_conditions:\n",
    "        temp.append({'part_order':text.find(part), 'part': part.strip(',. '), 'part_type': 'wo_conditions'})\n",
    "    \n",
    "    # Reset order to start from zero\n",
    "    res = pd.DataFrame(temp).sort_values(by=['part_order'])\n",
    "    res = res.reset_index(drop=True)\n",
    "    res['part_order'] = res.index\n",
    "\n",
    "    for col in original_cols:\n",
    "        res[col] = pop_row[col]\n",
    "        \n",
    "    return res\n",
    "\n",
    "cards_df_pops['pop_parts'] = cards_df_pops.apply(get_conditions_and_effects_df,\n",
    "                                                           args=(cards_df_pops.columns,),\n",
    "                                                           axis=1)\n",
    "cards_df_pop_parts = pd.concat(cards_df_pops['pop_parts'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cards_df_pop_parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect named cards cited inside cards text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For later: define a way to get card named cited in other cards text. Same approach of self should suffice:\n",
    "1. Detect the names (done below)\n",
    "2. Replace the names with a place holder. CARD_NAME_1, CARD_NAME_2 (for each card name in a cards text).\n",
    "3. Create columns CARD_NAME_1, CARD_NAME_2, etc. in dataframe, holding the actual name in the cell value\n",
    "4. Create entity detector for CARD_NAME_1, CARD_NAME_2,...\n",
    "5. Manually add edge between CARD_NAME_1 and its actual value (the actual card name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "named_card_pattern = r'('+r'|'.join(['{0}'.format(n) for n in cards_names])+r')'\n",
    "named_card_regex = r' named ' + named_card_pattern + '((?: or )' + named_card_pattern + ')?' + r'.*?'\n",
    "#named_card_regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "test_text = 'Add {G} for every card named Path of Peace in all graveyards.'\n",
    "test = re.findall(named_card_regex, test_text)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "a = cards_df['text_preworked'].apply(\n",
    "    lambda x: re.findall(named_card_regex, x)\n",
    "    if re.findall(named_card_regex, x)\n",
    "    else pd.np.nan\n",
    ").dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "'Zhang Fei, Fierce Warrior' in named_card_regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hideCode": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "cards_df.loc[a.index[0]]['text_preworked']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "a.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "test_text = 'SELF gets +2/+2 as long as you control a permanent named Guan Yu, Sainted Warrior or a permanent named Zhang Fei, Fierce Warrior in the battlefield.'\n",
    "test = re.findall(named_card_regex, test_text)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hideCode": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "cards_df.loc['ef0fe275d7e5625b20f4c5cd7fc34301df0bea6d']['text_preworked']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "a['ef0fe275d7e5625b20f4c5cd7fc34301df0bea6d']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save / Load (this process took some time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './cards_df_pop_parts.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "cards_df_pop_parts.to_pickle(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "cards_df_pop_parts = pd.read_pickle(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "(cards_df_pop_parts==cards_df_pop_parts2).all().all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cards_df_pop_parts[cards_df_pop_parts['part_type']=='step_condition']['part'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets avoid creating a pop node\n",
    "cards_df_pop_parts['part_type_full'] = cards_df_pop_parts['pop_type'] + '-' + cards_df_pop_parts['part_type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investingating abilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "Now, how to work with abilites followed by cost?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "def detect_abilities_sentence(sentlist):\n",
    "    for sent in sentlist:\n",
    "        if set(sent.split(', ')).issubset(set(abilities)):\n",
    "            return True\n",
    "    return False\n",
    "t = df['split_sentences'].apply(detect_abilities_sentence)\n",
    "df[t][df['text'].str.contains('umulative upkeep').fillna(False)]['text_preworked']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "df[df['text'].str.contains('umulative upkeep').fillna(False)]['text_preworked'].loc[30808]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Deal with cummulative upkeep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "Seems like, if followed by mana cost, cumulative upkeep COST may be followed by , (comma) or \\n (newline). But if the text for cumulative upkeep is longer, it seems to end with \\n everytime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# Check that these things are always the same\n",
    "cumulative_upkeep_pattern1 = r'(?:, )?cumulative upkeep—.*?[.]'\n",
    "cumulative_upkeep_pattern2 = r'(?:, )?cumulative upkeep—.*?[.\\n]'\n",
    "print(cumulative_upkeep_pattern1)\n",
    "def get_cumup1(xstr):\n",
    "    res = re.findall(cumulative_upkeep_pattern1, str(xstr), re.IGNORECASE)\n",
    "    if res:\n",
    "        return res\n",
    "    return pd.np.nan\n",
    "def get_cumup2(xstr):\n",
    "    res = re.findall(cumulative_upkeep_pattern2, str(xstr), re.IGNORECASE)\n",
    "    if res:\n",
    "        return res\n",
    "    return pd.np.nan\n",
    "df['cumup1'] = df['text_preworked'].apply(get_cumup1).fillna(False)\n",
    "df['cumup2'] = df['text_preworked'].apply(get_cumup2).fillna(False)\n",
    "diff = df['cumup1']==df['cumup2']\n",
    "df[~diff][['cumup1', 'cumup2', 'text_preworked']]\n",
    "assert diff.all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# Check that this never matches anything\n",
    "cumulative_upkeep_pattern1 = r'(?:, )?cumulative upkeep—.*?,'\n",
    "print(cumulative_upkeep_pattern1)\n",
    "def get_cumup1(xstr):\n",
    "    res = re.findall(cumulative_upkeep_pattern1, str(xstr), re.IGNORECASE)\n",
    "    if res:\n",
    "        return res\n",
    "    return pd.np.nan\n",
    "df['cumup1'] = df['text_preworked'].apply(get_cumup1)\n",
    "df['cumup1'].dropna()\n",
    "assert df['cumup1'].dropna().empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# Check that this never matches anything\n",
    "cumulative_upkeep_pattern1 = r', cumulative upkeep—'\n",
    "print(cumulative_upkeep_pattern1)\n",
    "def get_cumup1(xstr):\n",
    "    res = re.findall(cumulative_upkeep_pattern1, str(xstr), re.IGNORECASE)\n",
    "    if res:\n",
    "        return res\n",
    "    return pd.np.nan\n",
    "df['cumup1'] = df['text_preworked'].apply(get_cumup1)\n",
    "df['cumup1'].dropna()\n",
    "assert df['cumup1'].dropna().empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "#re.search('test', 'TeSt', re.IGNORECASE)\n",
    "#re.match('test', 'TeSt', re.IGNORECASE)\n",
    "#re.sub('test', 'xxxx', 'Testing', flags=re.IGNORECASE)\n",
    "# Non capturing group https://stackoverflow.com/questions/2703029/why-regular-expressions-non-capturing-group-is-not-working\n",
    "\n",
    "#cumulative_upkeep_pattern = r' ?cumulative upkeep[ |—].*?[.|,|\\n]'\n",
    "type1_cost = r' (\\{[A-Z0-9]+\\})+'\n",
    "type2_cost = r'—.*?[.|\\n]'\n",
    "cumulative_upkeep_pattern = r'(?:, )?(cumulative upkeep)({0}|{1})'.format(type1_cost, type2_cost)\n",
    "print(cumulative_upkeep_pattern)\n",
    "def get_cumup(xstr):\n",
    "    res = re.findall(cumulative_upkeep_pattern, str(xstr), re.IGNORECASE)\n",
    "    if res:\n",
    "        return res\n",
    "    return pd.np.nan\n",
    "df['cumup'] = df['text_preworked'].apply(get_cumup)\n",
    "posit = 28118\n",
    "display(df[['cumup', 'text_preworked']].dropna())#.loc[posit]['text_preworked'])\n",
    "#display(df[['cumup', 'text_preworked']].dropna()#.loc[posit]['cumup'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# check what is not contained (GREAT: the only card should not be considered anyway)\n",
    "cumup_all = df[df['text_preworked'].str.contains('umulative up')]\n",
    "cumup_detected = df[['cumup', 'text_preworked']].dropna()\n",
    "cumup_all[~cumup_all.index.isin(cumup_detected.index)]['text_preworked'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Extend procedure to other abilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "Check what 'Enchant' ability can enchant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# Get everythin that can follow Enchant\n",
    "def get_whats_enchanted(xstr):\n",
    "    res = re.findall(r'Enchant .*?[.|\\n|$]', str(xstr))#, re.IGNORECASE)\n",
    "    if res:\n",
    "        return tuple(res)\n",
    "    return pd.np.nan\n",
    "df['enchant_something'] = df['text_preworked'].apply(get_whats_enchanted)\n",
    "df['enchant_something'].dropna().drop_duplicates()\n",
    "enchant_abilities = set([x[0].strip('\\n') for x in df['enchant_something'].dropna().drop_duplicates()])\n",
    "#enchant_abilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regex below can detect any abilities with costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "type1_cost = r' (\\{[A-Z0-9]+\\})+'\n",
    "type2_cost = r'—.*?[.|\\n]'\n",
    "type3_cost = r' \\d+[,|\\n]'\n",
    "abilities_lower = '|'.join(abilities).lower()\n",
    "cumulative_upkeep_pattern = r'(?:, )?({abi})({cost1}|{cost2}|{cost3})'.format(\n",
    "    cost1=type1_cost, cost2=type2_cost, cost3=type3_cost, abi=abilities_lower)\n",
    "print(cumulative_upkeep_pattern)\n",
    "def get_cumup(xstr):\n",
    "    res = re.findall(cumulative_upkeep_pattern, str(xstr), re.IGNORECASE)\n",
    "    if res:\n",
    "        return res\n",
    "    return pd.np.nan\n",
    "df['cost_abilities'] = df['text_preworked'].apply(get_cumup)\n",
    "posit = 227\n",
    "display(df[['cost_abilities', 'text_preworked']].dropna())#.loc[posit]['text_preworked'])\n",
    "#display(df[['cumup', 'text_preworked']].dropna()#.loc[posit]['cumup'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# Detect other things following abilities\n",
    "abilities_follower = r' .*?[.|\\n]'\n",
    "abilities_lower = '|'.join(abilities)\n",
    "ability_w_follower = r'({abi})({fol})'.format(fol=abilities_follower, abi=abilities_lower)\n",
    "print(ability_w_follower)\n",
    "def get_cumup(xstr):\n",
    "    res = re.findall(ability_w_follower, str(xstr))\n",
    "    if res:\n",
    "        return tuple(res)\n",
    "    return pd.np.nan\n",
    "df['ability_w_follower'] = df['text_preworked'].apply(get_cumup)\n",
    "posit = 1100\n",
    "display(df[['ability_w_follower', 'text_preworked']].dropna())#.loc[posit]['text_preworked'])\n",
    "\n",
    "detected_cost_abi = df['cost_abilities'].dropna()\n",
    "df[~df.index.isin(detected_cost_abi.index)]['ability_w_follower'].dropna().drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What can bt in place of X in +X/+x (or actually +|-X/+|-X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides number, only X or Y will appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "def get_increases(text_str, pat=r'([+-][^\\d]/[+-][^\\d])'):\n",
    "    '''Given a text, extract a pattern and return the extraction or None'''\n",
    "    res = re.findall(pat, text_str)\n",
    "    return res\n",
    "t = cards_df['text_preworked'].apply(get_increases)\n",
    "res = set(chain(*(t.values)))\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which numbers may it contain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "def get_increases(text_str, pat=r'([+-]\\d+/)|(/[+-]\\d+)'):\n",
    "    '''Given a text, extract a pattern and return the extraction or None'''\n",
    "    res = re.findall(pat, text_str)\n",
    "    return res\n",
    "t = cards_df['text_preworked'].apply(get_increases)\n",
    "res = set(chain(*(t.values)))\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "pincre_pat=r'([+-][\\dXxYx]+/)'\n",
    "rincre_pat=r'(/[+-][\\dXxYx]+)'\n",
    "def get_increases(text_str, pat=r'([+-]\\d+/)|(/[+-]\\d+)'):\n",
    "    '''Given a text, extract a pattern and return the extraction or None'''\n",
    "    res = re.findall(pat, text_str)\n",
    "    return res\n",
    "pincre = cards_df['text_preworked'].apply(get_increases, args=(pincre_pat,))\n",
    "pincre_res = set(chain(*(pincre.values)))\n",
    "rincre = cards_df['text_preworked'].apply(get_increases, args=(rincre_pat,))\n",
    "rincre_res = set(chain(*(rincre.values)))\n",
    "print(pincre_res, rincre_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no +\\*/+\\*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "cards_df[cards_df['text_preworked'].str.contains('\\-\\*')]\n",
    "cards_df[cards_df['text_preworked'].str.contains('\\+\\*')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "def get_increases(text_str, pat=r'([+-][*])'):\n",
    "    '''Given a text, extract a pattern and return the extraction or None'''\n",
    "    res = re.findall(pat, text_str)\n",
    "    return res\n",
    "t = cards_df['text_preworked'].apply(get_increases)\n",
    "res = set(chain(*(t.values)))\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting special symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patt = r'\\{.*?\\}'\n",
    "t = cards_df_pop_parts['pop'].apply(lambda x: re.findall(patt, str(x))\n",
    "                             if re.findall(patt, str(x)) else pd.np.nan)\n",
    "symbols_set=set(itertools.chain.from_iterable(t.dropna()))\n",
    "#symbols_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weird_symbols = []\n",
    "worth_ignoring = ['{hr}','{½}','{∞}'] # Unglued or similar\n",
    "worth_ignoring.append('{CHAOS}')\n",
    "symbols_explanation = {\n",
    "    '{S}': {'explanation': 'Snow mana', 'example_card': 'Glacial Plating'},\n",
    "    '{R/P}': {'explanation': 'can be paid with either {R} or 2 life', 'example_card': 'Rage Extractor'},\n",
    "    '{Q}': {'explanation': '{Q} is the untap symbol', 'example_card': 'Order of Whiteclay'},\n",
    "    '{E}': {'explanation': 'Energy counter', 'example_card': 'Consulate Surveillance'},\n",
    "    '{C}': {'explanation': 'Colorless mana', 'example_card': 'Skarrg, the Rage Pits'},\n",
    "    '{CHAOS}': {'explanation': 'It is only in Plane cards and for a specific kind of game',\n",
    "                'example_card': 'Glimmervoid Basin'},\n",
    "}\n",
    "weird_cards = []\n",
    "for item in weird_symbols:\n",
    "    weird = cards_df_sentences[cards_df_sentences['sentences'].str.contains(item)]\n",
    "    weird_cards.append(cards_df[cards_df['id'].isin(weird['card_id'])])\n",
    "if weird_symbols:\n",
    "    weird_cards = pd.concat(weird_cards)\n",
    "    weird_cards[mains_col_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "def get_increases(text_str, pat=r'([+-][\\d+XxYx]{1,4}/[+-][\\d+XxYx]{1,4})'):\n",
    "    '''Given a text, extract a pattern and return the extraction or None'''\n",
    "    res = re.findall(pat, text_str)\n",
    "    return res\n",
    "t = cards_df_pop_parts['part'].apply(get_increases)\n",
    "pr_increase_symbols = set(chain(*(t.values)))\n",
    "#pr_increase_symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/51766157/how-to-force-a-pos-tag-in-spacy-before-after-tagger/51776803#51776803\n",
    "from spacy.symbols import ORTH, POS, NOUN, VERB\n",
    "\n",
    "nlp.tokenizer.add_special_case('{G}', [{ORTH: '{G}', POS: NOUN}])\n",
    "nlp.tokenizer.add_special_case('{T}', [{ORTH: '{T}', POS: VERB}])\n",
    "for symb in pr_increase_symbols:\n",
    "    nlp.tokenizer.add_special_case(symb, [{ORTH: symb, POS: NOUN}])\n",
    "\n",
    "doc = nlp('{T}: This {G} is a noun. Target creature gets +1/+1')\n",
    "\n",
    "for token in doc:\n",
    "    print('{:10}{:10}'.format(token.text, token.pos_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# Interpret {something} as NOUN (but tap and untap as verb)\n",
    "#https://stackoverflow.com/questions/51766157/how-to-force-a-pos-tag-in-spacy-before-after-tagger/51776803#51776803\n",
    "from spacy.symbols import ORTH, POS, NOUN, VERB, LOWER,LEMMA, TAG, NounType_com, nn, VerbForm_inf\n",
    "import spacy\n",
    "from spacy import displacy\n",
    " \n",
    "doc = nlp('I just bought 2 shares at 9 a.m. because the stock went up 30% in just 2 days according to the WSJ')\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# Noun phrases\n",
    "doc = nlp(\"Wall Street Journal just published an interesting piece on crypto currencies\")\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text, chunk.label_, chunk.root.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# Dependency parser\n",
    "doc = nlp('Wall Street Journal just published an interesting piece on crypto currencies')\n",
    " \n",
    "for token in doc:\n",
    "    print(\"{0}/{1} <--{2}-- {3}/{4}\".format(\n",
    "        token.text, token.tag_, token.dep_, token.head.text, token.head.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp('Wall Street Journal just published an interesting piece on crypto currencies')\n",
    "displacy.render(doc, style='dep', jupyter=True, options={'distance': 90})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "#test_sentence = cards_df[cards_df['static_abilities']==('Phasing',)].text.values[0]\n",
    "test_sentence ='\\nWhenever SELF attacks, it gets +1/+1.' #test_sentence +'\\nWhenever SELF attacks, it gets +1/+1.'\n",
    "test_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(test_sentence)\n",
    "displacy.render(doc, style='dep', jupyter=True, options={'distance': 90})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.symbols import ORTH, POS, NOUN, VERB, LOWER,LEMMA, TAG, nn#, VerbForm_inf,NounType_com,\n",
    "import spacy\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL = 'en_core_web_lg'\n",
    "MODEL = 'en_core_web_sm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Token\n",
    "\n",
    "def get_token_sent(token):\n",
    "    token_span = token.doc[token.i:token.i+1]\n",
    "    return token_span.sent\n",
    "\n",
    "try:\n",
    "    Token.set_extension('sent', getter=get_token_sent, force=True)\n",
    "except Exception:\n",
    "    Token.set_extension('sent', getter=get_token_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#MODEL = r'C:\\Users\\cs294662\\Downloads\\programas\\spacy\\data\\en_core_web_md-2.0.0\\en_core_web_md\\en_core_web_md-2.0.0'\n",
    "#MODEL = r'C:\\Users\\cs294662\\Downloads\\programas\\spacy\\data\\en_coref_lg-3.0.0\\en_coref_lg\\en_coref_lg-3.0.0'\n",
    "nlp = spacy.load(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "a=nlp('a')\n",
    "b=nlp('a')\n",
    "a==b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set custom tags for special cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#symbols_explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols_set_valid = symbols_set.difference(set(worth_ignoring))\n",
    "symbols_set_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add {SYMBOL} to NOUN recognizer\n",
    "symbols_set_mana = set()\n",
    "symbols_set_action = set()\n",
    "for sym in symbols_set_valid:\n",
    "    if not sym in ['{T}', '{Q}']:\n",
    "        symbols_set_mana.add(sym)\n",
    "        #nlp.tokenizer.add_special_case(sym, [{ORTH: sym, POS: NOUN, TAG:nn}])\n",
    "        nlp.tokenizer.add_special_case(sym, [{ORTH: sym, POS: NOUN}])\n",
    "    else:\n",
    "        symbols_set_action.add(sym)\n",
    "        nlp.tokenizer.add_special_case(sym, [{ORTH: sym, POS: VERB, TAG:'VB'}])\n",
    "\n",
    "# Add power and toughness in/decresing symbols to NOUN recognizer\n",
    "for sym in pr_increase_symbols:\n",
    "    #nlp.tokenizer.add_special_case(sym, [{ORTH: sym, POS: NOUN, TAG:nn}])\n",
    "    nlp.tokenizer.add_special_case(sym, [{ORTH: sym, POS: NOUN}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/44594759/spacy-adding-special-case-tokenization-rules-by-regular-expression-or-pattern\n",
    "#cost_pattern = r'{[\\dWGBURTX]}'\n",
    "#cost_pattern = re.compile(r'{[\\dWGBURTX]}')\n",
    "# add special case rule\n",
    "#special_case = [{ORTH: cost_pattern, LEMMA: 'COST', POS: 'NOUN'}]\n",
    "#nlp.tokenizer.add_special_case(cost_pattern, special_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "should_be_verbs = ['attacks', 'block', 'blocks', 'cast', 'control','controls', 'deal','deals', 'dies', 'enchant', 'flip', 'gain', 'gains', 'pay', 'return', 'sacrifice', 'shares', 'tap', 'untap']\n",
    "#for token in should_be_verbs:\n",
    "#    nlp.tokenizer.add_special_case(token, [{ORTH: token, POS: VERB}])\n",
    "#    nlp.tokenizer.add_special_case(token.title(), [{ORTH: token.title(), POS: VERB}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "test_phrase = '{G}: Two target creatures get +1/-1'\n",
    "#test_phrase = 'Target creature has flying'\n",
    "doc = nlp(test_phrase)\n",
    "displacy.render(doc, style='dep', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create custom entity matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher, Matcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "class EntityPhraseMatcher(object):\n",
    "    '''https://stackoverflow.com/questions/49097804/spacy-entity-from-phrasematcher-only'''\n",
    "    \n",
    "    name = 'entity_phrase_matcher'\n",
    "\n",
    "    def __init__(self, nlp, terms, label):\n",
    "        patterns = [nlp(term) for term in terms]\n",
    "        self.matcher = PhraseMatcher(nlp.vocab)\n",
    "        self.matcher.add(label, None, *patterns)\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        matches = self.matcher(doc)\n",
    "        spans = []\n",
    "        for label, start, end in matches:\n",
    "            span = Span(doc, start, end, label=label)\n",
    "            spans.append(span)\n",
    "        doc.ents = spans\n",
    "        return doc\n",
    "    \n",
    "class EntityMatcher(object):\n",
    "    name = 'entity_matcher'\n",
    "\n",
    "    def __init__(self, nlp, dict_label_terms):\n",
    "        '''dict_label_terms shoould be a dictionary in the format\n",
    "        {label(str): patterns(list)}'''\n",
    "        self.matcher = Matcher(nlp.vocab)\n",
    "        for label, patterns in dict_label_terms.items():\n",
    "            self.matcher.add(label, None, *patterns)\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        matches = self.matcher(doc)\n",
    "        spans = []\n",
    "        for label, start, end in matches:\n",
    "            span = Span(doc, start, end, label=label)\n",
    "            spans.append(span)\n",
    "        doc.ents = spans\n",
    "        return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zones = ['graveyard', 'play', 'library', 'hand', 'battlefield', 'exile', 'stack']\n",
    "players = ['opponent', 'you', 'controller', 'owner', 'player']\n",
    "steps = ['upkeep', 'draw step', 'end step', 'cleanup step', 'main phase', 'main phases']\n",
    "\n",
    "entities = {}\n",
    "entities['zones'] = ['graveyard', 'play', 'library', 'hand', 'battlefield', 'exile', 'stack']\n",
    "entities['players'] = ['opponent', 'you', 'controller', 'owner', 'player']\n",
    "entities['steps'] = ['upkeep', 'draw step', 'end step', 'cleanup step', 'main phase', 'main phases']\n",
    "entities['types'] = cards_types\n",
    "entities['subtypes'] = cards_subtypes\n",
    "entities['supertypes'] = cards_supertypes\n",
    "entities['supertypes'] = cards_supertypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import hashlib\n",
    "class HashableDict(OrderedDict):\n",
    "    def __hash__(self):\n",
    "        return hash(tuple(sorted(self.items())))\n",
    "    \n",
    "    def hexdigext(self):\n",
    "        return hashlib.sha256(''.join([str(k)+str(v) for k, v in self.items()]).encode()).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "if 'ner' in nlp.pipe_names:\n",
    "    nlp.remove_pipe('ner')\n",
    "if 'entity_matcher' in nlp.pipe_names:\n",
    "    nlp.remove_pipe('entity_matcher')\n",
    "#nlp.remove_pipe('ent_type_matcher')\n",
    "#nlp.remove_pipe('ent_subtype_matcher')\n",
    "#nlp.remove_pipe('ent_supertype_matcher')\n",
    "\n",
    "dict_label_terms = defaultdict(list)\n",
    "entity_to_kind_map = {}\n",
    "entity_key_to_hash_map = {} # entity key: entity node hash (node_id)\n",
    "\n",
    "for typ in cards_types:\n",
    "    key = 'TYPE: ' + typ.lower()\n",
    "    dict_label_terms[key].append([{'LOWER': t} for t in typ.lower().split()])\n",
    "    dict_label_terms[key].append([{'LOWER': t+'s'} for t in typ.lower().split()])\n",
    "    entity_to_kind_map[key] = 'TYPE'\n",
    "    entity_key_to_hash_map[key] = HashableDict({'entity': key}).hexdigext()\n",
    "# TODO define plural for subtypes and types, like elves\n",
    "cards_subtypes.add('elves')\n",
    "for typ in cards_subtypes:\n",
    "    key = 'SUBTYPE: ' + typ.lower()\n",
    "    dict_label_terms[key].append([{'LOWER': t} for t in typ.lower().split()])\n",
    "    dict_label_terms[key].append([{'LOWER': t+'s'} for t in typ.lower().split()])\n",
    "    entity_to_kind_map[key] = 'SUBTYPE'\n",
    "    entity_key_to_hash_map[key] = HashableDict({'entity': key}).hexdigext()\n",
    "for typ in cards_supertypes:\n",
    "    key = 'SUPERTYPE: '+typ.lower()\n",
    "    dict_label_terms[key].append([{'LOWER': t} for t in typ.lower().split()])\n",
    "    dict_label_terms[key].append([{'LOWER': t+'s'} for t in typ.lower().split()])\n",
    "    entity_to_kind_map[key] = 'SUPERTYPE'\n",
    "    entity_key_to_hash_map[key] = HashableDict({'entity': key}).hexdigext()\n",
    "for typ in ['white','black','blue','white','red','green','colorless', 'multicolored', 'multicolor']:\n",
    "    key = 'COLOR: '+typ.lower()\n",
    "    dict_label_terms[key].append([{'LOWER': t} for t in typ.lower().split()])\n",
    "    entity_to_kind_map[key] = 'COLOR'\n",
    "    entity_key_to_hash_map[key] = HashableDict({'entity': key}).hexdigext()\n",
    "for abi in abilities:\n",
    "    key = 'ABILITY: '+abi.lower()\n",
    "    dict_label_terms[key].append([{'LOWER': t} for t in abi.lower().split()])\n",
    "    entity_to_kind_map[key] = 'ABILITY'\n",
    "    entity_key_to_hash_map[key] = HashableDict({'entity': key}).hexdigext()\n",
    "for zone in zones:\n",
    "    key = 'ZONE: '+zone.lower()\n",
    "    dict_label_terms[key].append([{'LOWER': t, 'POS': NOUN} for t in zone.lower().split()])\n",
    "    dict_label_terms[key].append([{'LOWER': t+'s', 'POS': NOUN} for t in zone.lower().split()])\n",
    "    entity_to_kind_map[key] = 'ZONE'\n",
    "    entity_key_to_hash_map[key] = HashableDict({'entity': key}).hexdigext()\n",
    "for player in players:\n",
    "    key = 'PLAYER: '+player.lower()\n",
    "    dict_label_terms[key].append([{'LOWER': t, 'POS':spacy.symbols.PRON} for t in player.lower().split()])\n",
    "    dict_label_terms[key].append([{'LOWER': t, 'POS':spacy.symbols.NOUN} for t in player.lower().split()])\n",
    "    entity_to_kind_map[key] = 'PLAYER'\n",
    "    entity_key_to_hash_map[key] = HashableDict({'entity': key}).hexdigext()\n",
    "for step in steps:\n",
    "    key = 'STEP: '+step.lower()\n",
    "    dict_label_terms[key].append([{'LOWER': t} for t in step.lower().split()])\n",
    "    entity_to_kind_map[key] = 'STEP'\n",
    "    entity_key_to_hash_map[key] = HashableDict({'entity': key}).hexdigext()\n",
    "for sym in symbols_set_mana:\n",
    "    #print([{'ORTH': t} for t in sym.split()])\n",
    "    key = 'MANA: '+sym.lower()\n",
    "    if sym.strip('{}').isdigit() or sym.strip('{}').upper() == 'X':\n",
    "        key = 'MANA: '+'{generic}'\n",
    "    dict_label_terms[key].append([{'ORTH': t} for t in sym.split()])\n",
    "    entity_to_kind_map[key] = 'MANA'\n",
    "    entity_key_to_hash_map[key] = HashableDict({'entity': key}).hexdigext()\n",
    "for sym in symbols_set_action:\n",
    "    key = 'ACTION: '+sym.lower()\n",
    "    dict_label_terms[key].append([{'ORTH': t} for t in sym.split()])\n",
    "    entity_to_kind_map[key] = 'ACTION'\n",
    "    entity_key_to_hash_map[key] = HashableDict({'entity': key}).hexdigext()\n",
    "\n",
    "entity_matcher = EntityMatcher(nlp, dict_label_terms)\n",
    "try:\n",
    "    nlp.add_pipe(entity_matcher, before='ner')\n",
    "except Exception:\n",
    "    nlp.add_pipe(entity_matcher)\n",
    "\n",
    "print(nlp.pipe_names)  # see all components in the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select subset of cards to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work on full cards_df_pop_parts (cards_df was probably filtered right at the beginning)\n",
    "cards_df_for_graph = cards_df_pop_parts.copy()\n",
    "cards_df_for_graph.loc[:, 'part_doc'] = cards_df_for_graph['part'].apply(lambda x: nlp(x.strip('.,')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# Work on sample for now\n",
    "sample_cards = ['Tolarian Academy', 'Pacifism','Pariah','Congregate', 'Priest of Titania',\n",
    "             'Disenchant', ' Brilliant Halo', 'Thran Quarry', 'Path of Peace', 'Arcane Laboratory',\n",
    "            'Plains', 'Mountain', 'Forest', 'Swamp', 'Island']\n",
    "sample_ids = (cards_df[cards_df['name'].isin(sample_cards)]\n",
    "              .drop_duplicates(subset=['name'])\n",
    "              .index.unique()\n",
    "             )\n",
    "cards_df_for_graph = cards_df_pop_parts[cards_df_pop_parts['card_id'].isin(sample_ids)].copy()\n",
    "cards_df_for_graph.loc[:, 'part_doc'] = cards_df_for_graph['part'].apply(lambda x: nlp(x.strip('.,')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "test_phrase = \"{G}: At your end step, you can put target creatures from an opponent's graveyard into play under your control\"\n",
    "#test_phrase = 'At your end step, target creature has flying'\n",
    "doc = nlp(test_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "displacy.render(doc, style='ent', jupyter=True)\n",
    "displacy.render(doc, style='dep', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "list(doc.ents)+list(doc.noun_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extend pop with spacy dep_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective: card_id <- pop <- part <- root <- (children) <- entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     18
    ],
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "def get_all_children(root, parts, extra_keys):\n",
    "    '''From root, get all the children'''\n",
    "    while root.children:\n",
    "        for child in root.children:\n",
    "            key = child.dep_\n",
    "            while key in parts.keys():\n",
    "                key += '_'\n",
    "            parts[key] = child\n",
    "            if child.ent_type_:\n",
    "                parts['TYPE_'+key] = child.ent_type_\n",
    "                for i, c in enumerate(child.children):\n",
    "                    parts[key+'-'+'CHILD_'+str(i)] = {c.dep_:c}\n",
    "            #extra_keys[child] = key\n",
    "            #parts['key_of_head_of_'+key] = extra_keys[child.head]\n",
    "            get_all_children(child, parts, extra_keys)\n",
    "        break\n",
    "    return parts, extra_keys\n",
    "        \n",
    "def make_df_from_doc(doc):\n",
    "    '''Transform doc into a dataframe with interesting stuff'''\n",
    "    sents = []\n",
    "    \n",
    "    for sent in doc.sents:\n",
    "        extra_keys = {}\n",
    "        parts = defaultdict(list)\n",
    "        for root in sent:\n",
    "            if root.dep_ == 'ROOT':\n",
    "                parts['root'] = root\n",
    "                if root.ent_type_:\n",
    "                    parts['TYPE_'+'root'] = root.ent_type_\n",
    "                    for i, c in enumerate(root.children):\n",
    "                        parts['root'+'-'+'CHILD_'+str(i)] = {c.dep_:c}\n",
    "                #extra_keys[root] = 'root'\n",
    "                parts, extra_keys = get_all_children(root, parts, extra_keys)\n",
    "            sents.append(parts) if parts else []\n",
    "            parts = defaultdict(list)\n",
    "\n",
    "    return pd.DataFrame.from_records(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "test_sents = ['Target creature gets +1/+1 until end of turn. You may gain 4 life', 'You may put target creatures from graveyard into play']\n",
    "doc = nlp('\\n'.join(test_sents))\n",
    "df = make_df_from_doc(doc)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cards_df_pop_parts.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_defining_cols = ['card_id', 'paragraph_order', 'pop_order', 'pop_type', 'pop']\n",
    "part_defining_cols = ['card_id', 'paragraph_order', 'pop_order', 'part_order', 'part_type_full', 'pop', 'part']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Function to parse docs to list of dicts\n",
    "def parse_doc_to_list_of_dicts(df_row, original_cols=[], doc_col = 'part_doc'):\n",
    "    '''Get a dataframe, parse the doc column to token and entity nodes and edges dict, return the dataframe'''\n",
    "    doc = df_row[doc_col]\n",
    "    \n",
    "    token_node = [] # Source node\n",
    "    token_node_text = []\n",
    "    token_node_pos = []\n",
    "    token_node_tag = []\n",
    "    token_node_label = []\n",
    "    \n",
    "    token_head_dep = [] # Token edge do head\n",
    "    token_head_node = [] # Target node\n",
    "#     pop_node = [] # Pop node (avoided)\n",
    "    part_node = [] # part node\n",
    "    \n",
    "    token_to_entity_edge = []\n",
    "    entity_node = [] # Target entity nodes (target of token_node)\n",
    "    entity_node_ent_type = []\n",
    "    entity_node_entity = []\n",
    "    entity_node_desc = []\n",
    "    \n",
    "    # Track relations between token_nodes\n",
    "    tracker = HashableDict()\n",
    "    \n",
    "    # Avoided\n",
    "#     pop_dic = HashableDict()\n",
    "#     for col in pop_defining_cols:\n",
    "#         pop_dic[col] = df_row[col]\n",
    "    \n",
    "    part_dic = HashableDict()\n",
    "    for col in part_defining_cols:\n",
    "        part_dic[col] = df_row[col]\n",
    "    \n",
    "    for t in doc:\n",
    "        '''Create token and entity nodes and edges dict.'''\n",
    "        \n",
    "        token_dic = HashableDict()\n",
    "        ent_dic = HashableDict()\n",
    "\n",
    "        # Create node object as dict\n",
    "        for col in ['card_id', 'paragraph_order', 'part_order', 'pop_order', 'part_type_full']:\n",
    "            token_dic[col] = df_row[col]\n",
    "        token_dic['text'] = t.text.lower()\n",
    "        token_node_text.append(t.text.lower())\n",
    "        #token_node_label.append(t.text.lower())\n",
    "        token_dic['pos'] = t.pos_.lower()\n",
    "        token_node_pos.append(t.pos_.lower())\n",
    "        token_dic['tag'] = t.tag_.lower()\n",
    "        token_node_tag.append(t.tag_.lower())\n",
    "        token_dic['i'] = t.i\n",
    "        \n",
    "        # Create entity node object as dict. All entities should be equal in all processed cards\n",
    "        if t.ent_type_:\n",
    "            \n",
    "            ent = t.ent_type_\n",
    "            ent_dic['entity'] = ent\n",
    "            entity_node_entity.append(ent)\n",
    "            typ, desc = ent.split(': ')\n",
    "            entity_node_ent_type.append(typ)\n",
    "            entity_node_desc.append(desc)\n",
    "        \n",
    "        else:\n",
    "            entity_node_ent_type.append(pd.np.nan)\n",
    "            entity_node_entity.append(pd.np.nan)\n",
    "            entity_node_desc.append(pd.np.nan)\n",
    "            \n",
    "        token_node.append(token_dic.hexdigext())\n",
    "        token_head_dep.append(t.dep_.lower())\n",
    "        entity_node.append(ent_dic.hexdigext())\n",
    "        token_to_entity_edge.append(t.ent_iob_.lower())\n",
    "#         pop_node.append(pop_dic.hexdigext())\n",
    "        part_node.append(part_dic.hexdigext())\n",
    "        \n",
    "        tracker[t] = {'token_dic': token_dic}\n",
    "        \n",
    "    # Now, set the head of a token as its target node\n",
    "    for t, dicts in tracker.items():\n",
    "        head = t.head\n",
    "        head_dict = tracker[head]['token_dic']\n",
    "        token_head_node.append(head_dict.hexdigext())\n",
    "    \n",
    "    # Create dataframe \n",
    "    res = pd.DataFrame()\n",
    "    res['token_node'] = token_node\n",
    "    res['token_node_text'] = token_node_text\n",
    "    res['token_node_pos'] = token_node_pos\n",
    "    res['token_node_tag'] = token_node_tag\n",
    "    res['token_head_node'] = token_head_node\n",
    "    res['token_head_dep'] = token_head_dep\n",
    "\n",
    "    # Entity\n",
    "    res['entity_node'] = entity_node\n",
    "    res['entity_node_ent_type'] = entity_node_ent_type\n",
    "    res['entity_node_entity'] = entity_node_entity\n",
    "    res['entity_node_desc'] = entity_node_desc\n",
    "    \n",
    "    res['token_to_entity_edge'] = token_to_entity_edge\n",
    "#     res['pop_node'] = pop_node # avoided\n",
    "    res['part_node'] = part_node\n",
    "    \n",
    "    #res['label'] = token_node_label\n",
    "    #res = res.reset_index(drop=True)\n",
    "\n",
    "    for col in original_cols:\n",
    "        res[col] = df_row[col]\n",
    "    \n",
    "    # If src and target are the same, the token is a root, set target to card_id\n",
    "    res = res.reset_index(drop=True)\n",
    "    try:\n",
    "        res.loc[res[res['token_head_dep'] == 'root'].index, 'token_head_node'] = res['part_node']\n",
    "        res.loc[res[res['token_head_dep'] == 'root'].index, 'label'] = part_dic['part']\n",
    "    except TypeError as e:\n",
    "        # Someime res['token_head_dep'] = [] and cannot be compared to 'root'\n",
    "        pass\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse for graph\n",
    "cards_df_for_graph = cards_df_for_graph.apply(parse_doc_to_list_of_dicts, args=(cards_df_pop_parts.columns,), axis=1)\n",
    "cards_df_for_graph = pd.concat(cards_df_for_graph.values, sort=False).reset_index(drop=True)\n",
    "#cards_df_for_graph.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a graph for the cards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helping functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#G = nx.petersen_graph()\n",
    "shapes = ['box', 'polygon', 'ellipse', 'oval', 'circle', 'egg', 'triangle', 'exagon', 'star']\n",
    "colors = ['blue', 'black', 'red', '#db8625', 'green', 'gray', 'cyan', '#ed125b']\n",
    "styles = ['filled', 'rounded', 'rounded, filled', 'dashed', 'dotted, bold']\n",
    "\n",
    "entities_colors = {\n",
    "    'PLAYER': '#FF6E6E',\n",
    "    'ZONE': '#F5D300',\n",
    "    'ACTION': '#1ADA00',\n",
    "    'MANA': '#00DA84',\n",
    "    'SUBTYPE': '#0DE5E5',\n",
    "    'TYPE': '#0513F0',\n",
    "    'SUPERTYPE': '#8D0BCA',\n",
    "    'ABILITY': '#cc3300',\n",
    "    'COLOR': '#666633',\n",
    "    'STEP': '#E0E0F8'\n",
    "}\n",
    "\n",
    "def draw_graph(G, filename='test.png'):\n",
    "    pdot = nx.drawing.nx_pydot.to_pydot(G)\n",
    "\n",
    "\n",
    "    for i, node in enumerate(pdot.get_nodes()):\n",
    "        attrs = node.get_attributes()\n",
    "        node.set_label(str(attrs.get('label', 'none')))\n",
    "    #     node.set_fontcolor(colors[random.randrange(len(colors))])\n",
    "        entity_node_ent_type = attrs.get('entity_node_ent_type', pd.np.nan)\n",
    "        if not pd.isnull(entity_node_ent_type):\n",
    "            color = entities_colors[entity_node_ent_type.strip('\"')]\n",
    "            node.set_fillcolor(color)\n",
    "            node.set_color(color)\n",
    "            node.set_shape('hexagon')\n",
    "            #node.set_colorscheme()\n",
    "            node.set_style('filled')\n",
    "        \n",
    "        node_type = attrs.get('type', None)\n",
    "        if node_type == '\"card\"':\n",
    "            color = '#999966'\n",
    "            node.set_fillcolor(color)\n",
    "#             node.set_color(color)\n",
    "            node.set_shape('star')\n",
    "            #node.set_colorscheme()\n",
    "            node.set_style('filled')\n",
    "    #     \n",
    "        #pass\n",
    "\n",
    "    for i, edge in enumerate(pdot.get_edges()):\n",
    "        att = edge.get_attributes()\n",
    "        att = att.get('label', 'NO-LABEL')\n",
    "        edge.set_label(att)\n",
    "    #     edge.set_fontcolor(colors[random.randrange(len(colors))])\n",
    "    #     edge.set_style(styles[random.randrange(len(styles))])\n",
    "    #     edge.set_color(colors[random.randrange(len(colors))])\n",
    "\n",
    "    png_path = filename\n",
    "    pdot.write_png(png_path)\n",
    "\n",
    "    from IPython.display import Image\n",
    "    return Image(png_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cards_df_for_graph.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdfg=cards_df_for_graph\n",
    "cdfg = cards_df_for_graph.merge(cards_df, left_on=['card_id'], right_index=True)\n",
    "#cdfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# types|colors to cards graph\n",
    "nodes_card_df = cdfg[['card_id', 'supertypes', 'types', 'subtypes', 'colors', 'manaCost']].copy()\n",
    "\n",
    "# Generate df with card_id and entity_node_id refering to card's type, color, supertype, etc.\n",
    "res = []\n",
    "for col in nodes_card_df:\n",
    "    if col == 'card_id':\n",
    "        continue\n",
    "    if col == 'manaCost':\n",
    "        nodes_card_df[col] = nodes_card_df[col].apply(\n",
    "            lambda x: ['{generic}'\n",
    "                       if (y.strip('{}').isdigit() or y.strip('{}').upper()=='X')\n",
    "                       else y\n",
    "                       for y in re.findall(r'{.*?}', x)\n",
    "                      ]\n",
    "            if not pd.isnull(x) else x)\n",
    "        nodes_card_df = nodes_card_df.rename(columns={'manaCost':'manas'})\n",
    "        col = 'manas'\n",
    "    temp = nodes_card_df[['card_id', col]].copy().dropna()\n",
    "    temp = splitDataFrameList(temp, col)\n",
    "    temp['entity_node_ent_type'] = col.upper()[:-1]\n",
    "    # Build the name which can be maped to hexdigext\n",
    "    temp['entity_node_entity'] = temp.apply(lambda x: ': '.join([x['entity_node_ent_type'], x[col].lower()]), axis=1)\n",
    "    temp['entity_node'] = temp['entity_node_entity'].apply(lambda x: entity_key_to_hash_map[x])\n",
    "    temp = temp.rename(columns={col: 'entity_node_desc'})\n",
    "    temp = temp.drop_duplicates(subset=['card_id', 'entity_node'])\n",
    "    res.append(temp)\n",
    "    \n",
    "res = pd.concat(res, sort=True)\n",
    "res['edge_type'] = 'entity_to_card'\n",
    "\n",
    "res.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdfg = pd.concat([cdfg, res], sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different approach: build node by node\n",
    "\n",
    "# Objective: card_id <- part <- root <- (children) <- entities # pop was avoided\n",
    "\n",
    "# Token to head\n",
    "# Manipulate df to generate two others: nodes and edges\n",
    "# Edges relate (token to head, head to part, part to pop, pop to card, and token to entity)\n",
    "# attention: head is also a token\n",
    "# and set node and edge attributes (both dfs should contain the attributes)\n",
    "\n",
    "# NODES ########################################## \n",
    "nodes = {}\n",
    "nodes_cols = {}\n",
    "nodes_attr = {}\n",
    "\n",
    "# Token nodes\n",
    "nodes_cols['token'] = ['token_node', 'token_node_text', 'token_node_pos', 'token_node_tag',\n",
    "       'token_head_node', 'token_head_dep',\n",
    "       'part_order', 'part_type', 'card_id', 'paragraph_order', \n",
    "       'pop_order', 'pop_type']\n",
    "nodes['token'] = (cdfg[nodes_cols['token']]\n",
    "                  .rename(columns={'token_node':'node_id'})\n",
    "                  .dropna(subset=['node_id'])\n",
    "                 )\n",
    "nodes['token']['type'] = 'token'\n",
    "nodes['token']['label'] = nodes['token'].apply(lambda x:\n",
    "                                              '-'.join([x['token_node_text'],\n",
    "                                                        x['token_node_pos'],\n",
    "                                                        x['token_node_tag']]), axis=1)\n",
    "nodes_attr['token'] = [x for x in nodes['token'].columns if x not in ['node_id']]\n",
    "\n",
    "# Entity nodes\n",
    "nodes_cols['entity'] = ['entity_node', 'entity_node_entity','entity_node_ent_type', 'entity_node_desc']\n",
    "nodes['entity'] = (cdfg[nodes_cols['entity']]\n",
    "                   .dropna(subset=['entity_node_ent_type'])\n",
    "                   .rename(columns={'entity_node':'node_id'})\n",
    "                  )\n",
    "nodes['entity']['type'] = 'entity'\n",
    "nodes['entity']['label'] = nodes['entity'].apply(lambda x:\n",
    "                                              '-'.join([x['entity_node_entity'],\n",
    "                                                        ]), axis=1)\n",
    "nodes_attr['entity'] = [x for x in nodes['entity'].columns if x not in ['node_id']]\n",
    "\n",
    "# Part nodes\n",
    "nodes_cols['part'] = ['part_node',\n",
    "                      'part', 'part_order', 'part_type',\n",
    "                      'card_id',\n",
    "                      'paragraph_order',\n",
    "                      'pop_order', 'pop_type']\n",
    "nodes['part'] = (cdfg[nodes_cols['part']]\n",
    "                 .rename(columns={'part_node':'node_id'})\n",
    "                 .dropna(subset=['node_id'])\n",
    "                 )\n",
    "nodes['part']['type'] = 'part'\n",
    "nodes['part']['label'] = nodes['part'].apply(lambda x:\n",
    "                                              '-'.join([x['part']]), axis=1)\n",
    "nodes_attr['part'] = [x for x in nodes['part'].columns if x not in ['node_id']]\n",
    "\n",
    "# Pop nodes (avoided)\n",
    "# nodes_cols['pop'] = ['pop_node',\n",
    "#                       'card_id',\n",
    "#                       'paragraph_order',\n",
    "#                       'pop', 'pop_order', 'pop_type']\n",
    "# nodes['pop'] = (cdfg[nodes_cols['pop']]\n",
    "#                 .rename(columns={'pop_node':'node_id'})\n",
    "#                 .dropna(subset=['node_id'])\n",
    "#                  )\n",
    "# nodes['pop']['type'] = 'pop'\n",
    "# nodes['pop']['label'] = nodes['pop'].apply(lambda x:\n",
    "#                                               '-'.join([x['pop']]), axis=1)\n",
    "# nodes_attr['pop'] = [x for x in nodes['pop'].columns if x not in ['node_id']]\n",
    "\n",
    "# Card nodes\n",
    "nodes_cols['card'] =  ['card_id'] + mains_col_names\n",
    "nodes['card'] = cdfg[nodes_cols['card']]\n",
    "nodes['card'] = nodes['card'].rename(columns={'name':'card_name'})\n",
    "nodes['card']['node_id'] = nodes['card']['card_id']\n",
    "nodes['card'] = nodes['card'].dropna(subset=['node_id', 'card_name'], how='any')                 \n",
    "nodes['card']['type'] = 'card'\n",
    "nodes['card']['label'] = nodes['card'].apply(lambda x:\n",
    "                                              '-'.join([x['card_name']]), axis=1)\n",
    "nodes_attr['card'] = [x for x in nodes['card'].columns if x not in ['node_id']]\n",
    "\n",
    "# EDGES #########################################\n",
    "card_as_start = True # Sets card as source and pop, part, token, entity as targets\n",
    "edges = {} # k->type, v-> dataframe\n",
    "edges_cols = {} # list\n",
    "edges_attr = {} # list\n",
    "\n",
    "# Token edges to head (and head to part)\n",
    "edges_cols['token_to_head_part'] = ['token_node', 'token_head_node', 'token_head_dep',\n",
    "                       'part_order', 'part_type', 'card_id', 'paragraph_order', \n",
    "                       'pop_order', 'pop_type']\n",
    "renamer = {'token_node':'source', 'token_head_node':'target'}\n",
    "if card_as_start:\n",
    "    renamer = {'token_head_node':'source', 'token_node':'target'}\n",
    "edges['token_to_head_part'] = (cdfg[edges_cols['token_to_head_part']]\n",
    "                               .rename(columns=renamer)\n",
    "                               .dropna(subset=['source', 'target'], how='any')\n",
    "                              )\n",
    "edges['token_to_head_part']['type'] = 'token_to_head_part'\n",
    "edges['token_to_head_part']['label'] = edges['token_to_head_part'].apply(lambda x:\n",
    "                                              '-'.join([x['token_head_dep'],\n",
    "                                                       ]).upper(), axis=1)\n",
    "edges_attr['token_to_head_part'] = [x for x in edges['token_to_head_part'].columns\n",
    "                                    if x not in ['source', 'target']]\n",
    "\n",
    "# Entity edges to Token \n",
    "edges_cols['entity_to_token'] = ['token_node', 'entity_node']\n",
    "renamer = {'entity_node':'source', 'token_node':'target'}\n",
    "if card_as_start:\n",
    "    renamer = {'token_node':'source', 'entity_node':'target'}\n",
    "edges['entity_to_token'] = (cdfg[edges_cols['entity_to_token']]\n",
    "                            .dropna()\n",
    "                            .rename(columns=renamer)\n",
    "                           )\n",
    "edges['entity_to_token']['type'] = 'entity_to_token'\n",
    "edges['entity_to_token']['relation'] = 'is_class_of'\n",
    "edges['entity_to_token']['label'] = edges['entity_to_token'].apply(lambda x:\n",
    "                                              '-'.join([x['relation'],\n",
    "                                                       ]).upper(), axis=1)\n",
    "edges_attr['entity_to_token'] = [x for x in edges['entity_to_token'].columns\n",
    "                                    if x not in ['source', 'target']]\n",
    "\n",
    "# Entity edges to cards\n",
    "edges_cols['entity_to_card'] = ['card_id', 'entity_node']\n",
    "renamer = {'entity_node':'source', 'card_id':'target'}\n",
    "if card_as_start:\n",
    "    pass # Ignore in this case, because we want it reversed: card as target\n",
    "#     renamer = {'card_id':'source', 'entity_node':'target'}\n",
    "edges['entity_to_card'] = (cdfg[cdfg['edge_type']=='entity_to_card'][edges_cols['entity_to_card']]\n",
    "                            .dropna()\n",
    "                            .rename(columns=renamer)\n",
    "                           )\n",
    "edges['entity_to_card']['type'] = 'entity_to_card'\n",
    "edges['entity_to_card']['relation'] = 'is_contained_in'\n",
    "edges['entity_to_card']['label'] = edges['entity_to_card'].apply(lambda x:\n",
    "                                              '-'.join([x['relation'],\n",
    "                                                       ]).upper(), axis=1)\n",
    "edges_attr['entity_to_card'] = [x for x in edges['entity_to_card'].columns\n",
    "                                    if x not in ['source', 'target']]\n",
    "\n",
    "# Part and pop edges (avoided)\n",
    "# edges_cols['part_to_pop'] = ['part_node', 'pop_node',\n",
    "#                        'part_order', 'part_type',\n",
    "#                        'card_id', 'paragraph_order', \n",
    "#                        'pop_order', 'pop_type']\n",
    "# renamer = {'part_node':'source', 'pop_node':'target'}\n",
    "# if card_as_start:\n",
    "#     renamer = {'pop_node':'source', 'part_node':'target'}\n",
    "# edges['part_to_pop'] = (cdfg[edges_cols['part_to_pop']]\n",
    "#                         .rename(columns=renamer)\n",
    "#                         .dropna(subset=['source', 'target'], how='any')\n",
    "#                         )\n",
    "# edges['part_to_pop']['type'] = 'part_to_pop'\n",
    "# edges['part_to_pop']['label'] = edges['part_to_pop'].apply(lambda x:\n",
    "#                                               '-'.join([str(x['part_order']),\n",
    "#                                                         x['part_type'],\n",
    "#                                                        ]).upper(), axis=1)\n",
    "# edges_attr['part_to_pop'] = [x for x in edges['part_to_pop'].columns\n",
    "#                                     if x not in ['source', 'target']]\n",
    "\n",
    "# Pop to card edges (avoided)\n",
    "# edges_cols['pop_to_card'] = ['card_id', 'pop_node',\n",
    "#                        'paragraph_order', \n",
    "#                        'pop_order', 'pop_type']\n",
    "# renamer = {'pop_node':'source', 'card_id':'target'}\n",
    "# if card_as_start:\n",
    "#     renamer = {'card_id':'source', 'pop_node':'target'}\n",
    "# edges['pop_to_card'] = (cdfg[edges_cols['pop_to_card']]\n",
    "#                         .rename(columns=renamer)\n",
    "#                         .dropna(subset=['source', 'target'], how='any')\n",
    "#                         )\n",
    "# edges['pop_to_card']['type'] = 'pop_to_card'\n",
    "# edges['pop_to_card']['label'] = edges['pop_to_card'].apply(lambda x:\n",
    "#                                               '-'.join([str(x['paragraph_order']),\n",
    "#                                                         str(x['pop_order']),\n",
    "#                                                         x['pop_type'],\n",
    "#                                                        ]).upper(), axis=1)\n",
    "# edges_attr['pop_to_card'] = [x for x in edges['pop_to_card'].columns\n",
    "#                                     if x not in ['source', 'target']]\n",
    "\n",
    "# Part and card edges (avoided)\n",
    "edges_cols['part_to_card'] = ['part_node', 'card_id',\n",
    "                       'part_order', 'part_type_full',\n",
    "                       'paragraph_order', \n",
    "                       'pop_order', 'pop_type', 'part_type']\n",
    "renamer = {'pop_node':'source', 'card_id':'target'}\n",
    "if card_as_start:\n",
    "    renamer = {'card_id':'source', 'part_node':'target'}\n",
    "edges['part_to_card'] = (cdfg[edges_cols['part_to_card']]\n",
    "                        .rename(columns=renamer)\n",
    "                        .dropna(subset=['source', 'target'], how='any')\n",
    "                        )\n",
    "edges['part_to_card']['type'] = 'part_to_card'\n",
    "edges['part_to_card']['label'] = edges['part_to_card'].apply(lambda x:\n",
    "                                              '-'.join([str(int(x['paragraph_order'])),\n",
    "                                                        str(int(x['pop_order'])),\n",
    "                                                        str(int(x['part_order'])),\n",
    "                                                        x['part_type_full'],\n",
    "                                                       ]).upper(), axis=1)\n",
    "edges_attr['part_to_card'] = [x for x in edges['part_to_card'].columns\n",
    "                                    if x not in ['source', 'target']]\n",
    "\n",
    "# Build dfs\n",
    "nodes_df = pd.concat(nodes.values(), sort=True).drop_duplicates(subset=['node_id'])\n",
    "nodes_df = nodes_df.dropna(subset=[x for x in nodes_df.columns if not x in ['node_id', 'label']], how='all')\n",
    "edges_df = pd.concat(edges.values(), sort=True).drop_duplicates(subset=['source', 'target'])\n",
    "edges_df = edges_df[\n",
    "    (edges_df['source'].isin(nodes_df['node_id']))&\n",
    "    (edges_df['target'].isin(nodes_df['node_id']))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminate_and_wrap_in_quotes(text):\n",
    "    return '\"'+str(text).replace('\"', '')+'\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create nodes and edges from dataframe\n",
    "graphs = []\n",
    "# EDGES\n",
    "source = 'source'\n",
    "target = 'target'\n",
    "for k in edges_df['type'].unique():\n",
    "    #if not k=='token_to_head_part': continue\n",
    "    print(k)\n",
    "    \n",
    "    edge_attr = edges_attr[k]\n",
    "    graphs.append(\n",
    "        nx.from_pandas_edgelist(edges_df[edges_df['type']==k],\n",
    "                              source=source,\n",
    "                              target=target,\n",
    "                              edge_attr=edge_attr,\n",
    "                              create_using=nx.DiGraph())\n",
    "    )\n",
    "\n",
    "G = nx.compose_all(graphs)\n",
    "\n",
    "# NODES (set attributes)\n",
    "for k in nodes_df['type'].unique():\n",
    "    print(k)\n",
    "    node_col = 'node_id'\n",
    "    for node_attr in nodes_attr[k]: \n",
    "        temp = nodes_df[[node_attr, node_col]]\n",
    "        temp = temp.dropna()\n",
    "        \n",
    "        # Eliminate and wrap in quotes\n",
    "        temp[node_attr] = temp[node_attr].apply(eliminate_and_wrap_in_quotes)\n",
    "        nx.set_node_attributes(G, pd.Series(temp[node_attr].values, index=temp[node_col].values).copy().to_dict(), name=node_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to image\n",
    "draw_graph(G, 'Gtest.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# Build paths between each pair of cards\n",
    "card_nodes = [x for x,y in G.nodes(data=True) if y['type']=='\"card\"']\n",
    "temp = []\n",
    "for i, s in enumerate(card_nodes):\n",
    "    if not i%10: print(i)\n",
    "    for j, e in enumerate(card_nodes):\n",
    "        if s is e:\n",
    "            continue\n",
    "        # All simple paths becomes huge as it find path through many cards\n",
    "        # Lets try to create a different graph with only the nodes and edges with start card id, and the last node\n",
    "        # Entities do not have an attribute card_id\n",
    "        start_card_nodes = [x for x,y in G.nodes(data=True) if y.get('card_id', None) == G.node[s]['card_id']]\n",
    "        entity_nodes = [x for x,y in G.nodes(data=True) if y.get('type', None) == '\"entity\"']\n",
    "        interesting_subgraph = G.subgraph(start_card_nodes+entity_nodes+[e])\n",
    "        #temp_name = str('./paths_between_pairs/interesting_subgraph_{0}.png'.format(i))\n",
    "        #draw_graph(interesting_subgraph, temp_name)\n",
    "        for k, path in enumerate(nx.all_simple_paths(interesting_subgraph, s, e)):\n",
    "            subgraph = G.subgraph(path)\n",
    "            temp.append(G.subgraph(path))\n",
    "print('Avoid saving {0} images'.format(len(temp)))\n",
    "for i, g in enumerate(temp):\n",
    "    #if not i%10: print('{0}/{1}'.format(i, len(temp)))\n",
    "    #temp_name = str('./paths_between_pairs/temp_{0}.png'.format(i))\n",
    "    #draw_graph(g, temp_name)\n",
    "    #display(draw_graph(g, temp_name))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplify link between cards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's contratct the paths to a simple link containing the path itself as attribute, but a simple label describing it.\n",
    "\n",
    "ATTENTION: This approach does not seem too useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# Build card -> root -> other_card (not too useful)\n",
    "import copy\n",
    "start_nodes = [x for x,y in G.nodes(data=True) if y['type']=='\"card\"']\n",
    "end_nodes = [x for x,y in G.nodes(data=True) if y['type']=='\"card\"']\n",
    "H = nx.DiGraph()\n",
    "for s in start_nodes:\n",
    "    for e in end_nodes:\n",
    "        if s is e:\n",
    "            continue\n",
    "        for path in nx.all_simple_paths(G, s, e):\n",
    "            H.add_nodes_from([(path[0], G.nodes[path[0]])])\n",
    "            H.add_nodes_from([(path[-1], G.nodes[path[-1]])])\n",
    "            #H.add_node(G[path[0]])\n",
    "            #H.add_node(G[path[-1]])\n",
    "            # Build edge attributes and than add it (after the loop)\n",
    "            att = {}\n",
    "            label = ''\n",
    "            added_node = None\n",
    "            edge1_label = ''\n",
    "            edge2_label = ''\n",
    "            for i, (a, b) in enumerate(zip(path[:-1], path[1:])):\n",
    "                if not i:\n",
    "                    edge1_label += G.edges[a,b].get('label','')\n",
    "                if G.nodes[a].get('token_head_dep','').strip('\"') =='root': #build root node\n",
    "                    added_node = copy.deepcopy(a)\n",
    "                    added_node_attr = G.nodes[a]\n",
    "                    added_node_attr.update({'full_original_path':path})\n",
    "                    H.add_nodes_from([(added_node, added_node_attr)])\n",
    "                    edge2_label += G.nodes[b].get('token_head_dep','none').strip('\"')\n",
    "            \n",
    "            H.add_edge(path[0], added_node, label=edge1_label)\n",
    "            H.add_edge(added_node, path[-1], label=edge2_label)\n",
    "H_temp_name = str('H.png'.format(i))\n",
    "display(draw_graph(H, H_temp_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get paths between cards and join them, contracting same card nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hideCode": false
   },
   "outputs": [],
   "source": [
    "# Card to card paths (degree here may be interesting)\n",
    "import copy\n",
    "H = nx.DiGraph()\n",
    "card_nodes = [x for x,y in G.nodes(data=True) if y['type']=='\"card\"']\n",
    "temp = []\n",
    "for i, s in enumerate(card_nodes):\n",
    "    if not i%10: print(i)\n",
    "    for j, e in enumerate(card_nodes):\n",
    "        if s is e:\n",
    "            continue\n",
    "            \n",
    "        # All simple paths becomes huge as it find path through many cards\n",
    "        # Lets try to create a different graph with only the nodes and edges with start card id, and the last node\n",
    "        # Entities do not have an attribute card_id\n",
    "        start_card_nodes = [x for x,y in G.nodes(data=True) if y.get('card_id', None) == G.node[s]['card_id']]\n",
    "        entity_nodes = [x for x,y in G.nodes(data=True) if y.get('type', None) == '\"entity\"']\n",
    "        interesting_subgraph = G.subgraph(start_card_nodes+entity_nodes+[e])\n",
    "        #temp_name = str('./paths_between_pairs/interesting_subgraph_{0}.png'.format(i))\n",
    "        #draw_graph(interesting_subgraph, temp_name)\n",
    "        for k, path in enumerate(nx.all_simple_paths(interesting_subgraph, s, e)):\n",
    "            subgraph = G.subgraph(path)\n",
    "            H = nx.union(H, subgraph, rename=('H-', 'path-'))\n",
    "\n",
    "# Contract all card nodes, so all edges begin and end at a card\n",
    "# Comment this chunk and you will get all disjoint paths between cards\n",
    "card_names = set([y['card_name'] for x,y in H.nodes(data=True) if y['type'].strip('\"')=='card'])\n",
    "groups_of_same_nodes = []\n",
    "print(\"Start grouping\")\n",
    "for i, card_name in enumerate(card_names):\n",
    "    if not i%10: print('{0}/{1}'.format(i, len(card_names)))\n",
    "    temp = [x for x,y in H.nodes(data=True) if y.get('card_name','')==card_name]\n",
    "    if len(temp)>1:\n",
    "        groups_of_same_nodes.append(temp)\n",
    "print(\"Start contraction\")\n",
    "for i, group in enumerate(groups_of_same_nodes):\n",
    "    if not i%10: print('{0}/{1}'.format(i, len(groups_of_same_nodes)))\n",
    "    for node in group[1:]:\n",
    "        H = nx.contracted_nodes(H, group[0], node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# THIS TAKES FOREVER\n",
    "# Print to file \n",
    "print(\"Start writing image\")\n",
    "H_temp_name = str('H.png'.format(i))\n",
    "display(draw_graph(H, H_temp_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to cytoscape format\n",
    "print('Export graphml G')\n",
    "nx.write_graphml(G, 'G_test.graphml')\n",
    "print('Export graphml H')\n",
    "# Remove attributes of type dict\n",
    "for (n,d) in H.nodes(data=True):\n",
    "    if d.get(\"contraction\", None):\n",
    "        del d[\"contraction\"]\n",
    "nx.write_graphml(H, 'H_test.graphml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# To nodes and edges table in postgresql\n",
    "nodes_df['id'] = nodes_df['node_id']\n",
    "nodes_df.to_sql('nodes', engine, index=False, if_exists='replace')\n",
    "edges_df.to_sql('edges', engine, index=False, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count edges and their type related to nodes which are entites instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "entity_nodes = nodes_df[nodes_df['type']=='entity']\n",
    "test_ent_id = entity_nodes.iloc[0]['node_id']\n",
    "print(test_ent_id)\n",
    "entity_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "in_edges = []\n",
    "out_edges = []\n",
    "res = []\n",
    "for ent_node in entity_nodes['node_id']:\n",
    "    for node in G[ent_node]: # node is a neighbour of the entity\n",
    "        for in_ed in G.in_edges([node], data=True):\n",
    "            s, t, d = in_ed #source, target, data\n",
    "            in_edges.append(d)\n",
    "        for out_ed in G.out_edges([node], data=True):\n",
    "            s, t, d = out_ed #source, target, data\n",
    "            out_edges.append(d)\n",
    "\n",
    "    in_ = pd.DataFrame(in_edges)\n",
    "    in_['edge_type'] = 'in'\n",
    "    in_['ent_node'] = nx.get_node_attributes(G, 'label')[ent_node]\n",
    "    out_ = pd.DataFrame(out_edges)\n",
    "    out_['edge_type'] = 'out'\n",
    "    out_['ent_node'] = nx.get_node_attributes(G, 'label')[ent_node]\n",
    "    res.append(pd.concat([in_, out_]).copy())\n",
    "    \n",
    "res = pd.concat(res)\n",
    "res['cont'] = 1\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "res.pivot_table(values=['cont'], index=['ent_node', 'label', 'pop_type'], columns=['edge_type'], aggfunc=pd.np.sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Investigating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "sent = 120#12352#12350#1205\n",
    "test_phrase = cards_df_sentences[cards_df_sentences['sentences'].str.contains('\\{W}')]['sentences'].iloc[sent]\n",
    "\n",
    "#test_phrase = 'Tap something: get more'\n",
    "doc = nlp(test_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "for s in doc.sents:\n",
    "    print(s)\n",
    "    print('Change')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "displacy.render(doc, style='dep', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "for t in doc:\n",
    "    print(t, t.tag_, t.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "sents = []\n",
    "for sent in doc.sents:\n",
    "    print(sent)\n",
    "    for tr in sent.subtree:\n",
    "        sentd = {\n",
    "            'word': tr,\n",
    "            'ancestors': [x for x in tr.ancestors],\n",
    "            'children': [x for x in tr.children],\n",
    "            'cluster': tr.cluster,\n",
    "            'conjuncts': [x for x in tr.conjuncts],\n",
    "            'dep': tr.dep_,\n",
    "            'ent_type': tr.ent_type_,\n",
    "            'head': tr.head,\n",
    "            'lemma': tr.lemma_,\n",
    "            'tag':tr.tag_\n",
    "        }\n",
    "        sents.append(sentd)\n",
    "        #print(sentd)\n",
    "        #print('\\n')\n",
    "df = pd.DataFrame(sents)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## Detect verbs in each sentence of a card (mainly non-abilities ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "def get_main_nlp_feats(row):\n",
    "    sents = []\n",
    "    doc = row['doc']\n",
    "    for sent in doc.sents:\n",
    "        #print(sent)\n",
    "        for tr in sent.subtree:\n",
    "            sentd = {\n",
    "                'sent': sent,\n",
    "                'text': tr.text,\n",
    "                'word': tr,\n",
    "                'ancestors': [x for x in tr.ancestors],\n",
    "                'children': [x for x in tr.children],\n",
    "                'cluster': tr.cluster,\n",
    "                'conjuncts': [x for x in tr.conjuncts],\n",
    "                'dep': tr.dep_,\n",
    "                'ent_type': tr.ent_type_,\n",
    "                'head': tr.head,\n",
    "                'lemma': tr.lemma_,\n",
    "                'pos':tr.pos_,\n",
    "                'tag':tr.tag_\n",
    "            }\n",
    "            sents.append(sentd)\n",
    "            #print(sentd)\n",
    "            #print('\\n')\n",
    "    df = pd.DataFrame(sents)\n",
    "    df['card_id'] = row['id']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "def get_doc(text_str):\n",
    "    return nlp(text_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "cards_df_sample = cards_df.sample(10000).copy()\n",
    "print('creating docs')\n",
    "cards_df_sample['doc'] = cards_df_sample['text_preworked'].apply(get_doc)\n",
    "print('getting docs feats')\n",
    "cards_df_sample['nlp_feats'] = cards_df_sample.apply(get_main_nlp_feats, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# Concatanate sent_feats\n",
    "sent_feats = pd.concat(cards_df_sample['nlp_feats'].values,sort=True, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# Counting and showing ROOT verbs\n",
    "count_verbs = sent_feats[(sent_feats['dep']==\"ROOT\")&(sent_feats['pos']=='VERB')]['lemma'].unique()\n",
    "count_verbs.sort()\n",
    "print(count_verbs.shape, count_verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# Counting and showing ROOT nouns\n",
    "count_nouns = sent_feats[(sent_feats['dep']==\"ROOT\")&(sent_feats['pos']=='NOUN')]['lemma'].unique()\n",
    "count_nouns.sort()\n",
    "print(count_nouns.shape, count_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "spacy.explain(\"CD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "t = sent_feats[sent_feats['word'].apply(lambda x: x.text=='deals')]['word'].iloc[120]\n",
    "details={}\n",
    "print(t)\n",
    "print(t._.sent)\n",
    "for c in t.children:\n",
    "    details[c] = {'pos':c.pos_, 'tag':c.tag_, 'lemma':c.lemma_, 'dep_':c.dep_}\n",
    "print(details)\n",
    "displacy.render(t.doc, style='dep', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "for nounc in t.doc.noun_chunks:\n",
    "    print(nounc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "def get_children_and_attributes(token):\n",
    "    details = {}\n",
    "    #for t in token.children\n",
    "count_verbs = sent_feats[(sent_feats['dep']==\"ROOT\")&(sent_feats['pos']=='VERB')]['lemma'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# Show roots\n",
    "temp = sent_feats[(sent_feats['dep']==\"ROOT\")][['lemma', 'children', 'sent']].copy()\n",
    "temp['children'] = temp['children'].apply(lambda x: tuple(set(x)))\n",
    "#temp['lemma'] = temp['lemma'].apply(lambda x: x.text)\n",
    "temp.drop_duplicates(subset=['lemma', 'children'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Try to match types and set as entity\n",
    "https://stackoverflow.com/questions/49097804/spacy-entity-from-phrasematcher-only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "test_sents = []\n",
    "test_sents.append(test_phrase)\n",
    "test_sents.append('If a Sliver deals combat damage to a player, its controller may create a +1/+1 colorless Sliver creature token.')\n",
    "test_sents.append('Whenever a Sliver deals combat damage to a player, its controller may create a +1/+1 colorless Sliver creature token.')\n",
    "colorless = '\\n'.join([x for x in cards_df[cards_df['text'].str.contains('colorless').fillna(False)]['text'].iloc[:5]])\n",
    "test_sents.append(colorless)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp('\\n'.join(test_sents))\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "options = {'compact': False,\n",
    "          'collapse_punct': False}\n",
    "displacy.render(doc, style='dep', jupyter=True, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "sents = []\n",
    "for sent in doc.sents:\n",
    "    print(sent)\n",
    "    for tr in sent.subtree:\n",
    "        sentd = {\n",
    "            'word': tr,\n",
    "            'ancestors': [x for x in tr.ancestors],\n",
    "            'children': [x for x in tr.children],\n",
    "            'cluster': tr.cluster,\n",
    "            'conjuncts': [x for x in tr.conjuncts],\n",
    "            'dep': tr.dep_,\n",
    "            'ent_type': tr.ent_type_,\n",
    "            'head': tr.head,\n",
    "            'lemma': tr.lemma_,\n",
    "            'tag':tr.tag_\n",
    "        }\n",
    "        sents.append(sentd)\n",
    "        #print(sentd)\n",
    "        #print('\\n')\n",
    "df = pd.DataFrame(sents)\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "test_sents = ['Two target creatures get +1/+1 each until end of turn']\n",
    "doc = nlp('\\n'.join(test_sents))\n",
    "\n",
    "df = []\n",
    "for token in doc:\n",
    "    df.append({\n",
    "        'token.text': token.text,\n",
    "        'token.pos_': token.pos_,\n",
    "        'token.dep_':token.dep_,\n",
    "        'token.ent':token.ent_type_,\n",
    "        'token.head.text':token.head.text})\n",
    "df = pd.DataFrame.from_dict(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "test_sents = ['Target creature gets +1/+1 until end of turn', '+1/+1 gets target creature until end of turn']\n",
    "doc = nlp('\\n'.join(test_sents))\n",
    "\n",
    "df = []\n",
    "for chunk in doc.noun_chunks:\n",
    "    df.append({'chunk.text': chunk.text,\n",
    "               'chunk.root.text': chunk.root.text,\n",
    "               'chunk.root.tag': chunk.root.tag_,\n",
    "               'chunk.root.dep_':chunk.root.dep_,\n",
    "               'chunk.root.head.text':chunk.root.head.text})\n",
    "df = pd.DataFrame.from_dict(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "test_sents = ['Target creature gets +1/+1 until end of turn', '+1/+1 gets target creature until end of turn']\n",
    "doc = nlp('\\n'.join(test_sents))\n",
    "\n",
    "df = []\n",
    "for chunk in doc:\n",
    "    if chunk.pos_=='NOUN':\n",
    "        df.append({'chunk.text': chunk.text,\n",
    "                   'chunk.tag': chunk.tag_,\n",
    "                   'chunk.dep_':chunk.dep_,\n",
    "                   'chunk.head.text':chunk.head.text})\n",
    "df = pd.DataFrame.from_dict(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "test_sents = ['Target creature gets +1/+1 until end of turn', 'You may put target creature from graveyard to play']\n",
    "doc = nlp('\\n'.join(test_sents))\n",
    "\n",
    "parts = defaultdict(list)\n",
    "sents = []\n",
    "\n",
    "for sent in doc.sents:\n",
    "    for chunk in sent.noun_chunks:\n",
    "        if chunk.root.dep_=='nsubj':\n",
    "            parts['nsubj'].append(chunk.text)\n",
    "            parts['nsubj_root'].append(chunk.root.head.text)\n",
    "        elif chunk.root.dep_=='dobj':\n",
    "            parts['dobj'].append(chunk.text)\n",
    "            parts['dobj_root'].append(chunk.root.head.text)\n",
    "        elif chunk.root.dep_=='pobj':\n",
    "            parts['pobj'].append(chunk.text)\n",
    "            parts['pobj_root'].append(chunk.root.head.text)\n",
    "    \n",
    "    sents.append(parts)\n",
    "    parts = defaultdict(list)\n",
    "\n",
    "sents\n",
    "#df = pd.DataFrame.from_dict(parts)\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "test_sents = ['Target creature gets +1/+1 until end of turn', 'You may put target creatures from graveyard into play']\n",
    "doc = nlp('\\n'.join(test_sents))\n",
    "\n",
    "parts = defaultdict(list)\n",
    "sents = []\n",
    "extra_keys = {}\n",
    "\n",
    "def get_all_children(root, parts):\n",
    "    '''From root, get all the children'''\n",
    "    while root.children:\n",
    "        for child in root.children:\n",
    "            key = child.dep_\n",
    "            while key in parts.keys():\n",
    "                key += '_'\n",
    "            parts[key] = child\n",
    "            extra_keys[child] = key\n",
    "            parts['key_of_head_of_'+key] = extra_keys[child.head]\n",
    "            get_all_children(child, parts)\n",
    "        break\n",
    "    return parts\n",
    "            \n",
    "\n",
    "for sent in doc.sents:\n",
    "    \n",
    "    for root in doc:\n",
    "        if root.dep_ == 'ROOT':\n",
    "            parts['root'] = root\n",
    "            extra_keys[root] = 'root'\n",
    "            parts = get_all_children(root, parts)\n",
    "        sents.append(parts) if parts else []\n",
    "        parts = defaultdict(list)\n",
    "\n",
    "sents\n",
    "#df = pd.DataFrame.from_dict(parts)\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## Networkx and pygraphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import networkx as nx\n",
    "G = nx.petersen_graph()\n",
    "pdot = nx.drawing.nx_pydot.to_pydot(G)\n",
    "\n",
    "shapes = ['box', 'polygon', 'ellipse', 'oval', 'circle', 'egg', 'triangle', 'exagon', 'star', ]\n",
    "colors = ['blue', 'black', 'red', '#db8625', 'green', 'gray', 'cyan', '#ed125b']\n",
    "styles = ['filled', 'rounded', 'rounded, filled', 'dashed', 'dotted, bold']\n",
    "\n",
    "for i, node in enumerate(pdot.get_nodes()):\n",
    "    node.set_label(\"n%d\" % i)\n",
    "    node.set_shape(shapes[random.randrange(len(shapes))])\n",
    "    node.set_fontcolor(colors[random.randrange(len(colors))])\n",
    "    node.set_fillcolor(colors[random.randrange(len(colors))])\n",
    "    node.set_style(styles[random.randrange(len(styles))])\n",
    "    node.set_color(colors[random.randrange(len(colors))])\n",
    "\n",
    "for i, edge in enumerate(pdot.get_edges()):\n",
    "    edge.set_label(\"e%d\" % i)\n",
    "    edge.set_fontcolor(colors[random.randrange(len(colors))])\n",
    "    edge.set_style(styles[random.randrange(len(styles))])\n",
    "    edge.set_color(colors[random.randrange(len(colors))])\n",
    "\n",
    "png_path = \"test.png\"\n",
    "pdot.write_png(png_path)\n",
    "\n",
    "from IPython.display import Image\n",
    "Image(png_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "heading_collapsed": true,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Should we train a model for POSTAGGING?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "Not sure. Many verbs interpreted sometimes as nouns are also sometimes interpreted as verbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "sents = '\\n'.join([x for x in cards_df.sample(200)['text_preworked']])\n",
    "doc = nlp(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nouns = []\n",
    "for token in doc:\n",
    "    if token.pos_ == 'NOUN' and token.lower_ not in nouns:\n",
    "        nouns.append(token.lower_)\n",
    "nouns.sort()\n",
    "nouns\n",
    "# Nouns that should be verbs:\n",
    "# 'attacks', 'block', 'blocks', 'cast', 'control','controls', 'deal','deals', 'dies', 'enchant', 'flip', 'gain', 'gains', 'pay', 'return', 'sacrifice', 'shares', 'tap', 'untap'\n",
    "\n",
    "# Nouns that COULD be verbs:\n",
    "# 'counter(S)','exile'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "verbs = []\n",
    "for token in doc:\n",
    "    if token.pos_ == 'VERB' and token.lower_ not in verbs:\n",
    "        verbs.append(token.lower_)\n",
    "verbs.sort()\n",
    "verbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Get predictions ins a format easy to correct and feed back as training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "Check here https://spacy.io/usage/training#training-simple-style.\n",
    "\n",
    "It should be easy to train a model, as long as we have a fre things in place"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Build tables like:\n",
    "card | sentence | token0 | token1 | ... | tokenN\n",
    "card | sentence | tag0 | tag1 | ... | tagN\n",
    "card | sentence | deps0 | deps1 | ... | depsN\n",
    "card | sentence | head0 | head1 | ... | headN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "cards_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "tokens = []\n",
    "tags = []\n",
    "deps = []\n",
    "head_ids = []\n",
    "card_counter=0\n",
    "for idx, card in cards_df.sample(200).iterrows():\n",
    "    card_counter+=1\n",
    "    if not card_counter%40: print(card_counter)\n",
    "    for sentence in card['text_preworked'].split('\\n'):\n",
    "        doc = nlp(sentence)\n",
    "        basics = {\n",
    "                'card': card['name'],\n",
    "                'sentence': sentence,\n",
    "            }\n",
    "        toks, tag, dep, head = deepcopy(basics), deepcopy(basics), deepcopy(basics), deepcopy(basics)\n",
    "        for i, tok in enumerate(doc):\n",
    "            toks.update({'{0:04d}'.format(i): tok.text})\n",
    "            tag.update({'{0:04d}'.format(i): tok.tag_})\n",
    "            dep.update({'{0:04d}'.format(i): tok.dep_})\n",
    "            head.update({'{0:04d}'.format(i): tok.head.i})\n",
    "        tokens.append(toks)\n",
    "        tags.append(tag)\n",
    "        deps.append(dep)\n",
    "        head_ids.append(head)\n",
    "            \n",
    "df_tokens = pd.DataFrame(tokens)\n",
    "df_tags = pd.DataFrame(tags)\n",
    "df_deps = pd.DataFrame(deps)\n",
    "df_head_ids = pd.DataFrame(head_ids)\n",
    "\n",
    "display(df_tokens.head(2), df_tags.head(2), df_deps.head(2), df_head_ids.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "heading_collapsed": true,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# NLTK testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# https://www.nltk.org/book/ch10.html section 5.2\n",
    "dt = nltk.DiscourseTester(['A student dances', 'Every student is a person'])\n",
    "dt.readings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "dt.add_sentence('No person dances', consistchk=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "dt.retract_sentence('No person dances', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "dt.add_sentence('A person dances', informchk=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tag import RegexpTagger\n",
    "tagger = RegexpTagger(\n",
    "    [('^(chases|runs)$', 'VB'),\n",
    "     ('^(a)$', 'ex_quant'),\n",
    "     ('^(every)$', 'univ_quant'),\n",
    "     ('^(dog|boy)$', 'NN'),\n",
    "     ('^(He)$', 'PRP')\n",
    "])\n",
    "rc = nltk.DrtGlueReadingCommand(depparser=nltk.MaltParser(tagger=tagger))\n",
    "dt = nltk.DiscourseTester(['Every dog chases a boy', 'He runs'], rc)\n",
    "dt.readings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Spacy learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "test_sentence = \"Next week I'll   be in Madrid. Maybe.\"\n",
    "doc = nlp(test_sentence)\n",
    "for token in doc:\n",
    "    print(\"{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\\t{6}\\t{7}\".format(\n",
    "        token.text,\n",
    "        token.idx,\n",
    "        token.lemma_,\n",
    "        token.is_punct,\n",
    "        token.is_space,\n",
    "        token.shape_,\n",
    "        token.pos_,\n",
    "        token.tag_\n",
    "    ))\n",
    "    \n",
    "for sent in doc.sents:\n",
    "    print(sent)\n",
    "    \n",
    "print([(token.text, token.tag_) for token in doc])\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    " \n",
    "doc = nlp('I just bought 2 shares at 9 a.m. because the stock went up 30% in just 2 days according to the WSJ')\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# Noun phrases\n",
    "doc = nlp(\"Wall Street Journal just published an interesting piece on crypto currencies\")\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text, chunk.label_, chunk.root.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# Dependency parser\n",
    "doc = nlp('Wall Street Journal just published an interesting piece on crypto currencies')\n",
    " \n",
    "for token in doc:\n",
    "    print(\"{0}/{1} <--{2}-- {3}/{4}\".format(\n",
    "        token.text, token.tag_, token.dep_, token.head.text, token.head.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp('Wall Street Journal just published an interesting piece on crypto currencies')\n",
    "displacy.render(doc, style='dep', jupyter=True, options={'distance': 90})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(test_sentence)\n",
    "displacy.render(doc, style='dep', jupyter=True, options={'distance': 90})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')\n",
    "print(nlp.vocab['banana'].vector)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:mtgnltk]",
   "language": "python",
   "name": "conda-env-mtgnltk-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "203.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "455px",
    "left": "1008px",
    "right": "20px",
    "top": "120px",
    "width": "355px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is the start of the \"trial and error approach\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "- https://github.com/xurxodiz/cardwalker/tree/master/oracle\n",
    "- https://laterna--magica.blogspot.com/2011/10/oracle-parser.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T19:32:04.761042Z",
     "start_time": "2018-08-09T19:32:02.940375Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T19:32:05.349039Z",
     "start_time": "2018-08-09T19:32:04.766040Z"
    }
   },
   "outputs": [],
   "source": [
    "sets = json.load(open('./AllSets.json', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T19:32:05.379043Z",
     "start_time": "2018-08-09T19:32:05.354041Z"
    }
   },
   "outputs": [],
   "source": [
    "for k, v in sorted(sets.items()):\n",
    "    print(k, v['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T19:32:05.501041Z",
     "start_time": "2018-08-09T19:32:05.383041Z"
    }
   },
   "outputs": [],
   "source": [
    "cards_usaga = sets['USG']['cards']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T19:32:05.895868Z",
     "start_time": "2018-08-09T19:32:05.507042Z"
    }
   },
   "outputs": [],
   "source": [
    "cards_usaga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T19:32:05.925881Z",
     "start_time": "2018-08-09T19:32:05.899867Z"
    }
   },
   "outputs": [],
   "source": [
    "cards_all=[]\n",
    "for k, sett in sets.items():\n",
    "    if (k in ['UGL', 'UST']) or (len(k)>3): # Ignore Unglued, Unstable and promotional things\n",
    "        continue\n",
    "    cards_all.extend(sett['cards'])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's start by trying to extract static habilities from cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T19:32:06.288017Z",
     "start_time": "2018-08-09T19:32:05.930866Z"
    }
   },
   "outputs": [],
   "source": [
    "cards_df = pd.DataFrame.from_dict(cards_usaga)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T19:32:06.582151Z",
     "start_time": "2018-08-09T19:32:06.299018Z"
    }
   },
   "outputs": [],
   "source": [
    "texts = [card['text'].replace(card['name'], 'SELF') for card in cards_usaga if 'text' in card.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T19:32:06.736152Z",
     "start_time": "2018-08-09T19:32:06.590154Z"
    }
   },
   "outputs": [],
   "source": [
    "patterns = [\n",
    "    (r'^([A-Za-z]+ ?[A-Za-z]+)[$|\\n|,]| \\(', 'STATICABILITY'),\n",
    "    (r', ([A-Za-z]+ ?[A-Za-z]+)[$|\\n||,]| \\(', 'STATICABILITY'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T19:32:06.863153Z",
     "start_time": "2018-08-09T19:32:06.742170Z"
    }
   },
   "outputs": [],
   "source": [
    "#regexp_tagger = nltk.RegexpTagger(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T19:32:07.129192Z",
     "start_time": "2018-08-09T19:32:06.868151Z"
    }
   },
   "outputs": [],
   "source": [
    "res = defaultdict(list)\n",
    "r=None\n",
    "for text in texts:\n",
    "#     if r: break\n",
    "    for pat, tag in patterns:\n",
    "        r = re.search(pat, text)\n",
    "        if r:\n",
    "            res[text].append((r.groups(), tag))\n",
    "#             break\n",
    "pretty = pd.DataFrame.from_dict(res, orient='index')\n",
    "pretty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DF version: Let's start by trying to extract static habilities from cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T19:32:07.446241Z",
     "start_time": "2018-08-09T19:32:07.132193Z"
    }
   },
   "outputs": [],
   "source": [
    "#cards_df = pd.DataFrame.from_dict(cards_usaga)\n",
    "cards_df = pd.DataFrame.from_dict(cards_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T19:32:08.849722Z",
     "start_time": "2018-08-09T19:32:08.818728Z"
    }
   },
   "outputs": [],
   "source": [
    "cards_df.head(4).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T19:32:09.021732Z",
     "start_time": "2018-08-09T19:32:08.853726Z"
    }
   },
   "outputs": [],
   "source": [
    "#regexp_tagger = nltk.RegexpTagger(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T19:32:09.714169Z",
     "start_time": "2018-08-09T19:32:09.025723Z"
    }
   },
   "outputs": [],
   "source": [
    "patterns_static_abilities = [\n",
    "    (r'^(?!(Oubleday ikestray|Combined|Enchant|Choose|Target))([A-Z][a-z]+ ?[A-za-z]+)[$|\\n|,]| \\(', 'STATICABILITY'),\n",
    "    (r', (?!(Choose|Scheming))([A-Z][a-z]+ ?[A-za-z]+)[$|\\n||,]| \\(', 'STATICABILITY'),\n",
    "]\n",
    "\n",
    "def detect_staticabilities(text):\n",
    "    res = []# defaultdict(list)\n",
    "    r = None\n",
    "    for pat, tag in patterns_static_abilities:\n",
    "        r = re.search(pat, text)\n",
    "        if r:\n",
    "            for group in r.groups():\n",
    "                if group: res.append(group)\n",
    "    \n",
    "    return tuple(res) or pd.np.nan\n",
    "\n",
    "cards_df['static_abilities'] = cards_df['text_preworked'].apply(detect_staticabilities)\n",
    "cards_df.head(10).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T19:32:09.916771Z",
     "start_time": "2018-08-09T19:32:09.723169Z"
    }
   },
   "outputs": [],
   "source": [
    "cards_df.dropna(subset=['static_abilities']).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T19:32:09.928774Z",
     "start_time": "2018-08-09T19:32:09.920772Z"
    }
   },
   "outputs": [],
   "source": [
    "test_phrase = cards_df.iloc[288].text\n",
    "test_phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T19:32:10.160769Z",
     "start_time": "2018-08-09T19:32:09.933771Z"
    }
   },
   "outputs": [],
   "source": [
    "cards_df.dropna(subset=['static_abilities'])['static_abilities'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T19:32:10.274775Z",
     "start_time": "2018-08-09T19:32:10.171770Z"
    }
   },
   "outputs": [],
   "source": [
    "cards_df[cards_df['static_abilities']==('Phasing',)].text.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does parenthesis contain usefull info or only explanations of abilities/effects?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like its always an explanation (so, no usefull info to discern possible targets, zones affected, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T19:32:10.462102Z",
     "start_time": "2018-08-09T19:32:10.277776Z"
    }
   },
   "outputs": [],
   "source": [
    "pattern_parenthesis = r'\\((.*?)\\)'\n",
    "test = \"\"\"('Flying',), ('Trample',), ('Paper',), ('First strike',),\n",
    "       ('Phasing',), ('Haste',), ('Flash',), ('Island',), ('Defender',),\n",
    "       ('Blue',), ('Reach',), ('Devour X',), ('Vigilance',),\n",
    "       ('Double strike',), ('Indestructible',), ('Artifacts',),\n",
    "       ('Deathtouch',), ('Lifelink',), ('Menace',), ('Werewolf',),\n",
    "       ('Leviathans',), ('While voting',), ('Flying', 'Demon'),\n",
    "       ('Islandwalk',), ('Hexproof',), ('Plains',), ('Instant',),\n",
    "       ('Swamp',), ('Mountain',), ('Forest',), ('Dinosaur',),\n",
    "       ('Dinosaur Knight',), ('Leviathan',), ('Simultaneously',),\n",
    "       ('Rat',), ('During combat',), ('Investigate',),\n",
    "       ('Minotaur Pirate',), ('Each noncreature',), ('Vampire',),\n",
    "       ('Pyrogenius',), ('Swampwalk',), ('Bolster X',), ('Timebender',),\n",
    "       ('Bold Pyromancer',), ('Scry X',), ('Desertwalk',), ('Prowess',),\n",
    "       ('Martial Paragon',), ('Death Wielder',), ('Equipment',),\n",
    "       ('Valiant Protector',)\"\"\"\n",
    "a = re.findall(pattern_parenthesis, test)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T19:32:10.882098Z",
     "start_time": "2018-08-09T19:32:10.467102Z"
    }
   },
   "outputs": [],
   "source": [
    "pattern_parenthesis = r'\\((.*?)\\)'\n",
    "cards_df['in_parentheses'] = cards_df['text'].apply(lambda x: tuple(re.findall(pattern_parenthesis, str(x))))\n",
    "set(cards_df['in_parentheses'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T19:32:11.122104Z",
     "start_time": "2018-08-09T19:32:10.887098Z"
    }
   },
   "outputs": [],
   "source": [
    "#This example is not explaning an ability, but it is explaning something (an effect)\n",
    "st = 'If two or more creatures are tied for greatest power, target any one of them.'\n",
    "cards_df[cards_df['text'].str.contains(st).fillna(False)]['text'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove anything between parenthesis and replace name by SELF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T19:57:09.152169Z",
     "start_time": "2018-08-09T19:57:07.847909Z"
    }
   },
   "outputs": [],
   "source": [
    "#Replace name by SELF and remove anything between parethesis\n",
    "pattern_parenthesis = r' ?\\(.*?\\)'\n",
    "cards_df['text_preworked'] = cards_df.apply(lambda x: str(x['text']).replace(x['name'], 'SELF'), axis=1)\n",
    "cards_df['text_preworked'] = cards_df['text_preworked'].apply(lambda x: re.sub(pattern_parenthesis, '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T19:57:11.785348Z",
     "start_time": "2018-08-09T19:57:11.741350Z"
    }
   },
   "outputs": [],
   "source": [
    "cards_df[cards_df['text_preworked'].str.contains('\\(').fillna(False)]['text_preworked']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain specific vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build some domain specific vocabulary for MTG. For example, let's list supertypes, types, subtypes, know all card names, this kind f thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T19:32:11.153104Z",
     "start_time": "2018-08-09T19:32:11.126103Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create set of cards names\n",
    "cards_names = set(cards_df.name.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T19:32:11.284101Z",
     "start_time": "2018-08-09T19:32:11.157099Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create set of supertypes\n",
    "array_of_supertypes_tuples = cards_df['supertypes'].dropna().apply(tuple).unique()\n",
    "cards_supertypes = tuple()\n",
    "for tup in array_of_supertypes_tuples:\n",
    "    cards_supertypes += tup\n",
    "    \n",
    "cards_supertypes = set(cards_supertypes)\n",
    "cards_supertypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T19:32:11.464100Z",
     "start_time": "2018-08-09T19:32:11.290102Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create set of types\n",
    "array_of_types_tuples = cards_df['types'].dropna().apply(tuple).unique()\n",
    "cards_types = tuple()\n",
    "for tup in array_of_types_tuples:\n",
    "    cards_types += tup\n",
    "    \n",
    "cards_types = set(cards_types)\n",
    "cards_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T19:32:11.682100Z",
     "start_time": "2018-08-09T19:32:11.469099Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create set of types\n",
    "array_of_subtypes_tuples = cards_df['subtypes'].dropna().apply(tuple).unique()\n",
    "cards_subtypes = tuple()\n",
    "for tup in array_of_subtypes_tuples:\n",
    "    cards_subtypes += tup\n",
    "    \n",
    "cards_subtypes = set(cards_subtypes)\n",
    "cards_subtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T19:32:11.861099Z",
     "start_time": "2018-08-09T19:32:11.687100Z"
    }
   },
   "outputs": [],
   "source": [
    "cards_df.head(10).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T19:32:13.577133Z",
     "start_time": "2018-08-09T19:32:11.864102Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "r = requests.get('http://media.wizards.com/2018/downloads/MagicCompRules%2020180713.txt')\n",
    "comprules = r.text\n",
    "kw_abilities_pat = r'702\\.\\d+\\. ([A-Za-z ]+)'\n",
    "abilities = re.findall(kw_abilities_pat, comprules)\n",
    "abilities.pop(0) # Its just the rulings \n",
    "abilities.sort()\n",
    "abilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can we detect an abilities sentence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T20:22:08.925716Z",
     "start_time": "2018-08-09T20:22:08.866688Z"
    }
   },
   "outputs": [],
   "source": [
    "df = cards_df\n",
    "df['split_sentences'] = df['text_preworked'].apply(lambda x: x.split('\\n'))\n",
    "df['split_sentences']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, how to work with abilites followed by cost?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_abilities_sentence(sentlist):\n",
    "    for sent in sentlist:\n",
    "        if set(sent.split(', ')).issubset(set(abilities)):\n",
    "            return True\n",
    "    return False\n",
    "t = df['split_sentences'].apply(detect_abilities_sentence)\n",
    "df[t][df['text'].str.contains('umulative upkeep').fillna(False)]['text_preworked']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T20:29:23.712040Z",
     "start_time": "2018-08-09T20:29:23.662038Z"
    }
   },
   "outputs": [],
   "source": [
    "df[df['text'].str.contains('umulative upkeep').fillna(False)]['text_preworked'].loc[30808]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like, if followed by mana cost, cumulative upkeep COST may be followed by , (comma) or \\n (newline). But if the text for cumulative upkeep is longer, it seems to end with \\n everytime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-08-09T20:36:37.062Z"
    }
   },
   "outputs": [],
   "source": [
    "#re.search('test', 'TeSt', re.IGNORECASE)\n",
    "#re.match('test', 'TeSt', re.IGNORECASE)\n",
    "#re.sub('test', 'xxxx', 'Testing', flags=re.IGNORECASE)\n",
    "\n",
    "cumulative_upkeep_pattern = r'cumulative upkeep'\n",
    "t = df['text'].apply(lambda x: re.findall(cumulative_upkeep_pattern, str(x), re.IGNORECASE))\n",
    "t = t.replace([], pd.np.nan)\n",
    "t.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T18:32:55.267459Z",
     "start_time": "2018-08-09T18:32:55.255467Z"
    }
   },
   "outputs": [],
   "source": [
    "test_sentence = cards_df[cards_df['static_abilities']==('Phasing',)].text.values[0]\n",
    "test_sentence = test_sentence +'\\nWhenever SELF attacks, it gets +1/+1.'\n",
    "test_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T18:33:06.600701Z",
     "start_time": "2018-08-09T18:32:55.271463Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp('Hello World!')\n",
    "for token in doc:\n",
    "    print('\"' + token.text + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T18:33:06.644700Z",
     "start_time": "2018-08-09T18:33:06.604700Z"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp('Hello     World!')\n",
    "for token in doc:\n",
    "    print('\"' + token.text + '\"', token.idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T18:33:06.854699Z",
     "start_time": "2018-08-09T18:33:06.648699Z"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(\"Next week I'll   be in Madrid.\")\n",
    "for token in doc:\n",
    "    print(\"{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\\t{6}\\t{7}\".format(\n",
    "        token.text,\n",
    "        token.idx,\n",
    "        token.lemma_,\n",
    "        token.is_punct,\n",
    "        token.is_space,\n",
    "        token.shape_,\n",
    "        token.pos_,\n",
    "        token.tag_\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T18:33:07.041699Z",
     "start_time": "2018-08-09T18:33:06.858699Z"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(test_sentence)\n",
    "for token in doc:\n",
    "    print(\"{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\\t{6}\\t{7}\".format(\n",
    "        token.text,\n",
    "        token.idx,\n",
    "        token.lemma_,\n",
    "        token.is_punct,\n",
    "        token.is_space,\n",
    "        token.shape_,\n",
    "        token.pos_,\n",
    "        token.tag_\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T18:33:07.082695Z",
     "start_time": "2018-08-09T18:33:07.045697Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sentence detection\n",
    "doc = nlp(\"These are apples. These are oranges.\")\n",
    " \n",
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T18:33:07.179696Z",
     "start_time": "2018-08-09T18:33:07.086695Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sentence detection\n",
    "doc = nlp(test_sentence)\n",
    " \n",
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T18:33:07.389693Z",
     "start_time": "2018-08-09T18:33:07.183695Z"
    }
   },
   "outputs": [],
   "source": [
    "# POS tagging\n",
    "doc = nlp(test_sentence)\n",
    "print([(token.text, token.tag_) for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T18:33:07.486695Z",
     "start_time": "2018-08-09T18:33:07.393703Z"
    }
   },
   "outputs": [],
   "source": [
    "# NER named entity recognition\n",
    "doc = nlp(\"Next week I'll be in Madrid.\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T18:33:07.569694Z",
     "start_time": "2018-08-09T18:33:07.489693Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.chunk import conlltags2tree\n",
    " \n",
    "doc = nlp(\"Next week I'll be in Madrid.\")\n",
    "iob_tagged = [\n",
    "    (\n",
    "        token.text, \n",
    "        token.tag_, \n",
    "        \"{0}-{1}\".format(token.ent_iob_, token.ent_type_) if token.ent_iob_ != 'O' else token.ent_iob_\n",
    "    ) for token in doc\n",
    "]\n",
    " \n",
    "print(iob_tagged)\n",
    " \n",
    "# In case you like the nltk.Tree format\n",
    "print(conlltags2tree(iob_tagged))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T18:33:07.684692Z",
     "start_time": "2018-08-09T18:33:07.573693Z"
    }
   },
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    " \n",
    "doc = nlp('I just bought 2 shares at 9 a.m. because the stock went up 30% in just 2 days according to the WSJ')\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T18:33:07.801691Z",
     "start_time": "2018-08-09T18:33:07.688693Z"
    }
   },
   "outputs": [],
   "source": [
    "# Noun phrases\n",
    "doc = nlp(\"Wall Street Journal just published an interesting piece on crypto currencies\")\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text, chunk.label_, chunk.root.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T18:33:07.950695Z",
     "start_time": "2018-08-09T18:33:07.805693Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dependency parser\n",
    "doc = nlp('Wall Street Journal just published an interesting piece on crypto currencies')\n",
    " \n",
    "for token in doc:\n",
    "    print(\"{0}/{1} <--{2}-- {3}/{4}\".format(\n",
    "        token.text, token.tag_, token.dep_, token.head.text, token.head.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T18:33:08.071693Z",
     "start_time": "2018-08-09T18:33:07.954692Z"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp('Wall Street Journal just published an interesting piece on crypto currencies')\n",
    "displacy.render(doc, style='dep', jupyter=True, options={'distance': 90})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T18:33:08.185694Z",
     "start_time": "2018-08-09T18:33:08.075694Z"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(test_sentence)\n",
    "displacy.render(doc, style='dep', jupyter=True, options={'distance': 90})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T18:33:31.112580Z",
     "start_time": "2018-08-09T18:33:08.189693Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')\n",
    "print(nlp.vocab['banana'].vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-08-09T18:32:09.473Z"
    }
   },
   "outputs": [],
   "source": [
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-08-09T18:32:09.481Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://www.nltk.org/book/ch10.html section 5.2\n",
    "dt = nltk.DiscourseTester(['A student dances', 'Every student is a person'])\n",
    "dt.readings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-08-09T18:32:09.487Z"
    }
   },
   "outputs": [],
   "source": [
    "dt.add_sentence('No person dances', consistchk=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-08-09T18:32:09.491Z"
    }
   },
   "outputs": [],
   "source": [
    "dt.retract_sentence('No person dances', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-08-09T18:32:09.496Z"
    }
   },
   "outputs": [],
   "source": [
    "dt.add_sentence('A person dances', informchk=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T18:11:26.607137Z",
     "start_time": "2018-08-09T18:10:29.249Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tag import RegexpTagger\n",
    "tagger = RegexpTagger(\n",
    "    [('^(chases|runs)$', 'VB'),\n",
    "     ('^(a)$', 'ex_quant'),\n",
    "     ('^(every)$', 'univ_quant'),\n",
    "     ('^(dog|boy)$', 'NN'),\n",
    "     ('^(He)$', 'PRP')\n",
    "])\n",
    "rc = nltk.DrtGlueReadingCommand(depparser=nltk.MaltParser(tagger=tagger))\n",
    "dt = nltk.DiscourseTester(['Every dog chases a boy', 'He runs'], rc)\n",
    "dt.readings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-08-09T18:32:09.503Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "import re\n",
    "from spacy.symbols import ORTH, LEMMA, POS, TAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-08-09T18:32:09.510Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#MODEL = r'C:\\Users\\cs294662\\Downloads\\programas\\spacy\\data\\en_core_web_md-2.0.0\\en_core_web_md\\en_core_web_md-2.0.0'\n",
    "#MODEL = r'C:\\Users\\cs294662\\Downloads\\programas\\spacy\\data\\en_coref_lg-3.0.0\\en_coref_lg\\en_coref_lg-3.0.0'\n",
    "MODEL = 'en_core_web_lg'\n",
    "nlp = spacy.load(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-08-09T18:32:09.517Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/44594759/spacy-adding-special-case-tokenization-rules-by-regular-expression-or-pattern\n",
    "cost_pattern = r'{[\\dWGBURTX]}'\n",
    "#cost_pattern = re.compile(r'{[\\dWGBURTX]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-08-09T18:32:09.524Z"
    }
   },
   "outputs": [],
   "source": [
    "# add special case rule\n",
    "#special_case = [{ORTH: cost_pattern, LEMMA: 'COST', POS: 'NOUN'}]\n",
    "#nlp.tokenizer.add_special_case(cost_pattern, special_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-08-09T18:32:09.531Z"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(test_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-08-09T18:32:09.536Z"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(test_phrase)\n",
    "indexes = [m.span() for m in re.finditer(cost_pattern, test_phrase, flags=re.IGNORECASE)] +\\\n",
    "          [m.span() for m in re.finditer(r':', test_phrase, flags=re.IGNORECASE)]\n",
    "for start,end in indexes:\n",
    "    doc.merge(start_idx=start,end_idx=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-08-09T18:32:09.545Z"
    }
   },
   "outputs": [],
   "source": [
    "for s in doc.sents:\n",
    "    print(s)\n",
    "    print('Change')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-08-09T18:32:09.551Z"
    }
   },
   "outputs": [],
   "source": [
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-08-09T18:32:09.559Z"
    }
   },
   "outputs": [],
   "source": [
    "displacy.render(doc, style='dep', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-08-09T18:32:09.565Z"
    }
   },
   "outputs": [],
   "source": [
    "sents = []\n",
    "for sent in doc.sents:\n",
    "    print(sent)\n",
    "    for tr in sent.subtree:\n",
    "        sentd = {\n",
    "            'word': tr,\n",
    "            'ancestors': [x for x in tr.ancestors],\n",
    "            'children': [x for x in tr.children],\n",
    "            'cluster': tr.cluster,\n",
    "            'conjuncts': [x for x in tr.conjuncts],\n",
    "            'dep': tr.dep_,\n",
    "            'ent_type': tr.ent_type_,\n",
    "            'head': tr.head,\n",
    "            'lemma': tr.lemma_,\n",
    "            'tag':tr.tag_\n",
    "        }\n",
    "        sents.append(sentd)\n",
    "        #print(sentd)\n",
    "        #print('\\n')\n",
    "df = pd.DataFrame(sents)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T14:31:59.613539Z",
     "start_time": "2018-08-07T14:31:59.597539Z"
    }
   },
   "source": [
    "## Try to match types and set as entity\n",
    "https://stackoverflow.com/questions/49097804/spacy-entity-from-phrasematcher-only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-08-09T18:32:09.573Z"
    }
   },
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher, Matcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "class EntityPhraseMatcher(object):\n",
    "    name = 'entity_phrase_matcher'\n",
    "\n",
    "    def __init__(self, nlp, terms, label):\n",
    "        patterns = [nlp(term) for term in terms]\n",
    "        self.matcher = PhraseMatcher(nlp.vocab)\n",
    "        self.matcher.add(label, None, *patterns)\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        matches = self.matcher(doc)\n",
    "        spans = []\n",
    "        for label, start, end in matches:\n",
    "            span = Span(doc, start, end, label=label)\n",
    "            spans.append(span)\n",
    "        doc.ents = spans\n",
    "        return doc\n",
    "    \n",
    "class EntityMatcher(object):\n",
    "    name = 'entity_matcher'\n",
    "\n",
    "    def __init__(self, nlp, dict_label_terms):\n",
    "        '''dict_label_terms shoould be a dictionary in the format\n",
    "        {label(str): patterns(list)}'''\n",
    "        self.matcher = Matcher(nlp.vocab)\n",
    "        for label, patterns in dict_label_terms.items():\n",
    "            self.matcher.add(label, None, *patterns)\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        matches = self.matcher(doc)\n",
    "        spans = []\n",
    "        for label, start, end in matches:\n",
    "            span = Span(doc, start, end, label=label)\n",
    "            spans.append(span)\n",
    "        doc.ents = spans\n",
    "        return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-08-09T18:32:09.581Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "#nlp.remove_pipe('ner')\n",
    "#nlp.remove_pipe('entity_matcher')\n",
    "#nlp.remove_pipe('ent_type_matcher')\n",
    "#nlp.remove_pipe('ent_subtype_matcher')\n",
    "#nlp.remove_pipe('ent_supertype_matcher')\n",
    "\n",
    "dict_label_terms = defaultdict(list)\n",
    "\n",
    "for lem in ['if', 'whenever', 'when', 'only']:\n",
    "    condition_matcher = [{'LEMMA': lem}, {'IS_PUNCT': False, 'OP': '*'}, {'IS_PUNCT': True}]\n",
    "    dict_label_terms['CONDITION'].append(condition_matcher)\n",
    "\n",
    "for typ in cards_types:\n",
    "    dict_label_terms['TYPE'].append([{'LOWER': t} for t in typ.lower().split()])\n",
    "for typ in cards_subtypes:\n",
    "    dict_label_terms['SUBTYPE'].append([{'LOWER': t} for t in typ.lower().split()])\n",
    "for typ in cards_supertypes:\n",
    "    dict_label_terms['SUPERTYPE'].append([{'LOWER': t} for t in typ.lower().split()])\n",
    "for typ in ['white','black','blue','white','red','colorless', 'multicolored', 'multicolor']:\n",
    "    dict_label_terms['COLOR'].append([{'LOWER': t} for t in typ.lower().split()])\n",
    "\n",
    "entity_matcher = EntityMatcher(nlp, dict_label_terms)\n",
    "nlp.add_pipe(entity_matcher)\n",
    "\n",
    "print(nlp.pipe_names)  # see all components in the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-08-09T18:32:09.585Z"
    }
   },
   "outputs": [],
   "source": [
    "test_sents = []\n",
    "test_sents.append(test_phrase)\n",
    "test_sents.append('If a Sliver deals combat damage to a player, its controller may create a +1/+1 colorless Sliver creature token.')\n",
    "test_sents.append('Whenever a Sliver deals combat damage to a player, its controller may create a +1/+1 colorless Sliver creature token.')\n",
    "colorless = '\\n'.join([x for x in cards_df[cards_df['text'].str.contains('colorless').fillna(False)]['text'].iloc[:5]])\n",
    "test_sents.append(colorless)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-08-09T18:32:09.592Z"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp('\\n'.join(test_sents))\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-08-09T18:32:09.598Z"
    }
   },
   "outputs": [],
   "source": [
    "options = {'compact': False,\n",
    "          'collapse_punct': False}\n",
    "displacy.render(doc, style='dep', jupyter=True, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-08-09T18:32:09.607Z"
    }
   },
   "outputs": [],
   "source": [
    "sents = []\n",
    "for sent in doc.sents:\n",
    "    print(sent)\n",
    "    for tr in sent.subtree:\n",
    "        sentd = {\n",
    "            'word': tr,\n",
    "            'ancestors': [x for x in tr.ancestors],\n",
    "            'children': [x for x in tr.children],\n",
    "            'cluster': tr.cluster,\n",
    "            'conjuncts': [x for x in tr.conjuncts],\n",
    "            'dep': tr.dep_,\n",
    "            'ent_type': tr.ent_type_,\n",
    "            'head': tr.head,\n",
    "            'lemma': tr.lemma_,\n",
    "            'tag':tr.tag_\n",
    "        }\n",
    "        sents.append(sentd)\n",
    "        #print(sentd)\n",
    "        #print('\\n')\n",
    "df = pd.DataFrame(sents)\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-08-09T18:32:09.612Z"
    }
   },
   "outputs": [],
   "source": [
    "df[df['word'].apply(lambda x: x.lower_ in ['whenever', 'if', 'only'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Should we train a model for POSTAGGING?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not sure. Many verbs interpreted sometimes as nouns are also sometimes interpreted as verbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-08-09T18:32:09.620Z"
    }
   },
   "outputs": [],
   "source": [
    "sents = '\\n'.join([x for x in cards_df.sample(200)['text_preworked']])\n",
    "doc = nlp(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-08-09T18:32:09.628Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nouns = []\n",
    "for token in doc:\n",
    "    if token.pos_ == 'NOUN' and token.lower_ not in nouns:\n",
    "        nouns.append(token.lower_)\n",
    "nouns.sort()\n",
    "nouns\n",
    "# Nouns that should be verbs:\n",
    "# 'attacks', 'block', 'blocks', 'cast', 'control','controls', 'deal','deals', 'dies', 'enchant', 'flip', 'gain', 'gains', 'pay', 'return', 'sacrifice', 'shares', 'tap', 'untap'\n",
    "\n",
    "# Nouns that COULD be verbs:\n",
    "# 'counter(S)','exile'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-08-09T18:32:09.633Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "verbs = []\n",
    "for token in doc:\n",
    "    if token.pos_ == 'VERB' and token.lower_ not in verbs:\n",
    "        verbs.append(token.lower_)\n",
    "verbs.sort()\n",
    "verbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get predictions ins a format easy to correct and feed back as training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check here https://spacy.io/usage/training#training-simple-style.\n",
    "\n",
    "It should be easy to train a model, as long as we have a fre things in place"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build tables like:\n",
    "card | sentence | token0 | token1 | ... | tokenN\n",
    "card | sentence | tag0 | tag1 | ... | tagN\n",
    "card | sentence | deps0 | deps1 | ... | depsN\n",
    "card | sentence | head0 | head1 | ... | headN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-08-09T18:32:09.643Z"
    }
   },
   "outputs": [],
   "source": [
    "cards_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-08-09T18:32:09.648Z"
    }
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "tokens = []\n",
    "tags = []\n",
    "deps = []\n",
    "head_ids = []\n",
    "card_counter=0\n",
    "for idx, card in cards_df.sample(200).iterrows():\n",
    "    card_counter+=1\n",
    "    if not card_counter%40: print(card_counter)\n",
    "    for sentence in card['text_preworked'].split('\\n'):\n",
    "        doc = nlp(sentence)\n",
    "        basics = {\n",
    "                'card': card['name'],\n",
    "                'sentence': sentence,\n",
    "            }\n",
    "        toks, tag, dep, head = deepcopy(basics), deepcopy(basics), deepcopy(basics), deepcopy(basics)\n",
    "        for i, tok in enumerate(doc):\n",
    "            toks.update({'{0:04d}'.format(i): tok.text})\n",
    "            tag.update({'{0:04d}'.format(i): tok.tag_})\n",
    "            dep.update({'{0:04d}'.format(i): tok.dep_})\n",
    "            head.update({'{0:04d}'.format(i): tok.head.i})\n",
    "        tokens.append(toks)\n",
    "        tags.append(tag)\n",
    "        deps.append(dep)\n",
    "        head_ids.append(head)\n",
    "            \n",
    "df_tokens = pd.DataFrame(tokens)\n",
    "df_tags = pd.DataFrame(tags)\n",
    "df_deps = pd.DataFrame(deps)\n",
    "df_head_ids = pd.DataFrame(head_ids)\n",
    "\n",
    "display(df_tokens.head(2), df_tags.head(2), df_deps.head(2), df_head_ids.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:mtgnltk]",
   "language": "python",
   "name": "conda-env-mtgnltk-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "210.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "455px",
    "left": "1008px",
    "right": "20px",
    "top": "120px",
    "width": "355px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

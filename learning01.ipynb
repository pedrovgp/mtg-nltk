{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is the start of the \"trial and error approach\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "- https://github.com/xurxodiz/cardwalker/tree/master/oracle\n",
    "- https://laterna--magica.blogspot.com/2011/10/oracle-parser.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets = json.load(open('./AllSets.json', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in sorted(sets.items()):\n",
    "    print(k, v['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cards_usaga = sets['USG']['cards']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cards_usaga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cards_all=[]\n",
    "for k, sett in sets.items():\n",
    "    if (k in ['UGL', 'UST']) or (len(k)>3): # Ignore Unglued, Unstable and promotional things\n",
    "        continue\n",
    "    cards_all.extend(sett['cards'])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's start by trying to extract static habilities from cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [card['text'].replace(card['name'], 'SELF') for card in cards_usaga if 'text' in card.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [\n",
    "    (r'^([A-Za-z]+ ?[A-Za-z]+)[$|\\n|,]| \\(', 'STATICABILITY'),\n",
    "    (r', ([A-Za-z]+ ?[A-Za-z]+)[$|\\n||,]| \\(', 'STATICABILITY'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regexp_tagger = nltk.RegexpTagger(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = defaultdict(list)\n",
    "r=None\n",
    "for text in texts:\n",
    "#     if r: break\n",
    "    for pat, tag in patterns:\n",
    "        r = re.search(pat, text)\n",
    "        if r:\n",
    "            res[text].append((r.groups(), tag))\n",
    "#             break\n",
    "pretty = pd.DataFrame.from_dict(res, orient='index')\n",
    "pretty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DF version: Let's start by trying to extract static habilities from cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cards_df = pd.DataFrame.from_dict(cards_usaga)\n",
    "cards_df = pd.DataFrame.from_dict(cards_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cards_df['text_preworked'] = cards_df.apply(lambda x: str(x['text']).replace(x['name'], 'SELF'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cards_df.head(4).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regexp_tagger = nltk.RegexpTagger(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns_static_abilities = [\n",
    "    (r'^(?!(Oubleday ikestray|Combined|Enchant|Choose|Target))([A-Z][a-z]+ ?[A-za-z]+)[$|\\n|,]| \\(', 'STATICABILITY'),\n",
    "    (r', (?!(Choose|Scheming))([A-Z][a-z]+ ?[A-za-z]+)[$|\\n||,]| \\(', 'STATICABILITY'),\n",
    "]\n",
    "\n",
    "def detect_staticabilities(text):\n",
    "    res = []# defaultdict(list)\n",
    "    r = None\n",
    "    for pat, tag in patterns_static_abilities:\n",
    "        r = re.search(pat, text)\n",
    "        if r:\n",
    "            for group in r.groups():\n",
    "                if group: res.append(group)\n",
    "    \n",
    "    return tuple(res) or pd.np.nan\n",
    "\n",
    "cards_df['static_abilities'] = cards_df['text_preworked'].apply(detect_staticabilities)\n",
    "cards_df.head(10).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cards_df.dropna(subset=['static_abilities'])['static_abilities'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cards_df[cards_df['static_abilities']==('Phasing',)].text.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain specific vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build some domain specific vocabulary for MTG. For example, let's list supertypes, types, subtypes, know all card names, this kind f thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create set of cards names\n",
    "cards_names = set(cards_df.name.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create set of supertypes\n",
    "array_of_supertypes_tuples = cards_df['supertypes'].dropna().apply(tuple).unique()\n",
    "cards_supertypes = tuple()\n",
    "for tup in array_of_supertypes_tuples:\n",
    "    cards_supertypes += tup\n",
    "    \n",
    "cards_supertypes = set(cards_supertypes)\n",
    "cards_supertypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create set of types\n",
    "array_of_types_tuples = cards_df['types'].dropna().apply(tuple).unique()\n",
    "cards_types = tuple()\n",
    "for tup in array_of_types_tuples:\n",
    "    cards_types += tup\n",
    "    \n",
    "cards_types = set(cards_types)\n",
    "cards_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create set of types\n",
    "array_of_subtypes_tuples = cards_df['subtypes'].dropna().apply(tuple).unique()\n",
    "cards_subtypes = tuple()\n",
    "for tup in array_of_subtypes_tuples:\n",
    "    cards_subtypes += tup\n",
    "    \n",
    "cards_subtypes = set(cards_subtypes)\n",
    "cards_subtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cards_df.head(10).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "r = requests.get('http://media.wizards.com/2018/downloads/MagicCompRules%2020180713.txt')\n",
    "comprules = r.text\n",
    "kw_abilities_pat = r'702\\.\\d+\\. ([A-Za-z ]+)'\n",
    "abilities = re.findall(kw_abilities_pat, comprules)\n",
    "abilities.pop(0) # Its just the rulings \n",
    "abilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = cards_df[cards_df['static_abilities']==('Phasing',)].text.values[0]\n",
    "test_sentence = test_sentence +'\\nWhenever SELF attacks, it gets +1/+1.'\n",
    "test_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp('Hello World!')\n",
    "for token in doc:\n",
    "    print('\"' + token.text + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('Hello     World!')\n",
    "for token in doc:\n",
    "    print('\"' + token.text + '\"', token.idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Next week I'll   be in Madrid.\")\n",
    "for token in doc:\n",
    "    print(\"{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\\t{6}\\t{7}\".format(\n",
    "        token.text,\n",
    "        token.idx,\n",
    "        token.lemma_,\n",
    "        token.is_punct,\n",
    "        token.is_space,\n",
    "        token.shape_,\n",
    "        token.pos_,\n",
    "        token.tag_\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(test_sentence)\n",
    "for token in doc:\n",
    "    print(\"{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\\t{6}\\t{7}\".format(\n",
    "        token.text,\n",
    "        token.idx,\n",
    "        token.lemma_,\n",
    "        token.is_punct,\n",
    "        token.is_space,\n",
    "        token.shape_,\n",
    "        token.pos_,\n",
    "        token.tag_\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence detection\n",
    "doc = nlp(\"These are apples. These are oranges.\")\n",
    " \n",
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence detection\n",
    "doc = nlp(test_sentence)\n",
    " \n",
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tagging\n",
    "doc = nlp(test_sentence)\n",
    "print([(token.text, token.tag_) for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NER named entity recognition\n",
    "doc = nlp(\"Next week I'll be in Madrid.\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.chunk import conlltags2tree\n",
    " \n",
    "doc = nlp(\"Next week I'll be in Madrid.\")\n",
    "iob_tagged = [\n",
    "    (\n",
    "        token.text, \n",
    "        token.tag_, \n",
    "        \"{0}-{1}\".format(token.ent_iob_, token.ent_type_) if token.ent_iob_ != 'O' else token.ent_iob_\n",
    "    ) for token in doc\n",
    "]\n",
    " \n",
    "print(iob_tagged)\n",
    " \n",
    "# In case you like the nltk.Tree format\n",
    "print(conlltags2tree(iob_tagged))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    " \n",
    "doc = nlp('I just bought 2 shares at 9 a.m. because the stock went up 30% in just 2 days according to the WSJ')\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noun phrases\n",
    "doc = nlp(\"Wall Street Journal just published an interesting piece on crypto currencies\")\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text, chunk.label_, chunk.root.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependency parser\n",
    "doc = nlp('Wall Street Journal just published an interesting piece on crypto currencies')\n",
    " \n",
    "for token in doc:\n",
    "    print(\"{0}/{1} <--{2}-- {3}/{4}\".format(\n",
    "        token.text, token.tag_, token.dep_, token.head.text, token.head.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('Wall Street Journal just published an interesting piece on crypto currencies')\n",
    "displacy.render(doc, style='dep', jupyter=True, options={'distance': 90})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(test_sentence)\n",
    "displacy.render(doc, style='dep', jupyter=True, options={'distance': 90})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')\n",
    "print(nlp.vocab['banana'].vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.nltk.org/book/ch10.html section 5.2\n",
    "dt = nltk.DiscourseTester(['A student dances', 'Every student is a person'])\n",
    "dt.readings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.add_sentence('No person dances', consistchk=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.retract_sentence('No person dances', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.add_sentence('A person dances', informchk=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import RegexpTagger\n",
    "tagger = RegexpTagger(\n",
    "    [('^(chases|runs)$', 'VB'),\n",
    "     ('^(a)$', 'ex_quant'),\n",
    "     ('^(every)$', 'univ_quant'),\n",
    "     ('^(dog|boy)$', 'NN'),\n",
    "     ('^(He)$', 'PRP')\n",
    "])\n",
    "rc = nltk.DrtGlueReadingCommand(depparser=nltk.MaltParser(tagger=tagger))\n",
    "dt = nltk.DiscourseTester(['Every dog chases a boy', 'He runs'], rc)\n",
    "dt.readings()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

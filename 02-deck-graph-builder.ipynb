{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build graph for card ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea here is to\n",
    "\n",
    "1. Load the previously ETLelled outgoing and incoming nodes and edges (as dataframes)\n",
    "2. Build out and incoming graphs for each card id.\n",
    "3. Maybe build a composed graph (in and out) for each card_id\n",
    "\n",
    "Next:\n",
    "4. Next, we should work with graphs for a specific deck\n",
    "\n",
    "At the end, store it in a pickle to avoid parsing everything again next time, which takes a long time.\n",
    "\n",
    "**DESIRED RESULT**:\n",
    "result = {card_id1: {out: graph_from_text, in: graph_from_attributes},\n",
    "          card_id2: {out: graph_from_text, in: graph_from_attributes}\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('postgresql+psycopg2://mtg:mtg@localhost:5432/mtg')\n",
    "engine.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataframe of cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_nodes = pd.read_pickle('./pickles/cards_outgoing_nodes.pkl')\n",
    "out_edges = pd.read_pickle('./pickles/cards_outgoing_edges.pkl')\n",
    "in_nodes = pd.read_pickle('./pickles/cards_incoming_nodes.pkl')\n",
    "in_edges = pd.read_pickle('./pickles/cards_incoming_edges.pkl')\n",
    "# cards_df = cards_df.sample(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_out_nodes  = out_nodes[out_nodes['type']=='entity']\n",
    "ent_in_nodes = in_nodes[in_nodes['type']=='entity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is no need to build a graph for the same named card twice\n",
    "unique_cards = out_nodes[out_nodes['type']=='card'].drop_duplicates(subset=['card_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_to_process = unique_cards['card_id']#.sample(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helping functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Split dataframelist\n",
    "import collections\n",
    "def splitDataFrameList(df,target_column,separator=None):\n",
    "    '''\n",
    "    https://gist.github.com/jlln/338b4b0b55bd6984f883\n",
    "    df = dataframe to split,\n",
    "    target_column = the column containing the values to split\n",
    "    separator = the symbol used to perform the split\n",
    "    returns: a dataframe with each entry for the target column separated, with each element moved into a new row. \n",
    "    The values in the other columns are duplicated across the newly divided rows.\n",
    "    '''\n",
    "    def splitListToRows(row,row_accumulator,target_column,separator):\n",
    "        split_row = row[target_column]#.split(separator)\n",
    "        if isinstance(split_row, collections.Iterable):\n",
    "            for s in split_row:\n",
    "                new_row = row.to_dict()\n",
    "                new_row[target_column] = s\n",
    "                row_accumulator.append(new_row)\n",
    "        else:\n",
    "            new_row = row.to_dict()\n",
    "            new_row[target_column] = pd.np.nan\n",
    "            row_accumulator.append(new_row)\n",
    "    new_rows = []\n",
    "    df.apply(splitListToRows, axis=1, args=(new_rows,target_column,separator))\n",
    "    new_df = pd.DataFrame(new_rows)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Create hashable dict\n",
    "from collections import OrderedDict\n",
    "import hashlib\n",
    "class HashableDict(OrderedDict):\n",
    "    def __hash__(self):\n",
    "        return hash(tuple(sorted(self.items())))\n",
    "    \n",
    "    def hexdigext(self):\n",
    "        return hashlib.sha256(''.join([str(k)+str(v) for k, v in self.items()]).encode()).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Make defaultdict which depends on its key\n",
    "# Source: https://www.reddit.com/r/Python/comments/27crqg/making_defaultdict_create_defaults_that_are_a/\n",
    "from collections import defaultdict\n",
    "class key_dependent_dict(defaultdict):\n",
    "    def __init__(self, f_of_x):\n",
    "        super().__init__(None) # base class doesn't get a factory\n",
    "        self.f_of_x = f_of_x # save f(x)\n",
    "    def __missing__(self, key): # called when a default needed\n",
    "        ret = self.f_of_x(key) # calculate default value\n",
    "        self[key] = ret # and install it in the dict\n",
    "        return ret\n",
    "    \n",
    "def entity_key_hash(key):\n",
    "    return HashableDict({'entity': key}).hexdigext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# function to draw a graph to png\n",
    "shapes = ['box', 'polygon', 'ellipse', 'oval', 'circle', 'egg', 'triangle', 'exagon', 'star']\n",
    "colors = ['blue', 'black', 'red', '#db8625', 'green', 'gray', 'cyan', '#ed125b']\n",
    "styles = ['filled', 'rounded', 'rounded, filled', 'dashed', 'dotted, bold']\n",
    "\n",
    "entities_colors = {\n",
    "    'PLAYER': '#FF6E6E',\n",
    "    'ZONE': '#F5D300',\n",
    "    'ACTION': '#1ADA00',\n",
    "    'MANA': '#00DA84',\n",
    "    'SUBTYPE': '#0DE5E5',\n",
    "    'TYPE': '#0513F0',\n",
    "    'SUPERTYPE': '#8D0BCA',\n",
    "    'ABILITY': '#cc3300',\n",
    "    'COLOR': '#666633',\n",
    "    'STEP': '#E0E0F8'\n",
    "}\n",
    "\n",
    "def draw_graph(G, filename='test.png'):\n",
    "    pdot = nx.drawing.nx_pydot.to_pydot(G)\n",
    "\n",
    "\n",
    "    for i, node in enumerate(pdot.get_nodes()):\n",
    "        attrs = node.get_attributes()\n",
    "        node.set_label(str(attrs.get('label', 'none')))\n",
    "    #     node.set_fontcolor(colors[random.randrange(len(colors))])\n",
    "        entity_node_ent_type = attrs.get('entity_node_ent_type', pd.np.nan)\n",
    "        if not pd.isnull(entity_node_ent_type):\n",
    "            color = entities_colors[entity_node_ent_type.strip('\"')]\n",
    "            node.set_fillcolor(color)\n",
    "            node.set_color(color)\n",
    "            node.set_shape('hexagon')\n",
    "            #node.set_colorscheme()\n",
    "            node.set_style('filled')\n",
    "        \n",
    "        node_type = attrs.get('type', None)\n",
    "        if node_type == '\"card\"':\n",
    "            color = '#999966'\n",
    "            node.set_fillcolor(color)\n",
    "#             node.set_color(color)\n",
    "            node.set_shape('star')\n",
    "            #node.set_colorscheme()\n",
    "            node.set_style('filled')\n",
    "    #     \n",
    "        #pass\n",
    "\n",
    "    for i, edge in enumerate(pdot.get_edges()):\n",
    "        att = edge.get_attributes()\n",
    "        att = att.get('label', 'NO-LABEL')\n",
    "        edge.set_label(att)\n",
    "    #     edge.set_fontcolor(colors[random.randrange(len(colors))])\n",
    "    #     edge.set_style(styles[random.randrange(len(styles))])\n",
    "    #     edge.set_color(colors[random.randrange(len(colors))])\n",
    "\n",
    "    png_path = filename\n",
    "    pdot.write_png(png_path)\n",
    "\n",
    "    from IPython.display import Image\n",
    "    return Image(png_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build graph with Networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminate_and_wrap_in_quotes(text):\n",
    "    return '\"'+str(text).replace('\"', '')+'\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build out nodes and edges for all ids\n",
    "result = {}\n",
    "cards_graph_dir = './pickles/card_graphs/'\n",
    "import os.path\n",
    "\n",
    "for i, card_id in enumerate(ids_to_process):\n",
    "        \n",
    "    path_to_graph_file = cards_graph_dir+card_id\n",
    "    if os.path.isfile(path_to_graph_file):\n",
    "    #if i < 15890:\n",
    "        continue\n",
    "        \n",
    "    result[card_id] = {}\n",
    "    if not i%100:\n",
    "        clear_output()\n",
    "    else:\n",
    "        if not i%10:\n",
    "            print('{0}/{1} cards processed'.format(i, ids_to_process.shape[0]))\n",
    "    \n",
    "    # Card nodes\n",
    "    #card_0_nodes = out_nodes[(out_nodes['card_id']==card_id)\n",
    "    #                        |(out_nodes['type']=='entity')]\n",
    "    card_0_nodes = out_nodes[(out_nodes['card_id']==card_id)]\n",
    "    card_0_edges = out_edges[(out_edges['source'].isin(card_0_nodes['node_id']))\n",
    "                            |(out_edges['target'].isin(card_0_nodes['node_id']))]\n",
    "    \n",
    "    # Relevant entity nodes\n",
    "    ent_0_nodes = ent_out_nodes[ent_out_nodes['node_id'].isin(card_0_edges['source'])\n",
    "                                 |ent_out_nodes['node_id'].isin(card_0_edges['target'])]\n",
    "\n",
    "    card_0_nodes = pd.concat([card_0_nodes, ent_0_nodes], sort=False)\n",
    "    \n",
    "    #result[card_id] = {'nodes': card_0_nodes.copy(), 'edges': card_0_edges.copy()}\n",
    "    \n",
    "    # Build graph\n",
    "    edge_attr = [x for x in card_0_edges.columns if not x in ['source', 'target']]\n",
    "    G = nx.from_pandas_edgelist(card_0_edges,\n",
    "                                source='source',\n",
    "                                target='target',\n",
    "                                edge_attr=edge_attr,\n",
    "                                create_using=nx.DiGraph())\n",
    "    \n",
    "    ###### IN NODES\n",
    "    \n",
    "    # NODES (set attributes)\n",
    "    for k in card_0_nodes['type'].unique():\n",
    "        #print(k)\n",
    "        #import pdb\n",
    "        #pdb.set_trace()\n",
    "        node_col = 'node_id'\n",
    "        cols = [x for x in card_0_nodes[card_0_nodes['type']==k] if x not in ['node_id']]\n",
    "        for node_attr in cols:\n",
    "            temp = card_0_nodes[[node_attr, node_col]]\n",
    "            temp = temp.dropna()\n",
    "\n",
    "            # Eliminate and wrap in quotes\n",
    "            temp[node_attr] = temp[node_attr].apply(eliminate_and_wrap_in_quotes)\n",
    "            nx.set_node_attributes(G, pd.Series(temp[node_attr].values, index=temp[node_col].values).copy().to_dict(), name=node_attr)\n",
    "    \n",
    "    result[card_id]['outgoing'] = G\n",
    "    \n",
    "    # Card nodes\n",
    "    card_0_in_nodes = in_nodes[(in_nodes['card_id']==card_id)]\n",
    "    card_0_in_edges = in_edges[(in_edges['source'].isin(card_0_in_nodes['node_id']))\n",
    "                            |(in_edges['target'].isin(card_0_in_nodes['node_id']))]\n",
    "    \n",
    "    # Relevant entity nodes\n",
    "    ent_0_nodes = ent_in_nodes[ent_in_nodes['node_id'].isin(card_0_in_edges['source'])\n",
    "                                 |ent_in_nodes['node_id'].isin(card_0_in_edges['target'])]\n",
    "\n",
    "    card_0_in_nodes = pd.concat([card_0_in_nodes, ent_0_nodes], sort=False)\n",
    "    \n",
    "    #result[card_id] = {'nodes': card_0_in_nodes.copy(), 'edges': card_0_in_edges.copy()}\n",
    "    \n",
    "    # Build graph\n",
    "    edge_attr = [x for x in card_0_in_edges.columns if not x in ['source', 'target']]\n",
    "    H = nx.from_pandas_edgelist(card_0_in_edges,\n",
    "                                source='source',\n",
    "                                target='target',\n",
    "                                edge_attr=edge_attr,\n",
    "                                create_using=nx.DiGraph())\n",
    "    \n",
    "    # NODES (set attributes)\n",
    "    for k in card_0_in_nodes['type'].unique():\n",
    "        #print(k)\n",
    "        #import pdb\n",
    "        #pdb.set_trace()\n",
    "        node_col = 'node_id'\n",
    "        cols = [x for x in card_0_in_nodes[card_0_in_nodes['type']==k] if x not in ['node_id']]\n",
    "        for node_attr in cols:\n",
    "            temp = card_0_in_nodes[[node_attr, node_col]]\n",
    "            temp = temp.dropna()\n",
    "\n",
    "            # Eliminate and wrap in quotes\n",
    "            temp[node_attr] = temp[node_attr].apply(eliminate_and_wrap_in_quotes)\n",
    "            nx.set_node_attributes(H, pd.Series(temp[node_attr].values, index=temp[node_col].values).copy().to_dict(), name=node_attr)\n",
    "    \n",
    "    result[card_id]['incoming'] = H\n",
    "    \n",
    "    a = result[card_id]\n",
    "    pickle.dump(a, open(path_to_graph_file, 'wb'))\n",
    "    result = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_graph(result[card_id]['incoming'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false
   },
   "outputs": [],
   "source": [
    "draw_graph(result[card_id]['outgoing'])"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:mtgnltk]",
   "language": "python",
   "name": "conda-env-mtgnltk-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "203.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "455px",
    "left": "1008px",
    "right": "20px",
    "top": "120px",
    "width": "355px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

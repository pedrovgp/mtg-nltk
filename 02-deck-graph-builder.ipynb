{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build graph for a given deck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea here is to\n",
    "\n",
    "1. Get a deck as a txt (from deckstat)\n",
    "2. Load the previously ETLelled outgoing and incoming cards graph\n",
    "3. Merge 1 with 2 to get a deck in/outgoing nodes and edes\n",
    "4. Export the deck graph\n",
    "\n",
    "Next:\n",
    "1. Analysis will be the next natural step\n",
    "\n",
    "At the end, store it in a pickle to avoid parsing everything again next time, which takes a long time.\n",
    "\n",
    "**DESIRED RESULT**:\n",
    "deck_graph = graph which nodes are cards, entities, pop and tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hideCode": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "sets = json.load(open('./AllSets.json', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "cards_all=[]\n",
    "for k, sett in sets.items():\n",
    "    if (k in ['UGL', 'UST', 'UNH']) or (len(k)>3): # Ignore Unglued, Unstable and promotional things\n",
    "        continue\n",
    "    for card in sett['cards']:\n",
    "        card['set'] = k\n",
    "    cards_all.extend(sett['cards'])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('postgresql+psycopg2://mtg:mtg@localhost:5432/mtg')\n",
    "engine.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataframe of cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false
   },
   "outputs": [],
   "source": [
    "# Load deck list\n",
    "filename = './decks/Benalia-knights-rotation-proof.txt'\n",
    "deck_regex = r'^(?P<amount>\\d+) (?P<card_name>.*?)\\n'\n",
    "with open(filename, 'r') as f:\n",
    "    txt = f.readlines()\n",
    "    #print(txt)\n",
    "    deck_list = []\n",
    "    for x in txt:\n",
    "        deck_list.extend(re.findall(deck_regex, x))\n",
    "#deck_list # -> [(amount, card_name), (amount, card_name), ...]\n",
    "cards_in_deck_names_list = []\n",
    "for amount, card in deck_list:\n",
    "    for i in range(int(amount)):\n",
    "        cards_in_deck_names_list.append(card)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_nodes = pd.read_pickle('./pickles/cards_outgoing_nodes.pkl')\n",
    "out_edges = pd.read_pickle('./pickles/cards_outgoing_edges.pkl')\n",
    "in_nodes = pd.read_pickle('./pickles/cards_incoming_nodes.pkl')\n",
    "in_edges = pd.read_pickle('./pickles/cards_incoming_edges.pkl')\n",
    "# cards_df = cards_df.sample(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_out_nodes  = out_nodes[out_nodes['type']=='entity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_cards = out_nodes[out_nodes['type']=='card'].drop_duplicates(subset=['card_name'])\n",
    "deck_cards = pd.DataFrame(cards_in_deck_names_list)\n",
    "deck_cards.columns = ['card_name']\n",
    "deck_cards = deck_cards.merge(unique_cards, how='left', on=['card_name'])\n",
    "deck_cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build out nodes and edges for this deck\n",
    "deck_cards['deck_card_id'] = deck_cards.index\n",
    "deck_cards = deck_cards[['card_id', 'deck_card_id']]\n",
    "deck_out_nodes = deck_cards.merge(out_nodes, how='left', on=['card_id'], suffixes=('', '__DELETE_REPEATED'))\n",
    "deck_out_nodes = pd.concat([deck_out_nodes, ent_out_nodes], sort=False).drop_duplicates(subset=['node_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build out nodes and edges for this deck\n",
    "card_0_nodes = deck_out_nodes[(deck_out_nodes['deck_card_id']==0)]\n",
    "card_0_edges = out_edges[(out_edges['source'].isin(card_0_nodes['node_id']))\n",
    "                        |(out_edges['target'].isin(card_0_nodes['node_id']))]\n",
    "\n",
    "# Relevant entity nodes\n",
    "ent_out_nodes = ent_out_nodes[ent_out_nodes['node_id'].isin(card_0_edges['source'])\n",
    "                             |ent_out_nodes['node_id'].isin(card_0_edges['target'])]\n",
    "\n",
    "card_0_nodes = pd.concat([card_0_nodes, ent_out_nodes], sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helping functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Split dataframelist\n",
    "import collections\n",
    "def splitDataFrameList(df,target_column,separator=None):\n",
    "    '''\n",
    "    https://gist.github.com/jlln/338b4b0b55bd6984f883\n",
    "    df = dataframe to split,\n",
    "    target_column = the column containing the values to split\n",
    "    separator = the symbol used to perform the split\n",
    "    returns: a dataframe with each entry for the target column separated, with each element moved into a new row. \n",
    "    The values in the other columns are duplicated across the newly divided rows.\n",
    "    '''\n",
    "    def splitListToRows(row,row_accumulator,target_column,separator):\n",
    "        split_row = row[target_column]#.split(separator)\n",
    "        if isinstance(split_row, collections.Iterable):\n",
    "            for s in split_row:\n",
    "                new_row = row.to_dict()\n",
    "                new_row[target_column] = s\n",
    "                row_accumulator.append(new_row)\n",
    "        else:\n",
    "            new_row = row.to_dict()\n",
    "            new_row[target_column] = pd.np.nan\n",
    "            row_accumulator.append(new_row)\n",
    "    new_rows = []\n",
    "    df.apply(splitListToRows, axis=1, args=(new_rows,target_column,separator))\n",
    "    new_df = pd.DataFrame(new_rows)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Create hashable dict\n",
    "from collections import OrderedDict\n",
    "import hashlib\n",
    "class HashableDict(OrderedDict):\n",
    "    def __hash__(self):\n",
    "        return hash(tuple(sorted(self.items())))\n",
    "    \n",
    "    def hexdigext(self):\n",
    "        return hashlib.sha256(''.join([str(k)+str(v) for k, v in self.items()]).encode()).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Make defaultdict which depends on its key\n",
    "# Source: https://www.reddit.com/r/Python/comments/27crqg/making_defaultdict_create_defaults_that_are_a/\n",
    "from collections import defaultdict\n",
    "class key_dependent_dict(defaultdict):\n",
    "    def __init__(self, f_of_x):\n",
    "        super().__init__(None) # base class doesn't get a factory\n",
    "        self.f_of_x = f_of_x # save f(x)\n",
    "    def __missing__(self, key): # called when a default needed\n",
    "        ret = self.f_of_x(key) # calculate default value\n",
    "        self[key] = ret # and install it in the dict\n",
    "        return ret\n",
    "    \n",
    "def entity_key_hash(key):\n",
    "    return HashableDict({'entity': key}).hexdigext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# function to draw a graph to png\n",
    "shapes = ['box', 'polygon', 'ellipse', 'oval', 'circle', 'egg', 'triangle', 'exagon', 'star']\n",
    "colors = ['blue', 'black', 'red', '#db8625', 'green', 'gray', 'cyan', '#ed125b']\n",
    "styles = ['filled', 'rounded', 'rounded, filled', 'dashed', 'dotted, bold']\n",
    "\n",
    "entities_colors = {\n",
    "    'PLAYER': '#FF6E6E',\n",
    "    'ZONE': '#F5D300',\n",
    "    'ACTION': '#1ADA00',\n",
    "    'MANA': '#00DA84',\n",
    "    'SUBTYPE': '#0DE5E5',\n",
    "    'TYPE': '#0513F0',\n",
    "    'SUPERTYPE': '#8D0BCA',\n",
    "    'ABILITY': '#cc3300',\n",
    "    'COLOR': '#666633',\n",
    "    'STEP': '#E0E0F8'\n",
    "}\n",
    "\n",
    "def draw_graph(G, filename='test.png'):\n",
    "    pdot = nx.drawing.nx_pydot.to_pydot(G)\n",
    "\n",
    "\n",
    "    for i, node in enumerate(pdot.get_nodes()):\n",
    "        attrs = node.get_attributes()\n",
    "        node.set_label(str(attrs.get('label', 'none')))\n",
    "    #     node.set_fontcolor(colors[random.randrange(len(colors))])\n",
    "        entity_node_ent_type = attrs.get('entity_node_ent_type', pd.np.nan)\n",
    "        if not pd.isnull(entity_node_ent_type):\n",
    "            color = entities_colors[entity_node_ent_type.strip('\"')]\n",
    "            node.set_fillcolor(color)\n",
    "            node.set_color(color)\n",
    "            node.set_shape('hexagon')\n",
    "            #node.set_colorscheme()\n",
    "            node.set_style('filled')\n",
    "        \n",
    "        node_type = attrs.get('type', None)\n",
    "        if node_type == '\"card\"':\n",
    "            color = '#999966'\n",
    "            node.set_fillcolor(color)\n",
    "#             node.set_color(color)\n",
    "            node.set_shape('star')\n",
    "            #node.set_colorscheme()\n",
    "            node.set_style('filled')\n",
    "    #     \n",
    "        #pass\n",
    "\n",
    "    for i, edge in enumerate(pdot.get_edges()):\n",
    "        att = edge.get_attributes()\n",
    "        att = att.get('label', 'NO-LABEL')\n",
    "        edge.set_label(att)\n",
    "    #     edge.set_fontcolor(colors[random.randrange(len(colors))])\n",
    "    #     edge.set_style(styles[random.randrange(len(styles))])\n",
    "    #     edge.set_color(colors[random.randrange(len(colors))])\n",
    "\n",
    "    png_path = filename\n",
    "    pdot.write_png(png_path)\n",
    "\n",
    "    from IPython.display import Image\n",
    "    return Image(png_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build graph with Networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminate_and_wrap_in_quotes(text):\n",
    "    return '\"'+str(text).replace('\"', '')+'\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_attr = [x for x in card_0_edges.columns if not x in ['source', 'target']]\n",
    "G = nx.from_pandas_edgelist(card_0_edges,\n",
    "                            source='source',\n",
    "                            target='target',\n",
    "                            edge_attr=edge_attr,\n",
    "                            create_using=nx.DiGraph())\n",
    "# NODES (set attributes)\n",
    "for k in card_0_nodes['type'].unique():\n",
    "    print(k)\n",
    "    node_col = 'node_id'\n",
    "    cols = [x for x in card_0_nodes[card_0_nodes['type']==k] if x not in ['node_id']]\n",
    "    for node_attr in cols:\n",
    "        temp = card_0_nodes[[node_attr, node_col]]\n",
    "        temp = temp.dropna()\n",
    "        \n",
    "        # Eliminate and wrap in quotes\n",
    "        temp[node_attr] = temp[node_attr].apply(eliminate_and_wrap_in_quotes)\n",
    "        nx.set_node_attributes(G, pd.Series(temp[node_attr].values, index=temp[node_col].values).copy().to_dict(), name=node_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_graph(G)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:mtgnltk]",
   "language": "python",
   "name": "conda-env-mtgnltk-py"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "203.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "455px",
    "left": "1008px",
    "right": "20px",
    "top": "120px",
    "width": "355px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

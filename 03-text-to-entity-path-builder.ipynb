{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build graph for each path from a cards text to entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea here is to\n",
    "\n",
    "1. Load the previously ETLelled outgoing and incoming graphs\n",
    "2. Build simple paths from card to its entity nodes\n",
    "3. Build a paths df keyed by card_id, entity and orders with some common attributes of these paths:\n",
    "paragraph type/order, pop type/order, part type/order,\n",
    "entity pos (actualy head's pos), entity head (actually head's head)\n",
    "4. Store in postgres\n",
    "\n",
    "NEXT\n",
    "4. Don't know yet\n",
    "\n",
    "**DESIRED RESULT**:\n",
    "result = dataframe/postgres table: (Indexes: card_id, orders, entity)\n",
    "\n",
    "| card_id | paragraph_order | pop_order | part_order | entity | paragraph_type | pop_type | part_type | entity_pos | entity_head | main_verb_of_path |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| a2fh34 | 0 | 1 | 1 | TYPE: Instant | activated | effect | intensifier | pobj | for | destroy |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from IPython.display import clear_output\n",
    "# from tqdm import tqdm # https://stackoverflow.com/questions/18603270/progress-indicator-during-pandas-operations-python\n",
    "# tqdm.pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import sqlalchemy\n",
    "engine = create_engine('postgresql+psycopg2://mtg:mtg@localhost:5432/mtg')\n",
    "engine.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helping functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Split dataframelist\n",
    "import collections\n",
    "def splitDataFrameList(df,target_column,separator=None):\n",
    "    '''\n",
    "    https://gist.github.com/jlln/338b4b0b55bd6984f883\n",
    "    df = dataframe to split,\n",
    "    target_column = the column containing the values to split\n",
    "    separator = the symbol used to perform the split\n",
    "    returns: a dataframe with each entry for the target column separated, with each element moved into a new row. \n",
    "    The values in the other columns are duplicated across the newly divided rows.\n",
    "    '''\n",
    "    def splitListToRows(row,row_accumulator,target_column,separator):\n",
    "        split_row = row[target_column]#.split(separator)\n",
    "        if isinstance(split_row, collections.Iterable):\n",
    "            for s in split_row:\n",
    "                new_row = row.to_dict()\n",
    "                new_row[target_column] = s\n",
    "                row_accumulator.append(new_row)\n",
    "        else:\n",
    "            new_row = row.to_dict()\n",
    "            new_row[target_column] = pd.np.nan\n",
    "            row_accumulator.append(new_row)\n",
    "    new_rows = []\n",
    "    df.apply(splitListToRows, axis=1, args=(new_rows,target_column,separator))\n",
    "    new_df = pd.DataFrame(new_rows)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Create hashable dict\n",
    "from collections import OrderedDict\n",
    "import hashlib\n",
    "class HashableDict(OrderedDict):\n",
    "    def __hash__(self):\n",
    "        return hash(tuple(sorted(self.items())))\n",
    "    \n",
    "    def hexdigext(self):\n",
    "        return hashlib.sha256(''.join([str(k)+str(v) for k, v in self.items()]).encode()).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Make defaultdict which depends on its key\n",
    "# Source: https://www.reddit.com/r/Python/comments/27crqg/making_defaultdict_create_defaults_that_are_a/\n",
    "from collections import defaultdict\n",
    "class key_dependent_dict(defaultdict):\n",
    "    def __init__(self, f_of_x):\n",
    "        super().__init__(None) # base class doesn't get a factory\n",
    "        self.f_of_x = f_of_x # save f(x)\n",
    "    def __missing__(self, key): # called when a default needed\n",
    "        ret = self.f_of_x(key) # calculate default value\n",
    "        self[key] = ret # and install it in the dict\n",
    "        return ret\n",
    "    \n",
    "def entity_key_hash(key):\n",
    "    return HashableDict({'entity': key}).hexdigext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# function to draw a graph to png\n",
    "shapes = ['box', 'polygon', 'ellipse', 'oval', 'circle', 'egg', 'triangle', 'exagon', 'star']\n",
    "colors = ['blue', 'black', 'red', '#db8625', 'green', 'gray', 'cyan', '#ed125b']\n",
    "styles = ['filled', 'rounded', 'rounded, filled', 'dashed', 'dotted, bold']\n",
    "\n",
    "entities_colors = {\n",
    "    'PLAYER': '#FF6E6E',\n",
    "    'ZONE': '#F5D300',\n",
    "    'ACTION': '#1ADA00',\n",
    "    'MANA': '#00DA84',\n",
    "    'SUBTYPE': '#0DE5E5',\n",
    "    'TYPE': '#0513F0',\n",
    "    'SUPERTYPE': '#8D0BCA',\n",
    "    'ABILITY': '#cc3300',\n",
    "    'COLOR': '#666633',\n",
    "    'STEP': '#E0E0F8'\n",
    "}\n",
    "\n",
    "def draw_graph(G, filename='test.png'):\n",
    "    pdot = nx.drawing.nx_pydot.to_pydot(G)\n",
    "\n",
    "\n",
    "    for i, node in enumerate(pdot.get_nodes()):\n",
    "        attrs = node.get_attributes()\n",
    "        node.set_label(str(attrs.get('label', 'none')))\n",
    "    #     node.set_fontcolor(colors[random.randrange(len(colors))])\n",
    "        entity_node_ent_type = attrs.get('entity_node_ent_type', pd.np.nan)\n",
    "        if not pd.isnull(entity_node_ent_type):\n",
    "            color = entities_colors[entity_node_ent_type.strip('\"')]\n",
    "            node.set_fillcolor(color)\n",
    "            node.set_color(color)\n",
    "            node.set_shape('hexagon')\n",
    "            #node.set_colorscheme()\n",
    "            node.set_style('filled')\n",
    "        \n",
    "        node_type = attrs.get('type', None)\n",
    "        if node_type == '\"card\"':\n",
    "            color = '#999966'\n",
    "            node.set_fillcolor(color)\n",
    "#             node.set_color(color)\n",
    "            node.set_shape('star')\n",
    "            #node.set_colorscheme()\n",
    "            node.set_style('filled')\n",
    "    #     \n",
    "        #pass\n",
    "\n",
    "    for i, edge in enumerate(pdot.get_edges()):\n",
    "        att = edge.get_attributes()\n",
    "        att = att.get('label', 'NO-LABEL')\n",
    "        edge.set_label(att)\n",
    "    #     edge.set_fontcolor(colors[random.randrange(len(colors))])\n",
    "    #     edge.set_style(styles[random.randrange(len(styles))])\n",
    "    #     edge.set_color(colors[random.randrange(len(colors))])\n",
    "\n",
    "    png_path = filename\n",
    "    pdot.write_png(png_path)\n",
    "\n",
    "    from IPython.display import Image\n",
    "    return Image(png_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build graph with Networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.readwrite import json_graph\n",
    "import json\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get paths from cards_text to entities (simple paths from text -> entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "with engine.connect() as con:\n",
    "    con.execute('''CREATE TABLE public.cards_graphs_as_json AS\n",
    "(SELECT * FROM public.cards_graphs_as_json_temp)''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'cards_graphs_as_json'\n",
    "to_table_name = 'cards_text_to_entity_simple_paths'\n",
    "chunk_size = 200\n",
    "\n",
    "all_ids = pd.read_sql_query('SELECT card_id from {0}'.\n",
    "                       format(table_name),\n",
    "                       engine,\n",
    "                      )\n",
    "chunks = [all_ids.iloc[all_ids.index[i:i + chunk_size]] for i in range(0, all_ids.shape[0], chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_df_for_subgraphs_of_paths_from_card_to_entities(row):\n",
    "    '''INPUT: a row with outgoing graph of card\n",
    "    RETURNs: a dataframe with each row corresponding to a path from text to entity'''\n",
    "    if not row['outgoing']:\n",
    "        return []\n",
    "    G = json_graph.node_link_graph(json.loads(row['outgoing']))\n",
    "    card_nodes = [x for x,y in G.nodes(data=True) if y['type']=='card']\n",
    "    entity_nodes = [x for x,y in G.nodes(data=True) if y['type']=='entity']\n",
    "    assert len(card_nodes) == 1\n",
    "\n",
    "    new_rows = []\n",
    "    for entity_node in entity_nodes:\n",
    "        paths_list = nx.all_simple_paths(G, card_nodes[0], entity_node)\n",
    "        for path in paths_list:\n",
    "            graph_row = {}\n",
    "            path_g = G.subgraph(path)\n",
    "\n",
    "            graph_row['card_id'] = row['card_id']\n",
    "            graph_row['path_graph_json'] = json.dumps(json_graph.node_link_data(path_g))\n",
    "            graph_row['part'] = G.nodes[path[1]]['part']\n",
    "            has_add = re.findall(r'add ', str(graph_row['part']), flags=re.IGNORECASE)\n",
    "            graph_row['has_add'] = True if has_add else False\n",
    "                \n",
    "            # Text type and orders\n",
    "    #         graph_row['paragraph_type'] = G.node[path[1]]['paragraph_type']\n",
    "            graph_row['paragraph_order'] = G.nodes[path[1]]['paragraph_order']\n",
    "            graph_row['pop_type'] = G.nodes[path[1]]['pop_type']\n",
    "            graph_row['pop_order'] = G.nodes[path[1]]['pop_order']\n",
    "            graph_row['part_type'] = G.nodes[path[1]]['part_type']\n",
    "            graph_row['part_order'] = G.nodes[path[1]]['part_order']\n",
    "            \n",
    "            graph_row['path_text_key'] = (graph_row['card_id']\n",
    "                                    +'-'+str(int(graph_row['paragraph_order']))\n",
    "                                    +'-'+str(int(graph_row['pop_order']))\n",
    "                                    +'-'+str(int(graph_row['part_order']))\n",
    "                                   )\n",
    "\n",
    "            # Entities info\n",
    "            graph_row['entity_node_entity'] = G.nodes[path[-1]]['entity_node_entity']\n",
    "            graph_row['entity_node_ent_type'] = G.nodes[path[-1]]['entity_node_ent_type']\n",
    "            graph_row['entity_node_desc'] = G.nodes[path[-1]]['entity_node_desc']\n",
    "            \n",
    "            graph_row['path_pk'] = (graph_row['path_text_key']\n",
    "                                    +'-'+graph_row['entity_node_entity']\n",
    "                                   )\n",
    "\n",
    "            graph_row['entity_pos'] = G.nodes[path[-2]]['token_node_pos']\n",
    "            graph_row['entity_tag'] = G.nodes[path[-2]]['token_node_tag']\n",
    "            graph_row['entity_head_dep'] = G.nodes[path[-2]]['token_head_dep']\n",
    "\n",
    "            # Entities head info\n",
    "            if G.nodes[path[-3]]['type'] == 'token':\n",
    "                graph_row['entity_head'] = G.nodes[path[-3]]['token_node_text']\n",
    "                graph_row['entity_head_tag'] = G.nodes[path[-3]]['token_node_tag']\n",
    "                graph_row['entity_head_head_dep'] = G.nodes[path[-2]]['token_head_dep']\n",
    "                graph_row['entity_head_pos'] = G.nodes[path[-2]]['token_node_pos']\n",
    "\n",
    "\n",
    "\n",
    "            # Append row\n",
    "            new_rows.append(graph_row)\n",
    "    \n",
    "    return pd.DataFrame(new_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# Testing dataframe composition\n",
    "row = df.iloc[1]\n",
    "G = json_graph.node_link_graph(json.loads(row['outgoing']))\n",
    "card_nodes = [x for x,y in G.nodes(data=True) if y['type']=='card']\n",
    "entity_nodes = [x for x,y in G.nodes(data=True) if y['type']=='entity']\n",
    "assert len(card_nodes) == 1\n",
    "\n",
    "new_rows = []\n",
    "for entity_node in entity_nodes:\n",
    "    paths_list = nx.all_simple_paths(G, card_nodes[0], entity_node)\n",
    "    for path in paths_list:\n",
    "        graph_row = {}\n",
    "        path_g = G.subgraph(path)\n",
    "\n",
    "        graph_row['card_id'] = row['card_id']\n",
    "        graph_row['path_graph_json'] = json.dumps(json_graph.node_link_data(path_g))\n",
    "        graph_row['part'] = G.node[path[1]]['part']\n",
    "\n",
    "        # Text type and orders\n",
    "#         graph_row['paragraph_type'] = G.node[path[1]]['paragraph_type']\n",
    "        graph_row['paragraph_order'] = G.node[path[1]]['paragraph_order']\n",
    "        graph_row['pop_type'] = G.node[path[1]]['pop_type']\n",
    "        graph_row['pop_order'] = G.node[path[1]]['pop_order']\n",
    "        graph_row['part_type'] = G.node[path[1]]['part_type']\n",
    "        graph_row['part_order'] = G.node[path[1]]['part_order']\n",
    "\n",
    "        # Entities info\n",
    "        graph_row['entity_node_entity'] = G.node[path[-1]]['entity_node_entity']\n",
    "        graph_row['entity_node_ent_type'] = G.node[path[-1]]['entity_node_ent_type']\n",
    "        graph_row['entity_node_desc'] = G.node[path[-1]]['entity_node_desc']\n",
    "\n",
    "        graph_row['entity_pos'] = G.node[path[-2]]['token_node_pos']\n",
    "        graph_row['entity_tag'] = G.node[path[-2]]['token_node_tag']\n",
    "        graph_row['entity_head_dep'] = G.node[path[-2]]['token_head_dep']\n",
    "\n",
    "        # Entities head info\n",
    "        if G.node[path[-3]]['type'] == 'token':\n",
    "            graph_row['entity_head'] = G.node[path[-3]]['token_node_text']\n",
    "            graph_row['entity_head_tag'] = G.node[path[-3]]['token_node_tag']\n",
    "            graph_row['entity_head_head_dep'] = G.node[path[-2]]['token_head_dep']\n",
    "            graph_row['entity_head_pos'] = G.node[path[-2]]['token_node_pos']\n",
    "            \n",
    "\n",
    "\n",
    "        # Append row\n",
    "        new_rows.append(graph_row)\n",
    "n = pd.DataFrame(new_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql_query('SELECT * from {0} WHERE card_id IN ({1})'.\n",
    "                           format(table_name, ','.join([\"'\"+x+\"'\" for x in chunks[0]['card_id']])),\n",
    "                           engine,\n",
    "                          index_col='card_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Iter chunks and save simple paths\n",
    "start = datetime.datetime.now()\n",
    "for i, chunk in enumerate(chunks):\n",
    "    \n",
    "    df = pd.read_sql_query('SELECT * from {0} WHERE card_id IN ({1})'.\n",
    "                           format(table_name, ','.join([\"'\"+x+\"'\" for x in chunk['card_id']])),\n",
    "                           engine,\n",
    "                          )\n",
    "    \n",
    "    paths_series = df.apply(\n",
    "        get_df_for_subgraphs_of_paths_from_card_to_entities, axis='columns')\n",
    "\n",
    "\n",
    "    # Drop these ids to append them again\n",
    "#     DROP_QUERY = ('DELETE FROM {0} WHERE card_id IN ({1})'.\n",
    "#                            format(table_name, ','.join([\"'\"+x+\"'\" for x in df.index]))\n",
    "#                  )\n",
    "#     print(engine.execute(DROP_QUERY))\n",
    "\n",
    "    # Create columns if not exists\n",
    "#     NEW_QUERY = '''\n",
    "#         ALTER TABLE {0} ADD COLUMN {1} json;\n",
    "#             '''.format(table_name, 'list_of_subgraphs_of_paths_from_card_to_entities')\n",
    "#     try:\n",
    "#         print(engine.execute(NEW_QUERY))\n",
    "#     except sqlalchemy.exc.ProgrammingError:\n",
    "#         # Just ignore, it alredy exists\n",
    "#         pass\n",
    "\n",
    "    print('Concatenating')\n",
    "    df = (pd.concat(paths_series.values, sort=False)\n",
    "          .reset_index(drop=True)\n",
    "          .set_index(['card_id', 'paragraph_order', 'pop_order', 'part_order', 'entity_node_entity'])\n",
    "         )\n",
    "    \n",
    "    method = 'append' if i else 'replace'\n",
    "    df.to_sql(to_table_name, engine, if_exists=method,\n",
    "                  dtype = {'path_graph_json':sqlalchemy.types.JSON})\n",
    "\n",
    "    print('Chunk {0}/{1} ELAPSED: {2}'.format(i, len(chunks), datetime.datetime.now()-start))\n",
    "    print('Export finished')\n",
    "    if not i%15: clear_output()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false,
    "hideCode": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# Testing\n",
    "card_id = some_ids.iloc[0]\n",
    "G = json_graph.node_link_graph(json.loads(df.loc[card_id, 'outgoing']))\n",
    "card_nodes = [x for x,y in G.nodes(data=True) if y['type']=='card']\n",
    "entity_nodes = [x for x,y in G.nodes(data=True) if y['type']=='entity']\n",
    "assert len(card_nodes) == 1\n",
    "\n",
    "paths = []\n",
    "for entity_node in entity_nodes:\n",
    "    paths_list = nx.all_simple_paths(G, card_nodes[0], entity_node)\n",
    "    a = [json_graph.node_link_data(G.subgraph(path)) for path in paths_list]\n",
    "    paths.extend(a)\n",
    "\n",
    "json.dumps(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "test = pd.read_sql_query('SELECT * from {0}'.\n",
    "                       format(to_table_name),\n",
    "                       engine,\n",
    "                      index_col=['card_id', 'paragraph_order', 'pop_order', 'part_order', 'entity_node_entity'])\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:mtg]",
   "language": "python",
   "name": "conda-env-mtg-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "203.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "455px",
    "left": "1008px",
    "right": "20px",
    "top": "120px",
    "width": "355px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

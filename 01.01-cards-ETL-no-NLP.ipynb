{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL of all cards in database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea here is to\n",
    "\n",
    "1. Get the cards json, build a dataframe of it\n",
    "2. Prework a few things and split paragraphs to detect abilities, activated abilites/costs and effects.\n",
    "3. Additionaly, detect condition and intensifier parts of a paragraph.\n",
    "4. Use spacy to parse the cards text and identify entities.\n",
    "5. Build the outgoing nodes and edges for each card (card -> text -> entities)\n",
    "6. Build the incoming nodes and edges for a card (entities (cards attributes) -> card)\n",
    "\n",
    "Next:\n",
    "7. Build the graphs for each card\n",
    "\n",
    "At the end, store it in a pickle to avoid parsing everything again next time, which takes a long time.\n",
    "\n",
    "**DESIRED RESULT**:\n",
    "outgoing_nodes_df = nodes for cards, text, tokens, entities\n",
    "outgoing_edges_df = edges from the nodes above\n",
    "incoming_nodes_df = nodes for cards and attribute entities\n",
    "incoming_edges_df = edges from the nodes above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict\n",
    "#from tqdm import tqdm\n",
    "#tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false
   },
   "outputs": [],
   "source": [
    "sets = json.load(open('./AllPrintings.json', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cards_all=[]\n",
    "for k, sett in sets.items():\n",
    "    if (k in ['UGL', 'UST', 'UNH']) or (len(k)>3): # Ignore Unglued, Unstable and promotional things\n",
    "        continue\n",
    "    for card in sett['cards']:\n",
    "        card['set'] = k\n",
    "    cards_all.extend(sett['cards'])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASPAS_TEXT = \"ASPAS_TEXT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mains_col_names = ['name', 'manaCost', 'text_preworked', 'type', 'power', 'toughness',\n",
    "                   'types', 'supertypes', 'subtypes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('postgresql+psycopg2://mtg:mtg@localhost:5432/mtg')\n",
    "engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_table_name = 'cards_text_parts'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helping functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Split dataframelist\n",
    "import collections\n",
    "def splitDataFrameList(df,target_column,separator=None):\n",
    "    '''\n",
    "    https://gist.github.com/jlln/338b4b0b55bd6984f883\n",
    "    df = dataframe to split,\n",
    "    target_column = the column containing the values to split\n",
    "    separator = the symbol used to perform the split\n",
    "    returns: a dataframe with each entry for the target column separated, with each element moved into a new row. \n",
    "    The values in the other columns are duplicated across the newly divided rows.\n",
    "    '''\n",
    "    def splitListToRows(row,row_accumulator,target_column,separator):\n",
    "        split_row = row[target_column]#.split(separator)\n",
    "        if isinstance(split_row, collections.Iterable):\n",
    "            for s in split_row:\n",
    "                new_row = row.to_dict()\n",
    "                new_row[target_column] = s\n",
    "                row_accumulator.append(new_row)\n",
    "        else:\n",
    "            new_row = row.to_dict()\n",
    "            new_row[target_column] = pd.np.nan\n",
    "            row_accumulator.append(new_row)\n",
    "    new_rows = []\n",
    "    df.apply(splitListToRows, axis=1, args=(new_rows,target_column,separator))\n",
    "    new_df = pd.DataFrame(new_rows)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataframe of cards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create tables for cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cards = cards_all\n",
    "cards_df = pd.DataFrame.from_dict(cards)\n",
    "cards_df = cards_df.drop_duplicates(subset=['name'])\n",
    "#cards_df = cards_df.sample(200)\n",
    "#cards_df = cards_df[cards_df['name'].isin(all_cards_names_in_decks)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add and transform features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make numeric power and toughness\n",
    "cards_df['power_num'] = pd.to_numeric(cards_df['power'], errors='coerce')\n",
    "cards_df['toughness_num'] = pd.to_numeric(cards_df['toughness'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add colored and generic cmcs\n",
    "# Find all patterns like {.*?} [1], than find all {\\d} or {X} (this is generic) [2]\n",
    "# Subtract the len([2]) from len([1])\n",
    "all_mana_pattern = r'{.*?}'\n",
    "generic_mana_pattern = r'{(?:X|\\d)}'\n",
    "cards_df['manaCost_tuples_generic'] = cards_df['manaCost'].apply(lambda x: list(re.findall(generic_mana_pattern, str(x))))\n",
    "cards_df['manaCost_tuples'] = cards_df['manaCost'].apply(lambda x: list(re.findall(all_mana_pattern, str(x))))\n",
    "cards_df['manaCost_tuples_len'] = cards_df['manaCost_tuples'].apply(lambda x: len(x))\n",
    "cards_df['manaCost_tuples_generic_len'] = cards_df['manaCost_tuples_generic'].apply(lambda x: len(x))\n",
    "cards_df['manaCost_coloured'] = cards_df['manaCost_tuples_len'] - cards_df['manaCost_tuples_generic_len']\n",
    "cards_df['manaCost_generic'] = cards_df['cmc'] - cards_df['manaCost_coloured']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace name by SELF and remove anything between parethesis\n",
    "pattern_parenthesis = r' ?\\(.*?\\)'\n",
    "def prework_text(card):\n",
    "    t = str(card['text']).replace(card['name'], 'SELF')\n",
    "    t = re.sub(pattern_parenthesis, '', t)\n",
    "    return t\n",
    "    \n",
    "cards_df['text_preworked'] = cards_df.apply(prework_text, axis=1)\n",
    "#cards_df['text_preworked']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set land text, which may be empty\n",
    "def add_mana_text(text, sym):\n",
    "    if not text:\n",
    "        return '{T}: Add ' + sym +'.'\n",
    "    elif '{T}: Add ' + sym not in text:\n",
    "        return text + '\\n' + '{T}: Add ' + sym +'.'\n",
    "    return text\n",
    "\n",
    "lands = [('Plains', '{W}'), ('Swamp', '{B}'), ('Island', '{U}'), ('Mountain', '{R}'), ('Forest', '{G}')]\n",
    "for land_name, sym in lands:\n",
    "    cards_df['text_preworked'] = cards_df.progress_apply(lambda x:\n",
    "                                      add_mana_text(x['text_preworked'], sym)\n",
    "                                      if isinstance(x['subtypes'], list) and land_name in x['subtypes']\n",
    "                                      else x['text_preworked'],\n",
    "                              axis=1\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether card can add mana\n",
    "cards_df['has_add'] = cards_df['text_preworked'].apply(\n",
    "    lambda x: True\n",
    "              if re.findall(r'add ', str(x), flags=re.IGNORECASE)\n",
    "              else False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sep = \"Âª\"\n",
    "if cards_df['text_preworked'].str.contains(sep).any():\n",
    "    raise Exception(\"Bad separator symbol. It is contained in some text.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert cards_df[cards_df['text_preworked'].str.contains('\\(').fillna(False)]['text_preworked'].empty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to sql\n",
    "cards_df.set_index(['id', 'name']).to_sql('cards', engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export keys tables\n",
    "unique_ids = pd.DataFrame(cards_df['id'].unique())\n",
    "unique_ids.to_sql('unique_card_ids', engine, index=False, if_exists='replace')\n",
    "\n",
    "unique_names = cards_df[['name']].drop_duplicates()\n",
    "unique_names.to_sql('unique_card_names', engine, index=False, if_exists='replace')\n",
    "\n",
    "unique_card_ids_names = cards_df[['id', 'name']].drop_duplicates(subset=['name'])\n",
    "unique_card_ids_names.to_sql('unique_card_ids_names', engine, index=False, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create accessory tables form lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_containing_lists = []\n",
    "for col, ob in zip(cards_df.iloc[0].index, cards_df.iloc[0]):\n",
    "    #print(i, ob, type(ob))\n",
    "    if isinstance(ob, list):\n",
    "        cols_containing_lists.append(col)\n",
    "cols_containing_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cols_containing_lists:\n",
    "    print(col)\n",
    "    temp = cards_df[['name', col]].drop_duplicates(subset=['name'])\n",
    "    temp = splitDataFrameList(temp, col)\n",
    "    if col in ['colorIdentity', 'colors']:\n",
    "        temp[col] = temp[col].fillna('colorless')\n",
    "    temp = temp.dropna().set_index('name')\n",
    "    print('Exporting')\n",
    "    temp.to_sql('cards_'+col, engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finishing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set id as index for later work\n",
    "cards_df = cards_df.set_index('id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain specific vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build some domain specific vocabulary for MTG. For example, let's list supertypes, types, subtypes, know all card names, this kind f thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create set of cards names\n",
    "cards_names = set(cards_df.name.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create set of supertypes\n",
    "array_of_supertypes_tuples = cards_df['supertypes'].dropna().apply(tuple).unique()\n",
    "cards_supertypes = tuple()\n",
    "for tup in array_of_supertypes_tuples:\n",
    "    cards_supertypes += tup\n",
    "    \n",
    "cards_supertypes = set(cards_supertypes)\n",
    "cards_supertypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create set of types\n",
    "array_of_types_tuples = cards_df['types'].dropna().apply(tuple).unique()\n",
    "cards_types = tuple()\n",
    "for tup in array_of_types_tuples:\n",
    "    cards_types += tup\n",
    "    \n",
    "cards_types = set(cards_types)\n",
    "#cards_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create set of types\n",
    "array_of_subtypes_tuples = cards_df['subtypes'].dropna().apply(tuple).unique()\n",
    "cards_subtypes = tuple()\n",
    "for tup in array_of_subtypes_tuples:\n",
    "    cards_subtypes += tup\n",
    "    \n",
    "cards_subtypes = set(cards_subtypes)\n",
    "#cards_subtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cards_df.head(10).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pickle\n",
    "r = requests.get('https://media.wizards.com/2020/downloads/MagicCompRules%2020200122.txt')\n",
    "if not r.status_code == 200:\n",
    "    r.raise_for_status()\n",
    "comprules = r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rules.txt', 'r', encoding='latin-1') as f:\n",
    "    comprules = '\\n'.join(f.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_abilities_pat = r'702\\.\\d+\\. ([A-Za-z ]+)'\n",
    "abilities = re.findall(kw_abilities_pat, comprules, re.IGNORECASE)\n",
    "abilities.pop(0) # Its just the rulings \n",
    "abilities.sort()\n",
    "#abilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect an abilities sentence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should:\n",
    "- Split sentences in a card by '\\n' (=card_sentences_list)\n",
    "- Split each element in card_sentences_list by ', ' (=split_candidate_sentences)\n",
    "- Search for the pattern r'^ability' in each item of split_candidate_sentences\n",
    "- If the pattern is found for evey item, then, split_candidate_sentences is an abilities sentence\n",
    "\n",
    "We can, at the same time, detect activated abilites sentences and \"rest\" sentences (which are not abilites and not triggered abilites ones).\n",
    "- Split sentences in a card by '\\n' (=card_sentences_list)\n",
    "- Those sentences which contain : are activated abilites\n",
    "\n",
    "Sentences which are not in any case above are \"rest\" sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "ability_start_pattern = r'|'.join(['^'+ab+r'\\b' for ab in abilities])\n",
    "#print(ability_start_pattern)\n",
    "def is_ability_sentence(sentence):\n",
    "    elem_starting_with_ability = []\n",
    "    exceptions = ['Cycling abilities you activate cost up to {2} less to activate.']\n",
    "    if sentence in exceptions:\n",
    "        return False\n",
    "    elems = sentence.replace(';', ',').split(', ')\n",
    "    for elem in elems:\n",
    "        if re.search(ability_start_pattern, elem, re.IGNORECASE):\n",
    "            elem_starting_with_ability.append(re.search(ability_start_pattern, elem, re.IGNORECASE))\n",
    "        else:\n",
    "            return False\n",
    "    if len(elems)==len(elem_starting_with_ability):\n",
    "        return True\n",
    "    raise Exception('We should never get here')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets detetect all paragraphs types (and keep each ability as a separate paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "def splitDataFrameList(df,target_column,separator=None):\n",
    "    '''\n",
    "    https://gist.github.com/jlln/338b4b0b55bd6984f883\n",
    "    df = dataframe to split,\n",
    "    target_column = the column containing the values to split\n",
    "    separator = the symbol used to perform the split\n",
    "    returns: a dataframe with each entry for the target column separated, with each element moved into a new row. \n",
    "    The values in the other columns are duplicated across the newly divided rows.\n",
    "    '''\n",
    "    def splitListToRows(row,row_accumulator,target_column,separator):\n",
    "        split_row = row[target_column]#.split(separator)\n",
    "        if isinstance(split_row, collections.Iterable):\n",
    "            for s in split_row:\n",
    "                new_row = row.to_dict()\n",
    "                new_row[target_column] = s\n",
    "                row_accumulator.append(new_row)\n",
    "        else:\n",
    "            new_row = row.to_dict()\n",
    "            new_row[target_column] = pd.np.nan\n",
    "            row_accumulator.append(new_row)\n",
    "    new_rows = []\n",
    "    df.apply(splitListToRows, axis=1, args=(new_rows,target_column,separator))\n",
    "    new_df = pd.DataFrame(new_rows)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     8,
     15,
     28
    ]
   },
   "outputs": [],
   "source": [
    "def get_paragraph_type(paragraph):\n",
    "    if is_ability_sentence(paragraph):\n",
    "        return 'ability'\n",
    "    elif ':' in paragraph:\n",
    "        return 'activated'\n",
    "    else:\n",
    "        return 'rest'\n",
    "\n",
    "def split_abilities_and_keep_the_rest(df_row):\n",
    "    '''Returns a list of abilities or a list of one element, which is not ability'''\n",
    "    if df_row['paragraph_type'] == 'ability':\n",
    "        return [x.strip() for x in df_row['paragraph'].replace(';',',').split(',')]\n",
    "    \n",
    "    return [df_row['paragraph']]\n",
    "\n",
    "def get_aspas(text):\n",
    "    if pd.isnull(text):\n",
    "        return pd.np.nan\n",
    "    \n",
    "    reg = re.findall(r'\\\"(.+?)\\\"', text)\n",
    "    \n",
    "    if not reg:\n",
    "        return pd.np.nan\n",
    "    \n",
    "    res = reg[0]\n",
    "    \n",
    "    return res\n",
    "        \n",
    "def get_paragraphs_and_types_df(card_row):\n",
    "    res = pd.DataFrame()\n",
    "    temp = pd.DataFrame()\n",
    "    \n",
    "    # Get initial paragraphs\n",
    "    temp['paragraph'] = card_row['text_preworked'].split('\\n')\n",
    "    temp[ASPAS_TEXT] = temp['paragraph'].apply(get_aspas)\n",
    "    temp['paragraph'] = temp.apply(lambda x: x['paragraph'].replace(x[ASPAS_TEXT], ASPAS_TEXT)\n",
    "                                             if not pd.isnull(x[ASPAS_TEXT]) else x['paragraph'],\n",
    "                                  axis=1)\n",
    "    \n",
    "    temp['paragraph_type'] = temp['paragraph'].apply(get_paragraph_type)\n",
    "    \n",
    "    # Split the abilities paragraphs into multiple rows\n",
    "    temp['paragraph'] = temp.apply(split_abilities_and_keep_the_rest, axis=1)\n",
    "    temp = splitDataFrameList(temp, 'paragraph')\n",
    "    res = temp\n",
    "    \n",
    "    res['card_id'] = card_row.name\n",
    "    res['paragraph_order'] = range(res.shape[0])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cards_df['df_paragraphs'] = cards_df.progress_apply(get_paragraphs_and_types_df, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#cards_df[['text_preworked','df_paragraphs']].iloc[21]['df_paragraphs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cards_df_paragraphs = pd.concat(cards_df['df_paragraphs'].values)\n",
    "cards_df_paragraphs.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# Show cards with triggered abilities\n",
    "cards_df[cards_df['df_sentences'].apply(lambda x: 'activated' in x['type'].values)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets use the same approach and separate paragraphs in abilities-complements, costs-effects and keep the rest as is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ability_and_complement_regex = r'(' + ability_start_pattern +')' + r'(.*)'\n",
    "#ability_and_complement_regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pop_and_complements_df(paragraph_row):\n",
    "    res = pd.DataFrame()\n",
    "    pat_ability = re.compile(ability_and_complement_regex, re.IGNORECASE)\n",
    "    \n",
    "    if paragraph_row['paragraph_type'] == 'ability':\n",
    "        \n",
    "        #print(res['pop'].iloc[0])\n",
    "        #print(re.findall(pat, res['pop'].iloc[0]))\n",
    "        x = paragraph_row['paragraph']\n",
    "        if (not pd.isnull(x)) and re.findall(pat_ability, x):\n",
    "            ability = re.findall(pat_ability, x)[0][0].strip()\n",
    "            ability_complement = re.findall(pat_ability, x)[0][1].strip()\n",
    "        else:\n",
    "            import pdb\n",
    "            pdb.set_trace()\n",
    "        \n",
    "        res['pop'] = [ability, ability_complement] \n",
    "        res['pop_type'] =  ['ability', 'ability_complement'] \n",
    "        res['pop_order'] = range(res['pop'].shape[0])\n",
    "    \n",
    "    elif paragraph_row['paragraph_type'] == 'activated':\n",
    "        '''Break the costs in individual ones'''\n",
    "        costs, effect = paragraph_row['paragraph'].split(':')\n",
    "        \n",
    "        exceptions = ['Pay half your life, rounded up']\n",
    "        if costs in exceptions:\n",
    "            costs = costs.replace(',','')\n",
    "            \n",
    "        res['pop'] =  costs.split(',') + [effect]\n",
    "        types = ['activation_cost' for x in costs.split(',')] + ['activated_effect']\n",
    "        \n",
    "        res['pop_type'] =  types\n",
    "        res['pop_order'] = range(res['pop'].shape[0])\n",
    "        \n",
    "    else:\n",
    "        '''Keep the rest as rest or effect'''\n",
    "        effect = paragraph_row['paragraph']\n",
    "        \n",
    "        res['pop'] =  [effect]\n",
    "        res['pop_type'] =  ['effect']\n",
    "        res['pop_order'] = range(res['pop'].shape[0])\n",
    "        \n",
    "        \n",
    "    res['card_id'] = paragraph_row['card_id']\n",
    "    res['paragraph_order'] = paragraph_row['paragraph_order']\n",
    "    res['paragraph_type'] = paragraph_row['paragraph_type']\n",
    "    res['paragraph'] = paragraph_row['paragraph']\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cards_df_paragraphs['pop'] = cards_df_paragraphs.progress_apply(get_pop_and_complements_df, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "cards_df_paragraphs.iloc[3]['pop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cards_df_pops = pd.concat(cards_df_paragraphs['pop'].values, sort=True)\n",
    "#cards_df_pops['pop_hash'] = cards_df_pops['pop'].apply(lambda x: uuid.uuid4().hex)\n",
    "#cards_df_pops.sort_values(by=['card_id','paragraph_order','pop_order']).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "cards_df_pops[cards_df_pops['pop_type']=='activation_cost']['pop'].dropna().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# Count how many abilities, activated abilities and effects there are\n",
    "temp = cards_df_pops\n",
    "temp['cont'] = 1\n",
    "\n",
    "index = ['pop_type']\n",
    "values = ['cont']\n",
    "\n",
    "pivot_pop = temp.pivot_table(index=index, values=values, aggfunc=pd.np.sum)\n",
    "pivot_pop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets use the same approach and separate conditions-\"result effect\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_regex = r'((?:if |whenever |when |only |unless |as long as ).*?[,.])'\n",
    "#condition_regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intensifier_regex = r'((?:for each ).*?[,.])'\n",
    "#intensifier_regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_condition_regex = r'(at the (?:beginning |end )of.*?[,.])'\n",
    "#step_condition_regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_conditions_and_effects_df(pop_row, original_cols=[]):\n",
    "    res = pd.DataFrame()\n",
    "    text = pop_row['pop']\n",
    "    \n",
    "    # Get list of conditions in text\n",
    "    reg_cond = re.findall(condition_regex, text, flags=re.IGNORECASE)\n",
    "    if not reg_cond:\n",
    "        reg_cond = []\n",
    "    \n",
    "    # Get list of step (time) conditions in text\n",
    "    reg_step_cond = re.findall(step_condition_regex, text, flags=re.IGNORECASE)\n",
    "    if not reg_step_cond:\n",
    "        reg_step_cond = []\n",
    "        \n",
    "    # Get list of intensifiers in text\n",
    "    reg_intensifier = re.findall(intensifier_regex, text, flags=re.IGNORECASE)\n",
    "    if not reg_intensifier:\n",
    "        reg_intensifier = []    \n",
    "    \n",
    "    # Get the rest of the text in a list\n",
    "    text_wo_conditions = text\n",
    "    for cond in reg_cond + reg_step_cond + reg_intensifier:\n",
    "        text_wo_conditions = text_wo_conditions.replace(cond, '')\n",
    "    text_wo_conditions = text_wo_conditions.strip(',. ')\n",
    "    text_wo_conditions = [text_wo_conditions]\n",
    "    \n",
    "    temp = []\n",
    "    for part in reg_cond:\n",
    "        temp.append({'part_order':text.find(part), 'part': part.strip(',. '), 'part_type': 'condition'})\n",
    "    for part in reg_step_cond:\n",
    "        temp.append({'part_order':text.find(part), 'part': part.strip(',. '), 'part_type': 'step_condition'})\n",
    "    for part in reg_intensifier:\n",
    "        temp.append({'part_order':text.find(part), 'part': part.strip(',. '), 'part_type': 'intensifier_for_each'})\n",
    "    for part in text_wo_conditions:\n",
    "        temp.append({'part_order':text.find(part), 'part': part.strip(',. '), 'part_type': 'wo_conditions'})\n",
    "    \n",
    "    # Reset order to start from zero\n",
    "    res = pd.DataFrame(temp).sort_values(by=['part_order'])\n",
    "    res = res.reset_index(drop=True)\n",
    "    res['part_order'] = res.index\n",
    "\n",
    "    for col in original_cols:\n",
    "        res[col] = pop_row[col]\n",
    "        \n",
    "    return res\n",
    "\n",
    "cards_df_pops['pop_parts'] = cards_df_pops.progress_apply(get_conditions_and_effects_df,\n",
    "                                                           args=(cards_df_pops.columns,),\n",
    "                                                           axis=1)\n",
    "cards_df_pop_parts = pd.concat(cards_df_pops['pop_parts'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#cards_df_pop_parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect named cards cited inside cards text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For later: define a way to get card named cited in other cards text. Same approach of self should suffice:\n",
    "1. Detect the names (done below)\n",
    "2. Replace the names with a place holder. CARD_NAME_1, CARD_NAME_2 (for each card name in a cards text).\n",
    "3. Create columns CARD_NAME_1, CARD_NAME_2, etc. in dataframe, holding the actual name in the cell value\n",
    "4. Create entity detector for CARD_NAME_1, CARD_NAME_2,...\n",
    "5. Manually add edge between CARD_NAME_1 and its actual value (the actual card name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "named_card_pattern = r'('+r'|'.join(['{0}'.format(n) for n in cards_names])+r')'\n",
    "named_card_regex = r' named ' + named_card_pattern + '((?: or )' + named_card_pattern + ')?' + r'.*?'\n",
    "#named_card_regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "test_text = 'Add {G} for every card named Path of Peace in all graveyards.'\n",
    "test = re.findall(named_card_regex, test_text)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "a = cards_df['text_preworked'].apply(\n",
    "    lambda x: re.findall(named_card_regex, x)\n",
    "    if re.findall(named_card_regex, x)\n",
    "    else pd.np.nan\n",
    ").dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "'Zhang Fei, Fierce Warrior' in named_card_regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "hideCode": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "cards_df.loc[a.index[0]]['text_preworked']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "a.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "test_text = 'SELF gets +2/+2 as long as you control a permanent named Guan Yu, Sainted Warrior or a permanent named Zhang Fei, Fierce Warrior in the battlefield.'\n",
    "test = re.findall(named_card_regex, test_text)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "hideCode": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "cards_df.loc['ef0fe275d7e5625b20f4c5cd7fc34301df0bea6d']['text_preworked']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "a['ef0fe275d7e5625b20f4c5cd7fc34301df0bea6d']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "#cards_df_pop_parts = pd.read_sql_table('cards_text_parts', engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cards_df_pop_parts['text_pk'] = cards_df_pop_parts.progress_apply(lambda x:\n",
    "                                                         '-'.join([x['card_id'],\n",
    "                                                                   str(x['paragraph_order']),\n",
    "                                                                   str(x['pop_order']),\n",
    "                                                                   str(x['part_order'])]), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoid pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Lets avoid creating a pop node\n",
    "cards_df_pop_parts['part_type_full'] = cards_df_pop_parts['pop_type'] + '-' + cards_df_pop_parts['part_type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Checkings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "(cards_df_pop_parts==cards_df_pop_parts2).all().all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "cards_df_pop_parts[cards_df_pop_parts['part_type']=='step_condition']['part'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cards_df_pop_parts.set_index(['card_id', 'paragraph_order', 'pop_order', 'part_order']).to_sql(\n",
    "    export_table_name, engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create metrics for pops and parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cards_df_pop_parts = pd.read_sql_table(export_table_name, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cards_df_pop_parts['paragraph_pk'] = cards_df_pop_parts.progress_apply(lambda x:\n",
    "                                                         '-'.join([x['card_id'],\n",
    "                                                                   str(int(x['paragraph_order']))]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cards_df_pop_parts['pop_pk'] = cards_df_pop_parts.progress_apply(lambda x:\n",
    "                                                         '-'.join([x['card_id'],\n",
    "                                                                   str(int(x['paragraph_order'])),\n",
    "                                                                   str(int(x['pop_order']))\n",
    "                                                                  ]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cards_df_pop_parts.set_index(['card_id', 'paragraph_order', 'pop_order', 'part_order',\n",
    "                              'paragraph_pk','pop_pk']).to_sql(\n",
    "    export_table_name, engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create pop metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false
   },
   "outputs": [],
   "source": [
    "metrics_pop = cards_df_pop_parts.pivot_table(\n",
    "    index = ['card_id', 'paragraph_pk', 'pop_type'],\n",
    "    values = ['pop'],\n",
    "    aggfunc = lambda x: len(x)\n",
    ")\n",
    "metrics_pop.columns = ['pop_count']\n",
    "metrics_pop[metrics_pop['pop_count']>5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_pop.to_sql(\n",
    "    export_table_name+'_metrics_pop', engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create part metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false
   },
   "outputs": [],
   "source": [
    "metrics_part = cards_df_pop_parts.pivot_table(\n",
    "    index = ['card_id', 'paragraph_pk', 'pop_pk', 'part_type'],\n",
    "    values = ['part'],\n",
    "    aggfunc = lambda x: len(x)\n",
    ")\n",
    "metrics_part.columns = ['part_count']\n",
    "metrics_part[metrics_part['part_count']>3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_part.to_sql(\n",
    "    export_table_name+'_metrics_part', engine, if_exists='replace')"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:mtg]",
   "language": "python",
   "name": "conda-env-mtg-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "203.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "455px",
    "left": "1008px",
    "right": "20px",
    "top": "120px",
    "width": "355px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
